{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f76dc2cce50>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# set seed for reproducibility\n",
        "import torch\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import uproot\n",
        "import numpy as np\n",
        "\n",
        "def load_root_file(file_path, branches=None, print_branches=False):\n",
        "    all_branches = {}\n",
        "    with uproot.open(file_path) as file:\n",
        "        tree = file[\"tree\"]\n",
        "        # Load all ROOT branches into array if not specified\n",
        "        if branches is None:\n",
        "            branches = tree.keys()\n",
        "        # Option to print the branch names\n",
        "        if print_branches:\n",
        "            print(\"Branches:\", tree.keys())\n",
        "        # Each branch is added to the dictionary\n",
        "        for branch in branches:\n",
        "            try:\n",
        "                all_branches[branch] = (tree[branch].array(library=\"np\"))\n",
        "            except uproot.KeyInFileError as e:\n",
        "                print(f\"KeyInFileError: {e}\")\n",
        "        # Number of events in file\n",
        "        all_branches['event'] = tree.num_entries\n",
        "    return all_branches\n",
        "\n",
        "branches_list = [\n",
        "    'sim_pT3_matched',\n",
        "    'pT3_pt',\n",
        "    'pT3_isFake',\n",
        "    'pT3_pix_eta',\n",
        "    'pT3_pix_phi',\n",
        "    'pT3_isDuplicate',\n",
        "    'pT3_eta',\n",
        "    'pT3_phi',\n",
        "    'pT3_score',\n",
        "    'pT3_pixelRadius',\n",
        "    'pT3_pixelRadiusError',\n",
        "    'pT3_tripletRadius',\n",
        "    'pT3_rPhiChiSquared',\n",
        "    'pT3_rPhiChiSquaredInwards',\n",
        "    'pT3_rzChiSquared',\n",
        "    'pT3_moduleType_binary',\n",
        "    'pT3_pLS_pMatched',\n",
        "    'pT3_t3_pMatched'\n",
        "]\n",
        "\n",
        "file_path = \"pt3_no_rphi_lstod_500.root\"\n",
        "branches = load_root_file(file_path, branches_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "eta_max = 2.5\n",
        "phi_max = np.pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_events = branches['event']\n",
        "\n",
        "# Concatenate the pt3 branches over all events\n",
        "pt3_rPhiChiSquared = np.concatenate([branches['pT3_rPhiChiSquared'][evt] for evt in range(n_events)])\n",
        "pt3_rPhiChiSquaredInwards = np.concatenate([branches['pT3_rPhiChiSquaredInwards'][evt] for evt in range(n_events)])\n",
        "pt3_rzChiSquared = np.concatenate([branches['pT3_rzChiSquared'][evt] for evt in range(n_events)])\n",
        "pt3_eta = np.abs(np.concatenate([branches['pT3_eta'][evt] for evt in range(n_events)]))\n",
        "pt3_trip_rad = np.abs(np.concatenate([branches['pT3_tripletRadius'][evt] for evt in range(n_events)]))\n",
        "pt3_pix_rad = np.abs(np.concatenate([branches['pT3_pixelRadius'][evt] for evt in range(n_events)]))\n",
        "pt3_pixRadError = np.abs(np.concatenate([branches['pT3_pixelRadiusError'][evt] for evt in range(n_events)]))\n",
        "pt3_moduleType_binary = np.concatenate([branches['pT3_moduleType_binary'][evt] for evt in range(n_events)])\n",
        "anchor3_isPS = ((pt3_moduleType_binary >> 2) & 1).astype(float)\n",
        "\n",
        "# Build the features array using the helper functions\n",
        "features = np.array([\n",
        "    np.log10(pt3_rPhiChiSquared),\n",
        "    np.log10(pt3_trip_rad),\n",
        "    np.log10(pt3_pix_rad),\n",
        "    np.log10(pt3_pixRadError),\n",
        "    np.log10(pt3_rzChiSquared),\n",
        "    np.abs(pt3_eta)/eta_max,\n",
        "    anchor3_isPS\n",
        "])\n",
        "\n",
        "eta_list = np.array([pt3_eta])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Initial dataset size: 796977\n",
            "Class distribution before downsampling - Real: 324588.0, Fake: 472389.0\n",
            "Class distribution after downsampling - Real: 324588.0, Fake: 472389.0\n",
            "Epoch [1/200], Train Loss: 0.2569, Test Loss: 0.2279\n",
            "Epoch [2/200], Train Loss: 0.2229, Test Loss: 0.2213\n",
            "Epoch [3/200], Train Loss: 0.2186, Test Loss: 0.2180\n",
            "Epoch [4/200], Train Loss: 0.2156, Test Loss: 0.2173\n",
            "Epoch [5/200], Train Loss: 0.2139, Test Loss: 0.2141\n",
            "Epoch [6/200], Train Loss: 0.2121, Test Loss: 0.2119\n",
            "Epoch [7/200], Train Loss: 0.2104, Test Loss: 0.2106\n",
            "Epoch [8/200], Train Loss: 0.2087, Test Loss: 0.2106\n",
            "Epoch [9/200], Train Loss: 0.2071, Test Loss: 0.2068\n",
            "Epoch [10/200], Train Loss: 0.2052, Test Loss: 0.2066\n",
            "Epoch [11/200], Train Loss: 0.2035, Test Loss: 0.2050\n",
            "Epoch [12/200], Train Loss: 0.2021, Test Loss: 0.2039\n",
            "Epoch [13/200], Train Loss: 0.2007, Test Loss: 0.2008\n",
            "Epoch [14/200], Train Loss: 0.1990, Test Loss: 0.1988\n",
            "Epoch [15/200], Train Loss: 0.1982, Test Loss: 0.1985\n",
            "Epoch [16/200], Train Loss: 0.1970, Test Loss: 0.1969\n",
            "Epoch [17/200], Train Loss: 0.1961, Test Loss: 0.1972\n",
            "Epoch [18/200], Train Loss: 0.1952, Test Loss: 0.1964\n",
            "Epoch [19/200], Train Loss: 0.1947, Test Loss: 0.1966\n",
            "Epoch [20/200], Train Loss: 0.1944, Test Loss: 0.1963\n",
            "Epoch [21/200], Train Loss: 0.1940, Test Loss: 0.1984\n",
            "Epoch [22/200], Train Loss: 0.1934, Test Loss: 0.1952\n",
            "Epoch [23/200], Train Loss: 0.1933, Test Loss: 0.1934\n",
            "Epoch [24/200], Train Loss: 0.1930, Test Loss: 0.1936\n",
            "Epoch [25/200], Train Loss: 0.1925, Test Loss: 0.1957\n",
            "Epoch [26/200], Train Loss: 0.1921, Test Loss: 0.1920\n",
            "Epoch [27/200], Train Loss: 0.1918, Test Loss: 0.1938\n",
            "Epoch [28/200], Train Loss: 0.1914, Test Loss: 0.1921\n",
            "Epoch [29/200], Train Loss: 0.1911, Test Loss: 0.1923\n",
            "Epoch [30/200], Train Loss: 0.1909, Test Loss: 0.1958\n",
            "Epoch [31/200], Train Loss: 0.1907, Test Loss: 0.1900\n",
            "Epoch [32/200], Train Loss: 0.1906, Test Loss: 0.1910\n",
            "Epoch [33/200], Train Loss: 0.1903, Test Loss: 0.1925\n",
            "Epoch [34/200], Train Loss: 0.1899, Test Loss: 0.1910\n",
            "Epoch [35/200], Train Loss: 0.1894, Test Loss: 0.1903\n",
            "Epoch [36/200], Train Loss: 0.1893, Test Loss: 0.1904\n",
            "Epoch [37/200], Train Loss: 0.1894, Test Loss: 0.1896\n",
            "Epoch [38/200], Train Loss: 0.1887, Test Loss: 0.1898\n",
            "Epoch [39/200], Train Loss: 0.1887, Test Loss: 0.1889\n",
            "Epoch [40/200], Train Loss: 0.1881, Test Loss: 0.1884\n",
            "Epoch [41/200], Train Loss: 0.1880, Test Loss: 0.1881\n",
            "Epoch [42/200], Train Loss: 0.1879, Test Loss: 0.1950\n",
            "Epoch [43/200], Train Loss: 0.1881, Test Loss: 0.1894\n",
            "Epoch [44/200], Train Loss: 0.1875, Test Loss: 0.1881\n",
            "Epoch [45/200], Train Loss: 0.1875, Test Loss: 0.1908\n",
            "Epoch [46/200], Train Loss: 0.1874, Test Loss: 0.1969\n",
            "Epoch [47/200], Train Loss: 0.1872, Test Loss: 0.1879\n",
            "Epoch [48/200], Train Loss: 0.1871, Test Loss: 0.1873\n",
            "Epoch [49/200], Train Loss: 0.1868, Test Loss: 0.1880\n",
            "Epoch [50/200], Train Loss: 0.1866, Test Loss: 0.1876\n",
            "Epoch [51/200], Train Loss: 0.1866, Test Loss: 0.1878\n",
            "Epoch [52/200], Train Loss: 0.1862, Test Loss: 0.1877\n",
            "Epoch [53/200], Train Loss: 0.1862, Test Loss: 0.1878\n",
            "Epoch [54/200], Train Loss: 0.1861, Test Loss: 0.1914\n",
            "Epoch [55/200], Train Loss: 0.1861, Test Loss: 0.1862\n",
            "Epoch [56/200], Train Loss: 0.1860, Test Loss: 0.1882\n",
            "Epoch [57/200], Train Loss: 0.1856, Test Loss: 0.1902\n",
            "Epoch [58/200], Train Loss: 0.1857, Test Loss: 0.1864\n",
            "Epoch [59/200], Train Loss: 0.1858, Test Loss: 0.1868\n",
            "Epoch [60/200], Train Loss: 0.1856, Test Loss: 0.1872\n",
            "Epoch [61/200], Train Loss: 0.1853, Test Loss: 0.1865\n",
            "Epoch [62/200], Train Loss: 0.1854, Test Loss: 0.1884\n",
            "Epoch [63/200], Train Loss: 0.1855, Test Loss: 0.1870\n",
            "Epoch [64/200], Train Loss: 0.1856, Test Loss: 0.1858\n",
            "Epoch [65/200], Train Loss: 0.1851, Test Loss: 0.1874\n",
            "Epoch [66/200], Train Loss: 0.1851, Test Loss: 0.1896\n",
            "Epoch [67/200], Train Loss: 0.1854, Test Loss: 0.1858\n",
            "Epoch [68/200], Train Loss: 0.1850, Test Loss: 0.1897\n",
            "Epoch [69/200], Train Loss: 0.1848, Test Loss: 0.1857\n",
            "Epoch [70/200], Train Loss: 0.1847, Test Loss: 0.1860\n",
            "Epoch [71/200], Train Loss: 0.1848, Test Loss: 0.1852\n",
            "Epoch [72/200], Train Loss: 0.1846, Test Loss: 0.1865\n",
            "Epoch [73/200], Train Loss: 0.1845, Test Loss: 0.1857\n",
            "Epoch [74/200], Train Loss: 0.1847, Test Loss: 0.1848\n",
            "Epoch [75/200], Train Loss: 0.1845, Test Loss: 0.1853\n",
            "Epoch [76/200], Train Loss: 0.1845, Test Loss: 0.1858\n",
            "Epoch [77/200], Train Loss: 0.1844, Test Loss: 0.1869\n",
            "Epoch [78/200], Train Loss: 0.1843, Test Loss: 0.1848\n",
            "Epoch [79/200], Train Loss: 0.1843, Test Loss: 0.1852\n",
            "Epoch [80/200], Train Loss: 0.1843, Test Loss: 0.1854\n",
            "Epoch [81/200], Train Loss: 0.1841, Test Loss: 0.1858\n",
            "Epoch [82/200], Train Loss: 0.1841, Test Loss: 0.1885\n",
            "Epoch [83/200], Train Loss: 0.1839, Test Loss: 0.1853\n",
            "Epoch [84/200], Train Loss: 0.1841, Test Loss: 0.1851\n",
            "Epoch [85/200], Train Loss: 0.1840, Test Loss: 0.1849\n",
            "Epoch [86/200], Train Loss: 0.1839, Test Loss: 0.1859\n",
            "Epoch [87/200], Train Loss: 0.1840, Test Loss: 0.1851\n",
            "Epoch [88/200], Train Loss: 0.1838, Test Loss: 0.1855\n",
            "Epoch [89/200], Train Loss: 0.1840, Test Loss: 0.1869\n",
            "Epoch [90/200], Train Loss: 0.1839, Test Loss: 0.1863\n",
            "Epoch [91/200], Train Loss: 0.1837, Test Loss: 0.1844\n",
            "Epoch [92/200], Train Loss: 0.1840, Test Loss: 0.1932\n",
            "Epoch [93/200], Train Loss: 0.1838, Test Loss: 0.1871\n",
            "Epoch [94/200], Train Loss: 0.1838, Test Loss: 0.1868\n",
            "Epoch [95/200], Train Loss: 0.1837, Test Loss: 0.1845\n",
            "Epoch [96/200], Train Loss: 0.1837, Test Loss: 0.1837\n",
            "Epoch [97/200], Train Loss: 0.1837, Test Loss: 0.1850\n",
            "Epoch [98/200], Train Loss: 0.1836, Test Loss: 0.1865\n",
            "Epoch [99/200], Train Loss: 0.1834, Test Loss: 0.1854\n",
            "Epoch [100/200], Train Loss: 0.1837, Test Loss: 0.1844\n",
            "Epoch [101/200], Train Loss: 0.1837, Test Loss: 0.1844\n",
            "Epoch [102/200], Train Loss: 0.1836, Test Loss: 0.1854\n",
            "Epoch [103/200], Train Loss: 0.1836, Test Loss: 0.1851\n",
            "Epoch [104/200], Train Loss: 0.1832, Test Loss: 0.1846\n",
            "Epoch [105/200], Train Loss: 0.1831, Test Loss: 0.1851\n",
            "Epoch [106/200], Train Loss: 0.1834, Test Loss: 0.1848\n",
            "Epoch [107/200], Train Loss: 0.1833, Test Loss: 0.1863\n",
            "Epoch [108/200], Train Loss: 0.1831, Test Loss: 0.1843\n",
            "Epoch [109/200], Train Loss: 0.1827, Test Loss: 0.1844\n",
            "Epoch [110/200], Train Loss: 0.1832, Test Loss: 0.1845\n",
            "Epoch [111/200], Train Loss: 0.1830, Test Loss: 0.1855\n",
            "Epoch [112/200], Train Loss: 0.1829, Test Loss: 0.1833\n",
            "Epoch [113/200], Train Loss: 0.1832, Test Loss: 0.1853\n",
            "Epoch [114/200], Train Loss: 0.1829, Test Loss: 0.1850\n",
            "Epoch [115/200], Train Loss: 0.1830, Test Loss: 0.1834\n",
            "Epoch [116/200], Train Loss: 0.1825, Test Loss: 0.1846\n",
            "Epoch [117/200], Train Loss: 0.1830, Test Loss: 0.1850\n",
            "Epoch [118/200], Train Loss: 0.1826, Test Loss: 0.1836\n",
            "Epoch [119/200], Train Loss: 0.1827, Test Loss: 0.1840\n",
            "Epoch [120/200], Train Loss: 0.1824, Test Loss: 0.1852\n",
            "Epoch [121/200], Train Loss: 0.1824, Test Loss: 0.1839\n",
            "Epoch [122/200], Train Loss: 0.1827, Test Loss: 0.1859\n",
            "Epoch [123/200], Train Loss: 0.1826, Test Loss: 0.1842\n",
            "Epoch [124/200], Train Loss: 0.1822, Test Loss: 0.1851\n",
            "Epoch [125/200], Train Loss: 0.1826, Test Loss: 0.1833\n",
            "Epoch [126/200], Train Loss: 0.1826, Test Loss: 0.1838\n",
            "Epoch [127/200], Train Loss: 0.1824, Test Loss: 0.1889\n",
            "Epoch [128/200], Train Loss: 0.1822, Test Loss: 0.1841\n",
            "Epoch [129/200], Train Loss: 0.1825, Test Loss: 0.1843\n",
            "Epoch [130/200], Train Loss: 0.1822, Test Loss: 0.1862\n",
            "Epoch [131/200], Train Loss: 0.1824, Test Loss: 0.1846\n",
            "Epoch [132/200], Train Loss: 0.1824, Test Loss: 0.1838\n",
            "Epoch [133/200], Train Loss: 0.1825, Test Loss: 0.1850\n",
            "Epoch [134/200], Train Loss: 0.1821, Test Loss: 0.1837\n",
            "Epoch [135/200], Train Loss: 0.1820, Test Loss: 0.1830\n",
            "Epoch [136/200], Train Loss: 0.1821, Test Loss: 0.1823\n",
            "Epoch [137/200], Train Loss: 0.1820, Test Loss: 0.1853\n",
            "Epoch [138/200], Train Loss: 0.1823, Test Loss: 0.1861\n",
            "Epoch [139/200], Train Loss: 0.1820, Test Loss: 0.1847\n",
            "Epoch [140/200], Train Loss: 0.1819, Test Loss: 0.1821\n",
            "Epoch [141/200], Train Loss: 0.1818, Test Loss: 0.1838\n",
            "Epoch [142/200], Train Loss: 0.1818, Test Loss: 0.1833\n",
            "Epoch [143/200], Train Loss: 0.1820, Test Loss: 0.1833\n",
            "Epoch [144/200], Train Loss: 0.1819, Test Loss: 0.1837\n",
            "Epoch [145/200], Train Loss: 0.1818, Test Loss: 0.1838\n",
            "Epoch [146/200], Train Loss: 0.1820, Test Loss: 0.1848\n",
            "Epoch [147/200], Train Loss: 0.1817, Test Loss: 0.1843\n",
            "Epoch [148/200], Train Loss: 0.1818, Test Loss: 0.1847\n",
            "Epoch [149/200], Train Loss: 0.1818, Test Loss: 0.1839\n",
            "Epoch [150/200], Train Loss: 0.1815, Test Loss: 0.1832\n",
            "Epoch [151/200], Train Loss: 0.1818, Test Loss: 0.1828\n",
            "Epoch [152/200], Train Loss: 0.1816, Test Loss: 0.1824\n",
            "Epoch [153/200], Train Loss: 0.1816, Test Loss: 0.1831\n",
            "Epoch [154/200], Train Loss: 0.1818, Test Loss: 0.1830\n",
            "Epoch [155/200], Train Loss: 0.1817, Test Loss: 0.1826\n",
            "Epoch [156/200], Train Loss: 0.1815, Test Loss: 0.1833\n",
            "Epoch [157/200], Train Loss: 0.1813, Test Loss: 0.1827\n",
            "Epoch [158/200], Train Loss: 0.1814, Test Loss: 0.1834\n",
            "Epoch [159/200], Train Loss: 0.1817, Test Loss: 0.1829\n",
            "Epoch [160/200], Train Loss: 0.1814, Test Loss: 0.1852\n",
            "Epoch [161/200], Train Loss: 0.1813, Test Loss: 0.1827\n",
            "Epoch [162/200], Train Loss: 0.1814, Test Loss: 0.1815\n",
            "Epoch [163/200], Train Loss: 0.1813, Test Loss: 0.1834\n",
            "Epoch [164/200], Train Loss: 0.1813, Test Loss: 0.1833\n",
            "Epoch [165/200], Train Loss: 0.1812, Test Loss: 0.1829\n",
            "Epoch [166/200], Train Loss: 0.1812, Test Loss: 0.1818\n",
            "Epoch [167/200], Train Loss: 0.1813, Test Loss: 0.1844\n",
            "Epoch [168/200], Train Loss: 0.1812, Test Loss: 0.1820\n",
            "Epoch [169/200], Train Loss: 0.1813, Test Loss: 0.1829\n",
            "Epoch [170/200], Train Loss: 0.1812, Test Loss: 0.1839\n",
            "Epoch [171/200], Train Loss: 0.1812, Test Loss: 0.1834\n",
            "Epoch [172/200], Train Loss: 0.1813, Test Loss: 0.1820\n",
            "Epoch [173/200], Train Loss: 0.1810, Test Loss: 0.1818\n",
            "Epoch [174/200], Train Loss: 0.1810, Test Loss: 0.1832\n",
            "Epoch [175/200], Train Loss: 0.1811, Test Loss: 0.1835\n",
            "Epoch [176/200], Train Loss: 0.1810, Test Loss: 0.1852\n",
            "Epoch [177/200], Train Loss: 0.1809, Test Loss: 0.1843\n",
            "Epoch [178/200], Train Loss: 0.1809, Test Loss: 0.1816\n",
            "Epoch [179/200], Train Loss: 0.1809, Test Loss: 0.1826\n",
            "Epoch [180/200], Train Loss: 0.1811, Test Loss: 0.1828\n",
            "Epoch [181/200], Train Loss: 0.1810, Test Loss: 0.1853\n",
            "Epoch [182/200], Train Loss: 0.1808, Test Loss: 0.1830\n",
            "Epoch [183/200], Train Loss: 0.1808, Test Loss: 0.1813\n",
            "Epoch [184/200], Train Loss: 0.1809, Test Loss: 0.1840\n",
            "Epoch [185/200], Train Loss: 0.1808, Test Loss: 0.1815\n",
            "Epoch [186/200], Train Loss: 0.1807, Test Loss: 0.1831\n",
            "Epoch [187/200], Train Loss: 0.1808, Test Loss: 0.1851\n",
            "Epoch [188/200], Train Loss: 0.1809, Test Loss: 0.1827\n",
            "Epoch [189/200], Train Loss: 0.1807, Test Loss: 0.1816\n",
            "Epoch [190/200], Train Loss: 0.1810, Test Loss: 0.1824\n",
            "Epoch [191/200], Train Loss: 0.1809, Test Loss: 0.1825\n",
            "Epoch [192/200], Train Loss: 0.1808, Test Loss: 0.1837\n",
            "Epoch [193/200], Train Loss: 0.1806, Test Loss: 0.1820\n",
            "Epoch [194/200], Train Loss: 0.1806, Test Loss: 0.1812\n",
            "Epoch [195/200], Train Loss: 0.1808, Test Loss: 0.1828\n",
            "Epoch [196/200], Train Loss: 0.1807, Test Loss: 0.1824\n",
            "Epoch [197/200], Train Loss: 0.1807, Test Loss: 0.1818\n",
            "Epoch [198/200], Train Loss: 0.1808, Test Loss: 0.1819\n",
            "Epoch [199/200], Train Loss: 0.1807, Test Loss: 0.1830\n",
            "Epoch [200/200], Train Loss: 0.1807, Test Loss: 0.1817\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import numpy as np\n",
        "\n",
        "# ------------------ Preprocessing ------------------\n",
        "input_features_numpy = np.stack(features, axis=-1)\n",
        "mask = ~np.isnan(input_features_numpy) & ~np.isinf(input_features_numpy)\n",
        "filtered_input_features_numpy = input_features_numpy[np.all(mask, axis=1)]\n",
        "t3_isFake_filtered = 1 - (np.concatenate(branches['pT3_isFake']))[np.all(mask, axis=1)]\n",
        "pt3_pLS_pMatched = np.concatenate(branches['pT3_pLS_pMatched'])[np.all(mask, axis=1)]\n",
        "\n",
        "labels_np = t3_isFake_filtered.copy()\n",
        "labels_np[pt3_pLS_pMatched < 0.99] = 0\n",
        "\n",
        "# Convert to PyTorch tensors.\n",
        "input_features_tensor = torch.tensor(filtered_input_features_numpy, dtype=torch.float32)\n",
        "labels_tensor = torch.tensor(labels_np, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# ------------------ Device Setup ------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ------------------ Neural Network ------------------\n",
        "class BinaryClassificationNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryClassificationNeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 32)\n",
        "        self.layer2 = nn.Linear(32, 32)\n",
        "        self.output_layer = nn.Linear(32, 1)  # Single output for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.layer2(x)\n",
        "        x = nn.ReLU()(x)\n",
        "        x = self.output_layer(x)\n",
        "        return torch.sigmoid(x)  # Sigmoid activation for output between 0 and 1\n",
        "\n",
        "# ------------------ Loss Function ------------------\n",
        "class WeightedBinaryCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WeightedBinaryCrossEntropyLoss, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, targets, weights):\n",
        "        eps = 1e-7\n",
        "        loss = -(targets * torch.log(outputs + eps) + (1 - targets) * torch.log(1 - outputs + eps))\n",
        "        weighted_loss = loss * weights\n",
        "        return weighted_loss.mean()\n",
        "\n",
        "# ------------------ Class Weight Calculation ------------------\n",
        "def calculate_binary_class_weights(labels):\n",
        "    total_samples = len(labels)\n",
        "    count_positive = labels.sum().item()\n",
        "    count_negative = total_samples - count_positive\n",
        "    weight_positive = total_samples / (2 * count_positive) if count_positive > 0 else 1.0\n",
        "    weight_negative = total_samples / (2 * count_negative) if count_negative > 0 else 1.0\n",
        "    \n",
        "    sample_weights = torch.zeros(total_samples)\n",
        "    for i in range(total_samples):\n",
        "        if labels[i] == 1:\n",
        "            sample_weights[i] = weight_positive\n",
        "        else:\n",
        "            sample_weights[i] = weight_negative\n",
        "    return sample_weights\n",
        "\n",
        "# ------------------ Data Preparation ------------------\n",
        "print(f\"Initial dataset size: {len(labels_tensor)}\")\n",
        "\n",
        "# Remove any rows with NaN in the input features (if any remain).\n",
        "nan_mask = torch.isnan(input_features_tensor).any(dim=1)\n",
        "filtered_inputs = input_features_tensor[~nan_mask]\n",
        "filtered_labels = labels_tensor[~nan_mask]\n",
        "\n",
        "# Print class distribution before downsampling.\n",
        "num_real = filtered_labels.sum().item()          # label = 1 means real\n",
        "num_fake = len(filtered_labels) - num_real         # label = 0 means fake\n",
        "print(f\"Class distribution before downsampling - Real: {num_real}, Fake: {num_fake}\")\n",
        "\n",
        "# Option to downsample the majority class.\n",
        "downsample_classes = False\n",
        "if downsample_classes:\n",
        "    downsample_ratios = {1: 1.0, 0: 1.0}\n",
        "    indices_list = []\n",
        "\n",
        "    # Process real class (label 1).\n",
        "    real_mask = (filtered_labels.squeeze() == 1)\n",
        "    real_indices = torch.nonzero(real_mask).squeeze()\n",
        "    num_real = real_indices.numel()\n",
        "    num_real_to_sample = int(num_real * downsample_ratios[1])\n",
        "    if num_real_to_sample < 1 and num_real > 0:\n",
        "        num_real_to_sample = 1\n",
        "    real_indices_shuffled = real_indices[torch.randperm(num_real)]\n",
        "    sampled_real_indices = real_indices_shuffled[:num_real_to_sample]\n",
        "    indices_list.append(sampled_real_indices)\n",
        "\n",
        "    # Process fake class (label 0).\n",
        "    fake_mask = (filtered_labels.squeeze() == 0)\n",
        "    fake_indices = torch.nonzero(fake_mask).squeeze()\n",
        "    num_fake = fake_indices.numel()\n",
        "    num_fake_to_sample = int(num_fake * downsample_ratios[0])\n",
        "    if num_fake_to_sample < 1 and num_fake > 0:\n",
        "        num_fake_to_sample = 1\n",
        "    fake_indices_shuffled = fake_indices[torch.randperm(num_fake)]\n",
        "    sampled_fake_indices = fake_indices_shuffled[:num_fake_to_sample]\n",
        "    indices_list.append(sampled_fake_indices)\n",
        "\n",
        "    # Combine indices from both classes.\n",
        "    selected_indices = torch.cat(indices_list)\n",
        "    filtered_inputs = filtered_inputs[selected_indices]\n",
        "    filtered_labels = filtered_labels[selected_indices]\n",
        "\n",
        "# Print class distribution after downsampling.\n",
        "num_real_after = filtered_labels.sum().item()\n",
        "num_fake_after = len(filtered_labels) - num_real_after\n",
        "print(f\"Class distribution after downsampling - Real: {num_real_after}, Fake: {num_fake_after}\")\n",
        "\n",
        "# Calculate sample weights after downsampling.\n",
        "sample_weights = calculate_binary_class_weights(filtered_labels)\n",
        "filtered_weights = sample_weights\n",
        "\n",
        "# Create the dataset.\n",
        "dataset = TensorDataset(filtered_inputs, filtered_labels, filtered_weights)\n",
        "\n",
        "# Split into train and test sets.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create data loaders.\n",
        "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=10, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=10, pin_memory=True)\n",
        "\n",
        "# ------------------ Model, Loss, and Optimizer ------------------\n",
        "input_dim = filtered_inputs.shape[1]\n",
        "model = BinaryClassificationNeuralNetwork(input_dim).to(device)\n",
        "loss_function = WeightedBinaryCrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=0.0025)\n",
        "\n",
        "def evaluate_loss(loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, weights in loader:\n",
        "            inputs, targets, weights = inputs.to(device), targets.to(device), weights.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, targets, weights)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "    return total_loss / num_batches\n",
        "\n",
        "# ------------------ Training Loop ------------------\n",
        "num_epochs = 200\n",
        "train_loss_log = []\n",
        "test_loss_log = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for inputs, targets, weights in train_loader:\n",
        "        inputs, targets, weights = inputs.to(device), targets.to(device), weights.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets, weights)\n",
        "        epoch_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    train_loss = epoch_loss / num_batches\n",
        "    test_loss = evaluate_loss(test_loader)\n",
        "    train_loss_log.append(train_loss)\n",
        "    test_loss_log.append(test_loss)\n",
        "    \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert tensors to numpy for simplicity in permutation\n",
        "input_features_np = input_features_tensor.numpy()\n",
        "labels_np = labels_tensor.numpy()\n",
        "\n",
        "def model_accuracy(features, labels, model):\n",
        "    # Move the model to CPU for evaluation\n",
        "    model.cpu()\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        # Ensure features and labels are on CPU\n",
        "        inputs = features.to('cpu')\n",
        "        labels = labels.to('cpu')\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        accuracy = (predicted == labels).float().mean().item()\n",
        "    return accuracy\n",
        "\n",
        "# Use the original input_features_tensor and labels_tensor directly\n",
        "baseline_accuracy = model_accuracy(input_features_tensor, labels_tensor, model)\n",
        "print(f\"Baseline accuracy: {baseline_accuracy}\")\n",
        "\n",
        "# Initialize an array to store feature importances\n",
        "feature_importances = np.zeros(input_features_tensor.shape[1])\n",
        "\n",
        "# Permute each feature and calculate the drop in accuracy\n",
        "for i in range(input_features_tensor.shape[1]):\n",
        "    permuted_features = input_features_tensor.clone()\n",
        "    permuted_features[:, i] = permuted_features[torch.randperm(permuted_features.size(0)), i]  # Permute feature\n",
        "\n",
        "    permuted_accuracy = model_accuracy(permuted_features, labels_tensor, model)\n",
        "    feature_importances[i] = baseline_accuracy - permuted_accuracy\n",
        "\n",
        "# Ranking features by importance\n",
        "important_features_indices = np.argsort(feature_importances)[::-1]  # Indices of features in descending importance\n",
        "important_features_scores = np.sort(feature_importances)[::-1]  # Importance scores in descending order\n",
        "\n",
        "print(\"Feature importances:\")\n",
        "for idx, score in zip(important_features_indices, important_features_scores):\n",
        "    print(f\"Feature {idx} importance: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HOST_DEVICE_CONSTANT float bias_layer1[32] = {\n",
            "1.5759399f, -0.3994772f, 0.4280974f, -1.6780146f, -0.1957098f, 1.0180659f, -1.9818025f, -0.1394756f, -1.4738522f, 0.0708124f, -0.5468803f, 1.2533343f, -1.1387161f, -2.0035551f, 0.4606403f, 0.0548070f, 1.4314936f, -1.5471562f, 0.3910650f, 0.3814445f, 0.0796451f, 1.3981149f, -1.1356467f, 0.2222562f, -1.0622172f, 0.4750573f, 0.8418977f, -1.1704211f, 0.5557163f, 1.0719343f, -0.1916229f, -0.1679582f };\n",
            "\n",
            "HOST_DEVICE_CONSTANT float wgtT_layer1[7][32] = {\n",
            "{ -0.3474015f, -0.0542566f, -0.0485157f, -0.1675780f, -0.0212521f, 0.1759759f, 0.1139071f, -0.2667485f, -0.0165271f, -0.0347156f, 0.4947113f, 0.0382834f, -0.0552550f, 0.9252654f, 0.5341441f, 1.4342718f, -0.1430950f, 0.1060660f, 0.5926535f, 0.1332781f, -0.1583868f, 0.0271739f, 0.0117791f, 0.0304052f, 1.6017749f, -0.1401283f, -0.0122920f, -0.3759536f, 0.4417094f, -0.0377251f, 0.0144219f, -0.0226532f },\n",
            "{ -0.3667378f, -0.6746863f, -0.1338243f, 0.6204596f, 0.3356600f, 0.7120172f, 0.1330214f, -0.6221143f, -0.0564318f, 0.2346845f, 0.1244973f, 0.4769411f, 0.4258578f, 0.0729432f, -0.2850539f, -0.3040272f, 0.8562855f, 0.4726804f, -0.7165542f, 0.7134872f, -0.2329236f, 0.3889499f, -0.6498508f, -0.2344403f, 0.1508345f, -0.6553926f, -1.2838581f, 0.6704699f, -0.0164178f, -0.1384076f, -1.3686306f, -0.2951273f },\n",
            "{ -0.0033338f, 1.0484898f, 0.0934406f, 0.6629775f, 0.0689807f, -1.1568865f, 0.1024116f, 0.4815159f, 0.0265825f, 0.4487650f, -1.1510973f, -0.5330415f, 0.2701699f, -0.0978494f, -0.6516586f, -1.0671266f, -0.9100941f, 0.3921767f, 0.7421433f, -0.6886612f, -0.2447457f, -0.1413060f, 0.4859841f, -0.3933356f, -0.6321754f, -0.4077085f, 1.1842752f, -0.5343203f, -0.1710724f, 0.0216467f, 1.4579115f, -0.2839714f },\n",
            "{ 0.8917388f, -0.7346919f, -1.0463046f, 0.3775975f, -0.2740558f, -0.1719273f, -1.6515648f, -0.0594413f, -0.0161534f, 1.8667692f, -1.3330016f, 1.3834783f, 0.5481292f, 1.6257373f, -0.5960386f, 0.6260622f, -0.0309609f, -0.3098203f, 0.6279967f, 0.1124809f, 0.0632304f, 0.6663120f, -1.1266142f, -0.1777018f, 1.0009360f, 1.2932386f, 0.0652186f, -1.1581264f, 0.5031686f, 0.0336957f, -0.0625242f, -0.1789904f },\n",
            "{ 0.0823773f, -0.0237806f, 0.0231955f, -0.2387511f, 1.3429683f, -0.2781298f, -0.1326117f, -0.8409966f, 0.0176393f, -0.7999352f, 0.2072813f, 0.0830497f, 0.8846267f, 1.2500743f, 0.8939823f, 0.2052755f, -0.1248793f, 0.1011277f, -0.4228626f, 0.3400199f, 0.0078322f, -0.1107475f, 0.0859198f, -0.1192811f, 0.1106006f, 1.0854321f, -0.0505248f, -0.0817660f, -0.3809140f, -1.2869573f, -0.0195171f, 0.3124282f },\n",
            "{ 0.4168696f, -0.9958051f, -1.0782611f, 0.0483475f, -2.5998909f, 0.0348478f, 0.6023145f, 1.0806988f, 2.6250148f, -0.2734412f, 1.4678140f, 1.0709250f, -0.6433306f, -0.0089214f, 0.1489374f, 1.3227717f, -0.7333025f, 0.1609016f, -1.5507437f, -0.5899166f, 0.0414102f, -3.2541153f, 1.8059956f, 0.1203557f, -1.0024394f, 1.7739828f, -0.7335321f, 0.2512074f, 1.0371581f, 0.5389997f, 0.1913349f, 0.0377848f },\n",
            "{ 0.2068907f, -0.3248565f, -1.4888443f, -0.0131287f, -0.2557163f, -0.3526284f, -0.0145357f, 0.4690470f, 0.0009708f, -0.0066097f, 0.0057493f, -0.1048828f, 0.2419544f, 0.6448194f, -0.1596510f, 0.3632039f, -0.6935519f, -0.5823233f, 0.6461959f, 0.1147442f, 0.0070467f, 0.4247423f, -0.0347635f, 0.3406380f, -0.5819650f, 1.2055371f, -0.2521732f, 0.0640389f, -0.1645643f, -0.7498630f, 0.0020844f, -0.0390695f },\n",
            "};\n",
            "\n",
            "HOST_DEVICE_CONSTANT float bias_layer2[32] = {\n",
            "0.0633579f, 0.4593537f, -0.0691272f, -0.1438287f, 0.9361685f, 0.3662346f, -0.2419607f, -0.0675904f, -0.4386955f, -0.2246882f, -0.0935369f, 0.3542993f, 0.4584119f, 0.5028061f, 0.4449652f, 1.2404934f, -0.5558411f, -0.3967998f, 0.0831212f, -0.0098973f, 0.7216183f, -0.1581921f, -0.3491245f, 0.7059122f, -0.1213400f, 0.4677292f, 0.1029293f, -0.1900419f, 0.2904402f, -0.3979444f, 0.6306248f, -0.0274149f };\n",
            "\n",
            "HOST_DEVICE_CONSTANT float wgtT_layer2[32][32] = {\n",
            "{ 0.5722624f, 0.3368938f, -1.8218364f, -0.0252748f, 1.5809534f, 0.2587431f, -0.6395526f, 0.5076606f, 0.4096195f, -0.2980015f, -0.1375373f, 0.3789146f, -0.1753593f, -0.4023470f, 0.6979520f, 0.5800683f, -0.0671642f, 0.4383081f, -1.3142579f, 0.6820988f, 0.8866743f, -1.1919681f, -0.2027263f, 0.3941090f, -0.0666155f, 0.5440816f, 0.4285937f, -0.0317254f, -0.1329492f, -0.2470990f, -0.2849207f, -0.1176251f },\n",
            "{ -0.8871774f, -0.2179974f, -0.1079302f, 0.9039204f, -0.4705651f, 0.1925211f, 0.0755439f, 0.1389809f, -1.4630297f, -0.0017908f, -0.0631670f, -0.2407371f, -0.2585385f, -0.5484543f, 0.3416178f, -0.0216174f, 0.8194001f, -0.2063909f, 0.2815488f, 0.5976820f, -0.6059444f, -0.1075521f, 0.5155245f, -1.6846395f, 0.0065877f, -0.3828012f, 0.4599103f, 0.0360636f, 0.2934900f, -5.5452971f, -1.1569753f, 0.0255817f },\n",
            "{ 0.0519854f, -0.4948820f, 0.2204677f, -0.6243419f, -0.3774720f, -0.6173939f, -0.2524445f, -2.5313017f, 0.3356981f, 0.0689486f, 0.0323522f, -1.5858946f, -0.6532866f, 0.0619118f, -0.1582293f, -0.0257011f, -0.0474944f, 0.7667286f, -1.2864348f, -0.7174448f, -0.0002346f, 0.7189957f, 0.0479244f, 1.0770253f, -0.1780624f, 0.2248868f, 0.6263909f, 0.0451234f, -0.0534327f, -0.4738616f, 0.3782449f, 0.0050786f },\n",
            "{ 0.3960356f, 1.0248016f, 0.8356014f, 0.5838362f, 0.0791774f, 0.9768089f, 1.1142952f, 0.1955252f, -0.8004215f, -0.6554992f, -0.1569969f, 0.8316295f, -0.0268809f, -0.2897643f, -0.1670559f, -0.1341080f, 0.5600795f, -0.4190452f, 0.0870086f, -0.2409004f, -0.0377129f, 0.6863511f, 0.1008233f, -1.1172857f, -0.0041256f, -1.9536994f, 0.1110794f, -0.0979697f, -0.4513276f, -0.4688559f, -0.4195471f, 0.0759091f },\n",
            "{ -0.6842685f, 0.7935900f, -0.3802448f, -0.2899687f, -0.7212173f, -0.5166797f, -0.1685155f, -1.2450526f, 1.4128737f, 0.1112472f, -0.0251580f, 0.7895823f, -0.3734271f, -0.1513002f, -0.3833641f, -0.7049287f, -0.1408319f, 0.0242678f, 1.0973767f, 1.3055599f, -0.2380463f, -0.1357138f, 1.2763869f, -0.4926352f, -0.0567776f, 0.3115025f, 0.6529822f, 0.0258648f, 1.0559049f, -2.6399384f, -1.8593594f, 0.0761756f },\n",
            "{ 1.0198060f, 0.5413188f, -0.1466975f, 0.1815583f, -0.1170207f, 0.4244478f, 0.0319607f, 0.9993107f, -1.1465575f, -0.0102748f, -0.1691968f, 0.4293271f, 0.7111287f, 0.8226286f, 0.5065497f, -0.1964381f, -0.8229968f, 0.3713945f, -6.1896348f, -0.3797095f, -0.2064250f, 0.1242395f, -0.3895251f, 0.5575079f, -0.1371019f, 0.2870941f, -0.3741273f, -0.0312969f, 0.5065262f, 0.2498390f, 0.7588174f, -0.0834824f },\n",
            "{ -1.1005622f, 0.6101134f, -0.0874131f, 0.6169051f, -1.5529906f, 2.1574590f, 0.6588187f, 1.0185667f, -0.4574422f, 0.2869343f, 0.0743489f, -0.0826598f, 0.5253894f, 0.1745321f, 1.7068485f, -0.1402569f, 0.8130929f, 1.0529518f, -0.0720364f, -0.3282907f, -2.3570311f, 0.7384629f, 0.4207625f, -1.9920623f, -0.1792511f, -1.2463542f, 0.2048784f, 0.0280013f, -2.0179057f, 1.1997482f, 0.0754208f, -0.0118306f },\n",
            "{ 0.3865936f, 0.3318528f, -1.0279512f, 0.1368982f, -0.0461441f, -0.2094522f, -0.2325482f, 0.0131031f, -0.9027597f, -2.1355910f, -0.0524198f, 0.3042864f, -0.9695396f, -0.1095738f, 0.0636847f, 0.4319417f, 0.4346441f, -0.6896937f, -0.0379933f, 0.8687896f, 0.3343717f, 1.1595794f, 1.0436133f, 0.4052865f, -0.1610223f, -0.5222809f, -0.9897834f, 0.0696832f, -0.5706311f, -1.1850901f, 0.2504498f, -0.1874883f },\n",
            "{ -0.1753103f, -4.8075385f, 1.3211262f, 0.3022377f, 1.6459174f, -0.8120282f, 0.2033681f, 0.3338327f, 1.5332142f, 4.9007325f, -0.1628737f, -0.8212898f, -1.9484377f, 0.1468963f, 0.6109570f, -19.1056442f, -0.7373948f, 1.0576172f, -1.1068689f, 0.4797799f, -0.3964227f, -1.4101220f, 2.3093228f, -1.4918159f, -0.0936932f, 1.1123024f, 2.4177876f, 0.0977647f, -0.4128056f, 0.3965254f, -1.2836667f, 0.1239657f },\n",
            "{ -0.8128132f, -1.0773605f, -0.1122338f, 0.6347984f, 0.1137699f, 0.3421929f, -0.2553200f, 0.1529037f, 1.3286103f, 0.1641684f, -0.0845387f, -1.2958752f, 0.2876272f, 0.2354304f, -0.2071219f, 0.0102590f, 0.6755572f, 1.0493816f, 0.8021399f, 0.0296581f, 0.1924900f, 0.3102576f, -1.4225770f, -0.4109799f, -0.0674724f, -0.9343492f, -0.7805969f, -0.0797405f, 0.2720354f, 0.1434160f, 0.2785307f, 0.0066909f },\n",
            "{ -0.9222970f, 0.2522908f, -0.8254806f, -0.1906937f, 1.6711587f, 1.2277987f, 0.1991323f, -2.0555885f, 0.7464225f, -0.6037270f, 0.0300641f, 1.9388044f, -0.6001194f, -0.5150595f, 0.3445944f, -0.6878693f, 1.0416027f, 0.3577910f, -0.5343616f, 1.8308610f, -0.6141064f, -3.0963488f, -2.0578072f, 0.3232226f, -0.0398607f, 1.7776744f, -0.8222060f, 0.0287887f, -0.5634399f, 1.3440483f, 0.0423750f, 0.0685727f },\n",
            "{ 0.3721088f, -0.4299771f, 0.0993314f, -1.5196574f, 0.1254475f, -0.6973576f, -0.3545670f, -0.0460710f, 0.8946281f, -1.2984545f, -0.0048220f, 0.3865936f, 0.2544036f, -0.2282323f, 0.3174834f, -0.0347605f, -0.6922470f, 0.7850752f, -0.6170731f, -0.2301883f, 1.2020174f, 0.4293755f, -0.0504088f, 1.2412726f, -0.0564411f, 0.8810320f, 0.2918807f, 0.0937446f, 0.6590444f, 0.2418903f, 0.4778298f, -0.1782496f },\n",
            "{ 0.7988566f, -0.6527615f, -0.5718162f, 0.5032688f, -0.4823666f, 0.0584276f, 0.9085317f, -0.2084765f, 0.1204882f, 0.3082633f, -0.1590981f, 0.3340847f, 0.0309015f, -0.1084387f, 0.4231290f, -0.0378121f, 0.5511918f, 0.3005213f, 0.5125559f, 0.4384551f, -0.6277609f, 0.6739240f, -0.0113177f, 0.1563658f, -0.1406748f, -0.3406479f, -0.8544245f, -0.1181284f, -0.5623586f, 0.1780860f, -1.6392833f, -0.0942076f },\n",
            "{ 1.1208180f, -6.3328695f, 1.9148669f, 0.5880720f, -0.5254930f, 0.3938402f, 2.2949946f, -0.2063849f, -1.9910876f, 0.5926652f, -0.0902372f, 1.7201430f, 1.4736956f, 0.0300882f, 0.2424765f, 0.3216630f, -0.2475029f, -1.4229246f, -0.7638879f, -0.4592536f, -0.5433440f, -0.9651304f, -1.0403175f, 1.2419538f, -0.1471411f, -0.1633304f, 2.9328635f, -0.0350444f, 1.9198908f, 0.8451792f, -0.8586314f, 0.0229657f },\n",
            "{ 1.8222539f, -0.6978710f, 0.7939411f, 0.1048962f, 0.4239220f, 0.0028993f, 0.1976198f, -0.1287343f, 0.0576430f, 0.2253173f, 0.0516658f, 0.8788821f, 0.9754007f, 0.3910621f, 0.8819395f, 0.4372335f, -0.6143405f, 0.6549071f, -0.0622513f, 0.6025750f, 0.2212418f, 0.6729483f, 1.4294475f, 0.0735264f, -0.1532093f, -0.1773045f, -0.1041135f, -0.0847967f, -0.0847790f, 0.3712001f, -0.6697780f, -0.0266177f },\n",
            "{ 0.6385242f, -1.0060105f, -0.8198127f, 0.7695595f, 1.3084469f, 1.0535327f, 0.6188902f, -1.0430634f, -0.1468703f, -2.5098417f, -0.0959021f, -2.7698650f, -1.1216303f, -0.1102578f, 0.4281122f, -0.7296471f, 0.3373347f, 0.5040104f, 0.4590167f, 0.0067346f, 1.8469110f, -0.9651734f, -1.4059519f, 0.5914748f, -0.0128208f, 0.2532027f, -1.4032685f, 0.1035364f, 1.1790072f, 1.1229508f, 0.8365484f, 0.0907936f },\n",
            "{ 0.7891247f, 0.1306015f, -1.3241760f, -0.3785312f, 0.7881665f, -0.5123938f, 0.1097103f, -1.1037354f, 0.1139448f, 0.3945138f, -0.0570675f, -0.1483177f, 1.3997582f, 1.3245368f, 0.3402426f, -0.5615861f, -1.6064751f, 0.1096867f, -0.5077539f, -0.2007247f, 0.7089014f, -0.1658731f, -0.4755763f, 0.4147099f, -0.1853163f, 0.6112117f, -0.4934111f, -0.1672353f, 0.9248938f, -0.1286147f, 1.5956756f, -0.0934100f },\n",
            "{ -0.2354999f, 0.6593857f, 0.4466551f, 0.7473737f, 0.3107813f, 0.6484476f, -0.0295832f, 0.3290562f, -0.6010941f, 0.1952592f, -0.0409567f, -0.5355819f, 0.1871330f, 0.4180622f, 0.0900965f, -0.4758534f, 0.1269251f, -0.0814411f, -0.5455685f, -0.6601026f, -0.3417285f, -0.1942614f, -0.6587195f, -0.9504131f, -0.1723215f, -1.0989089f, -0.6017314f, -0.1773497f, -0.5387052f, 0.2000405f, 0.2440681f, -0.1725183f },\n",
            "{ 0.1849156f, 1.0356791f, -1.3648641f, 0.4475305f, 0.4527933f, -0.8054384f, -0.0944429f, 0.5726690f, -0.9949646f, -0.0294485f, 0.0056633f, 0.8237581f, 0.9483374f, 0.0487413f, -1.8764948f, 0.6051844f, 0.2517006f, -0.3936634f, -0.8272371f, 0.5685397f, -0.2556044f, -1.2054704f, 1.4894514f, 0.3256601f, -0.1737982f, -0.2240586f, 1.9796519f, -0.0818237f, 0.4227292f, 0.6391121f, 0.3150747f, 0.0057420f },\n",
            "{ 0.4697051f, -0.1305843f, 0.5515794f, 0.0385911f, 0.3896470f, 0.2923502f, 0.1221989f, -0.7075058f, -0.1766742f, 0.3950472f, -0.0094887f, 0.6944149f, 0.6976904f, 0.6521913f, 1.1389599f, 0.1562081f, -0.3547825f, 0.7349009f, 0.0579283f, -0.3351633f, 0.7089083f, -0.0525217f, -0.0792412f, 0.4900373f, -0.0199860f, -0.1168333f, 0.3859770f, 0.0996692f, 0.1690076f, -0.3955406f, 0.7028608f, -0.0965852f },\n",
            "{ 0.0080151f, -0.0178046f, -0.0514502f, 0.0378601f, -0.0684637f, 0.1478085f, 0.1445412f, 0.0867788f, -0.0046887f, 0.1278147f, -0.0629931f, 0.0917773f, -0.1515353f, -0.1290499f, 0.1762302f, 0.1504106f, 0.0768159f, -0.0311486f, 0.1482349f, -0.1134991f, 0.0692187f, 0.0303672f, 0.1628487f, -0.0678173f, -0.1263500f, -0.0319935f, 0.1651396f, 0.1085092f, 0.0297102f, -0.0214256f, -0.1085957f, -0.0054797f },\n",
            "{ 0.1635808f, 0.6696899f, 1.1459655f, -1.8620111f, -1.4723020f, -1.8911736f, -0.0227967f, 1.2425743f, 0.0095291f, -0.1510487f, -0.0130580f, 0.6183267f, -0.0942807f, 0.2249234f, -1.0037369f, 0.0586605f, -1.7465464f, -0.3196641f, -1.3911572f, 0.3639389f, 1.1854845f, 0.6542436f, -0.0695166f, 0.1274556f, -0.1205573f, 1.1984328f, -0.2620020f, -0.1687358f, 1.1193085f, -1.3924385f, 0.0540378f, -0.1712857f },\n",
            "{ -0.8728495f, 0.6987583f, 0.6107583f, 1.0872483f, -0.2612286f, 1.5490466f, 0.2575256f, 0.7357552f, 0.4177470f, -0.8231928f, -0.1985539f, -0.7666418f, 0.4094535f, -0.6662753f, 0.8739655f, 0.1329452f, 1.0075372f, -0.8501315f, 0.7179881f, 0.5654334f, -1.6843783f, 0.1463230f, -0.3960015f, -1.1047410f, -0.1838727f, -2.5201492f, 0.4211314f, -0.0157428f, -1.2194939f, 0.6605545f, -0.1484727f, -0.0471320f },\n",
            "{ -0.0208039f, -0.0941521f, 0.1715404f, 0.1244262f, 0.1108568f, 0.0015475f, 0.0266803f, -0.1017057f, -0.1097517f, -0.0875320f, 0.0176085f, -0.0487771f, 0.0735395f, 0.1269764f, 0.0814725f, -0.0673299f, 0.1156253f, -0.1690659f, 0.1654003f, 0.0281075f, -0.0150414f, 0.0688313f, -0.0090835f, -0.0755056f, 0.1433578f, -0.1316643f, -0.1251376f, 0.0762050f, -0.1321571f, 0.1860849f, -0.1635163f, -0.1175106f },\n",
            "{ -0.3658982f, -3.4244840f, 2.5753267f, -0.1196649f, -0.5111231f, -0.7399639f, -0.2092710f, -0.5111302f, -1.4803964f, 3.1929042f, 0.1015677f, 0.1454117f, -0.3266153f, 0.7451766f, 0.4969999f, 0.2876650f, -0.4761235f, 0.4963956f, 1.2623167f, 1.2729640f, -0.9209481f, -7.7068243f, 1.2757560f, -2.6170399f, -0.1536412f, 1.6609026f, -2.2786829f, -0.0751027f, 0.1190552f, 0.7398465f, -3.9555848f, -0.0629902f },\n",
            "{ -0.5108770f, -0.2491422f, -1.0344092f, -0.1021060f, -2.3563685f, -0.8229386f, -1.1861585f, 1.4563291f, -0.2268777f, 1.5420488f, -0.0896737f, 0.5602717f, 0.3522401f, -0.1445159f, -0.2329012f, 0.5504952f, 0.2836413f, 0.2245877f, -0.6218588f, 0.1366249f, 0.0172137f, 0.6798177f, -0.2639569f, -0.1645236f, -0.0784609f, -0.2883270f, -0.0564055f, -0.0055247f, -1.5309701f, 0.3969671f, 1.8172739f, -0.0227272f },\n",
            "{ -0.9351805f, -2.4521739f, -1.1779455f, -0.1147421f, 2.1596577f, -1.7755131f, -2.9028165f, 0.0652323f, -1.1070383f, -0.1882304f, -0.0795557f, -0.2530831f, -1.6575696f, -4.2802634f, -2.7123969f, 0.2177701f, 2.0433569f, -3.7647798f, -0.5291906f, 2.3641863f, 1.4638669f, -3.1755233f, 2.1239731f, 1.1620560f, 0.0163967f, -0.1226057f, -0.6799309f, -0.1149374f, -0.8949907f, -7.5501499f, -0.0999937f, 0.0948916f },\n",
            "{ -0.5796216f, -0.4099113f, 0.4310489f, 0.2779022f, -1.5068766f, 0.3203520f, 1.0939468f, -0.5150675f, 0.8022348f, -0.0348284f, -0.1581970f, 0.8606878f, -0.1459637f, 0.5056880f, 0.1700027f, -0.0999308f, -0.0036557f, 1.5873796f, 0.2363566f, -0.9106732f, -1.7199920f, 0.8135584f, 0.2998293f, -0.4289346f, 0.1315143f, -0.0617834f, 0.2050890f, -0.1437266f, -0.1175391f, 0.7528495f, 0.7890192f, -0.0363525f },\n",
            "{ 0.2810111f, 0.4123453f, 0.6784044f, -0.6179276f, 0.0515678f, -0.2369226f, -0.4901982f, 0.7606613f, 0.7496457f, -1.1481746f, -0.1646169f, -0.5684467f, -0.5577074f, -0.3330893f, 0.1603930f, 0.0298266f, -0.6632493f, 0.5865896f, 0.1378208f, -0.0404083f, 0.6536301f, 0.5165275f, 0.2557961f, 0.1148743f, 0.0224334f, 0.1064130f, -0.6607357f, -0.0877633f, -0.4062766f, 0.4619230f, 0.1384063f, -0.1500547f },\n",
            "{ 0.5768040f, 0.0763197f, -1.1055049f, -0.1287479f, 0.2866294f, -0.1598310f, 0.5998008f, -0.4732241f, -0.2865516f, -3.1452153f, 0.0515751f, -2.5850151f, 0.5317019f, 0.0290872f, -0.1248025f, -1.0719649f, 0.0538755f, -0.8568730f, 0.3467563f, 0.2021242f, -0.0663058f, 0.4546203f, -1.1581416f, 0.0227904f, -0.0683903f, 0.3332043f, -0.5567750f, 0.0531820f, 0.3950544f, 0.1517823f, -1.3964405f, 0.0268177f },\n",
            "{ -0.3688937f, -0.8499152f, 0.0763780f, 2.9355760f, -0.5730656f, 0.0875161f, 0.1182916f, 1.4291385f, -0.9594801f, 0.8865954f, 0.0026722f, 0.9416156f, -3.0508001f, -4.3402481f, 0.7758306f, -0.6752788f, 3.2039535f, -0.5234624f, 1.8154153f, 2.5595496f, 0.0432465f, 1.4511095f, -0.2832824f, 0.6166583f, 0.0722110f, -1.0498918f, -3.4538679f, -0.0362559f, 0.7699417f, -1.7401149f, 0.4968873f, -0.1978272f },\n",
            "{ 0.0179032f, -0.1739047f, -0.0826548f, -0.1164299f, 0.1647398f, -0.1400619f, -0.1415714f, -0.0292283f, 0.1746308f, 0.0184301f, -0.0927308f, 0.1593546f, 0.0693988f, -0.1222979f, 0.0824402f, 0.0501017f, -0.0612161f, 0.1525431f, 0.1578687f, -0.1254782f, 0.0663695f, 0.1054125f, -0.1706932f, 0.1014426f, 0.0462569f, 0.1292049f, 0.1683123f, -0.1621062f, 0.1438605f, 0.0916909f, -0.1403798f, 0.0285798f },\n",
            "};\n",
            "\n",
            "HOST_DEVICE_CONSTANT float bias_output_layer[1] = {\n",
            "-0.1911382f };\n",
            "\n",
            "HOST_DEVICE_CONSTANT float wgtT_output_layer[32][1] = {\n",
            "{ 0.3833938f },\n",
            "{ 0.8938782f },\n",
            "{ -0.6683310f },\n",
            "{ -2.0997012f },\n",
            "{ 1.0822421f },\n",
            "{ -1.6222913f },\n",
            "{ 0.5798858f },\n",
            "{ -0.3351963f },\n",
            "{ -0.6017966f },\n",
            "{ -1.4739996f },\n",
            "{ 0.0138155f },\n",
            "{ -0.6730824f },\n",
            "{ 0.6080490f },\n",
            "{ -3.3383362f },\n",
            "{ 0.5202572f },\n",
            "{ 1.6656327f },\n",
            "{ 2.2888272f },\n",
            "{ 0.7616234f },\n",
            "{ -0.8992535f },\n",
            "{ -0.4048433f },\n",
            "{ -1.0768006f },\n",
            "{ 0.6336359f },\n",
            "{ -0.5348358f },\n",
            "{ -1.4301811f },\n",
            "{ -0.0362373f },\n",
            "{ -1.9248599f },\n",
            "{ -0.6708170f },\n",
            "{ 0.0836050f },\n",
            "{ 1.1138842f },\n",
            "{ -0.8488451f },\n",
            "{ 1.2371943f },\n",
            "{ 0.1396129f },\n",
            "};\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def print_formatted_weights_biases(weights, biases, layer_name):\n",
        "    # Print biases\n",
        "    print(f\"HOST_DEVICE_CONSTANT float bias_{layer_name}[{len(biases)}] = {{\")\n",
        "    print(\", \".join(f\"{b:.7f}f\" for b in biases) + \" };\")\n",
        "    print()\n",
        "\n",
        "    # Print weights\n",
        "    print(f\"HOST_DEVICE_CONSTANT float wgtT_{layer_name}[{len(weights[0])}][{len(weights)}] = {{\")\n",
        "    for row in weights.T:\n",
        "        formatted_row = \", \".join(f\"{w:.7f}f\" for w in row)\n",
        "        print(f\"{{ {formatted_row} }},\")\n",
        "    print(\"};\")\n",
        "    print()\n",
        "\n",
        "def print_model_weights_biases(model):\n",
        "    # Make sure the model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate through all named modules in the model\n",
        "    for name, module in model.named_modules():\n",
        "        # Check if the module is a linear layer\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Get weights and biases\n",
        "            weights = module.weight.data.cpu().numpy()\n",
        "            biases = module.bias.data.cpu().numpy()\n",
        "\n",
        "            # Print formatted weights and biases\n",
        "            print_formatted_weights_biases(weights, biases, name.replace('.', '_'))\n",
        "\n",
        "print_model_weights_biases(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1459615/1646812576.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(features, dtype=torch.float32).to('cpu')\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def model_outputs(features, model):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.tensor(features, dtype=torch.float32).to('cpu')\n",
        "        outputs = model(inputs).squeeze().cpu().numpy()\n",
        "    return outputs\n",
        "\n",
        "# Calculate model outputs\n",
        "probabilities = model_outputs(filtered_inputs, model)\n",
        "\n",
        "# Calculate ROC curve and AUC\n",
        "fpr, tpr, thresholds = roc_curve(filtered_labels, probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot ROC curve\n",
        "plt.figure()\n",
        "lw = 2  # Line width\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.3f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure input_features_tensor is moved to the appropriate device\n",
        "input_features_tensor = input_features_tensor.to('cpu')\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(input_features_tensor)\n",
        "    predictions = outputs.squeeze().cpu().numpy()\n",
        "\n",
        "full_tracks = (\n",
        "    (np.concatenate(branches['pT3_isFake']) == 0) &\n",
        "    (np.concatenate(branches['pT3_pLS_pMatched']) >= 0.95) & \n",
        "    (np.concatenate(branches['pT3_t3_pMatched']) >= 0.95)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "pt: 0 to 5\n",
            "84% Retention Cut: {0.9142f, 0.9412f, 0.9333f, 0.9274f, 0.9161f, 0.8633f, 0.6934f, 0.6983f, 0.6502f, 0.7037f} Mean: 0.8241\n",
            "85% Retention Cut: {0.9078f, 0.9375f, 0.9294f, 0.9217f, 0.9112f, 0.855f, 0.6642f, 0.6831f, 0.6328f, 0.6844f} Mean: 0.8127\n",
            "86% Retention Cut: {0.9006f, 0.9328f, 0.9246f, 0.9142f, 0.9048f, 0.8449f, 0.6273f, 0.6646f, 0.6177f, 0.6691f} Mean: 0.8\n",
            "88% Retention Cut: {0.8806f, 0.9207f, 0.9077f, 0.8969f, 0.8891f, 0.8168f, 0.5488f, 0.6142f, 0.581f, 0.6239f} Mean: 0.768\n",
            "90% Retention Cut: {0.8484f, 0.9038f, 0.8814f, 0.8727f, 0.869f, 0.7831f, 0.4618f, 0.5568f, 0.5394f, 0.5687f} Mean: 0.7285\n",
            "92% Retention Cut: {0.7936f, 0.882f, 0.8339f, 0.8339f, 0.8339f, 0.7374f, 0.3693f, 0.4745f, 0.4807f, 0.4985f} Mean: 0.6738\n",
            "93% Retention Cut: {0.757f, 0.8661f, 0.8065f, 0.8115f, 0.8099f, 0.7018f, 0.3164f, 0.4262f, 0.4432f, 0.4549f} Mean: 0.6393\n",
            "95% Retention Cut: {0.6288f, 0.8014f, 0.7218f, 0.743f, 0.7519f, 0.5953f, 0.2174f, 0.3177f, 0.3687f, 0.3591f} Mean: 0.5505\n",
            "98% Retention Cut: {0.3464f, 0.5355f, 0.4512f, 0.5705f, 0.5278f, 0.2869f, 0.0769f, 0.1188f, 0.2149f, 0.1755f} Mean: 0.3304\n",
            "99% Retention Cut: {0.1992f, 0.3243f, 0.3208f, 0.4736f, 0.2977f, 0.1502f, 0.0391f, 0.0471f, 0.1444f, 0.1007f} Mean: 0.2097\n",
            "99.5% Retention Cut: {0.1227f, 0.1901f, 0.218f, 0.3438f, 0.1011f, 0.0882f, 0.0207f, 0.0217f, 0.0825f, 0.0561f} Mean: 0.1245\n",
            "\n",
            "pt: 5 to inf\n",
            "84% Retention Cut: {0.8808f} Mean: 0.8808\n",
            "85% Retention Cut: {0.8758f} Mean: 0.8758\n",
            "86% Retention Cut: {0.8638f} Mean: 0.8638\n",
            "88% Retention Cut: {0.8406f} Mean: 0.8406\n",
            "90% Retention Cut: {0.7942f} Mean: 0.7942\n",
            "92% Retention Cut: {0.7483f} Mean: 0.7483\n",
            "93% Retention Cut: {0.7221f} Mean: 0.7221\n",
            "95% Retention Cut: {0.657f} Mean: 0.657\n",
            "98% Retention Cut: {0.3395f} Mean: 0.3395\n",
            "99% Retention Cut: {0.2582f} Mean: 0.2582\n",
            "99.5% Retention Cut: {0.1498f} Mean: 0.1498\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "\n",
        "def plot_for_pt_bin(pt_min, pt_max, percentiles, eta_bin_edges, eta_list, predictions, full_tracks, branches, show_plot=False):\n",
        "    \"\"\"\n",
        "    Calculate and (optionally) plot cut values for specified percentiles in a given pt bin\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    pt_min : float\n",
        "        Minimum pt value for the bin\n",
        "    pt_max : float\n",
        "        Maximum pt value for the bin\n",
        "    percentiles : list\n",
        "        List of percentiles to calculate (e.g., [92.5, 96.7, 99])\n",
        "    eta_bin_edges : array\n",
        "        Edges of the eta bins\n",
        "    eta_list : list\n",
        "        List of eta values\n",
        "    predictions : array\n",
        "        Array of DNN predictions\n",
        "    full_tracks : array\n",
        "        Boolean array for track selection\n",
        "    branches : dict\n",
        "        Dictionary containing branch data\n",
        "    show_plot : bool\n",
        "        If True, produce plots (default False)\n",
        "    \"\"\"\n",
        "    # Filter data based on pt bin\n",
        "    abs_eta = eta_list[0][full_tracks & (np.concatenate(branches['pT3_pt']) > pt_min) & \n",
        "                         (np.concatenate(branches['pT3_pt']) <= pt_max)]\n",
        "    predictions_filtered = predictions[full_tracks & (np.concatenate(branches['pT3_pt']) > pt_min) & \n",
        "                                    (np.concatenate(branches['pT3_pt']) <= pt_max)]\n",
        "    \n",
        "    # Dictionary to store cut values for different percentiles\n",
        "    cut_values = {p: [] for p in percentiles}\n",
        "\n",
        "    # Loop through each eta bin\n",
        "    for i in range(len(eta_bin_edges) - 1):\n",
        "        # Get indices of tracks within the current eta bin\n",
        "        bin_indices = (abs_eta >= eta_bin_edges[i]) & (abs_eta < eta_bin_edges[i + 1])\n",
        "        \n",
        "        # Get the corresponding DNN prediction scores\n",
        "        bin_predictions = predictions_filtered[bin_indices]\n",
        "        \n",
        "        # Calculate the percentile cut values for the current bin\n",
        "        for percentile in percentiles:\n",
        "            cut_value = np.percentile(bin_predictions, 100 - percentile)  # Convert retention to percentile\n",
        "            cut_values[percentile].append(cut_value)\n",
        "\n",
        "    # --- Plotting (toggled) ---\n",
        "    if show_plot:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.hist2d(abs_eta, predictions_filtered, bins=[eta_bin_edges, 50], norm=LogNorm())\n",
        "        plt.colorbar(label='Counts')\n",
        "        plt.xlabel(\"Absolute Eta\")\n",
        "        plt.ylabel(\"DNN Prediction Score\")\n",
        "        plt.title(f\"DNN Score vs. Abs Eta for 100% Matched Tracks (pt: {pt_min} to {pt_max})\")\n",
        "\n",
        "        # Plot the cut values with different colors\n",
        "        cut_x = eta_bin_edges[:-1] + (eta_bin_edges[1] - eta_bin_edges[0]) / 2  # Mid-points of the bins\n",
        "        colors = plt.cm.rainbow(np.linspace(0, 1, len(percentiles)))  # Generate distinct colors\n",
        "        \n",
        "        for percentile, color in zip(percentiles, colors):\n",
        "            plt.plot(cut_x, cut_values[percentile], '-', color=color, marker='o', \n",
        "                     label=f'{percentile}% Retention Cut')\n",
        "        \n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "    \n",
        "    # Print the cut values\n",
        "    print(f\"\\npt: {pt_min} to {pt_max}\")\n",
        "    for percentile in percentiles:\n",
        "        values = cut_values[percentile]\n",
        "        vals = np.round(values, 4)\n",
        "        print(f\"{percentile}% Retention Cut:\",\n",
        "              '{' + ', '.join(f'{x}f' for x in vals) + '}',\n",
        "              \"Mean:\", np.round(np.mean(values), 4))\n",
        "\n",
        "def analyze_pt_bins(pt_bins, percentiles, eta_bin_edges, eta_list, predictions, full_tracks, branches, show_plot=False):\n",
        "    \"\"\"\n",
        "    Analyze (and optionally plot) for multiple pt bins and percentiles\n",
        "    \"\"\"\n",
        "    for i in range(len(pt_bins) - 1):\n",
        "        plot_for_pt_bin(pt_bins[i], pt_bins[i + 1], percentiles, eta_bin_edges, \n",
        "                        eta_list, predictions, full_tracks, branches, show_plot=show_plot)\n",
        "\n",
        "percentiles = [84, 85, 86, 88, 90, 92, 93, 95, 98, 99, 99.5]\n",
        "\n",
        "# For pt <= 5 using multiple eta bins (no plots shown by default)\n",
        "pt_bins_low = [0, 5]\n",
        "analyze_pt_bins(pt_bins_low, percentiles, np.arange(0, 2.75, 0.25), eta_list, predictions, full_tracks, branches)\n",
        "\n",
        "# For pt > 5 using a single eta bin (no plots shown by default)\n",
        "pt_bins_high = [5, np.inf]\n",
        "single_eta_bin = np.array([0, 2.75])\n",
        "analyze_pt_bins(pt_bins_high, percentiles, single_eta_bin, eta_list, predictions, full_tracks, branches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "analysisenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
