{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f350c36dfb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "import torch\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uproot\n",
    "import numpy as np\n",
    "\n",
    "def load_root_file(file_path, branches=None, print_branches=False):\n",
    "    all_branches = {}\n",
    "    with uproot.open(file_path) as file:\n",
    "        tree = file[\"tree\"]\n",
    "        # Load all ROOT branches into array if not specified\n",
    "        if branches is None:\n",
    "            branches = tree.keys()\n",
    "        # Option to print the branch names\n",
    "        if print_branches:\n",
    "            print(\"Branches:\", tree.keys())\n",
    "        # Each branch is added to the dictionary\n",
    "        for branch in branches:\n",
    "            try:\n",
    "                all_branches[branch] = (tree[branch].array(library=\"np\"))\n",
    "            except uproot.KeyInFileError as e:\n",
    "                print(f\"KeyInFileError: {e}\")\n",
    "        # Number of events in file\n",
    "        all_branches['event'] = tree.num_entries\n",
    "    return all_branches\n",
    "\n",
    "def load_root_files(file_path1, file_path2, branches=None, print_branches=False):\n",
    "    all_branches = {}\n",
    "    def load_file(file_path, all_branches):\n",
    "        with uproot.open(file_path) as file:\n",
    "                tree = file[\"tree\"]\n",
    "                # Load all ROOT branches into array if not specified\n",
    "                if branches is None:\n",
    "                    file_branches = tree.keys()\n",
    "                else:\n",
    "                    file_branches = branches\n",
    "                # Option to print the branch names\n",
    "                if print_branches:\n",
    "                    print(f\"Branches in {file_path}:\", tree.keys())\n",
    "                # Each branch is added to the dictionary\n",
    "                for branch in file_branches:\n",
    "                    try:\n",
    "                        if branch in all_branches:\n",
    "                            all_branches[branch] = np.concatenate(\n",
    "                                (all_branches[branch], tree[branch].array(library=\"np\"))\n",
    "                            )\n",
    "                        else:\n",
    "                            all_branches[branch] = tree[branch].array(library=\"np\")\n",
    "                    except uproot.KeyInFileError as e:\n",
    "                        print(f\"KeyInFileError in {file_path}: {e}\")\n",
    "                # Number of events in file\n",
    "                all_branches['event'] = all_branches.get('event', 0) + tree.num_entries\n",
    "    load_file(file_path1, all_branches)\n",
    "    load_file(file_path2, all_branches)\n",
    "\n",
    "    return all_branches\n",
    "\n",
    "branches_list = [\n",
    "    't4_innerRadius',\n",
    "    't4_outerRadius',\n",
    "    't4_pt',\n",
    "    't4_eta',\n",
    "    't4_phi',\n",
    "    't4_isFake',\n",
    "    't4_t3_idx0',\n",
    "    't4_t3_idx1',\n",
    "    't4_pMatched',\n",
    "    't4_sim_vxy',\n",
    "    't4_sim_vz',\n",
    "    't4_t3_fakeScore1',\n",
    "    't4_t3_promptScore1',\n",
    "    't4_t3_displacedScore1',\n",
    "    't4_t3_fakeScore2',\n",
    "    't4_t3_promptScore2',\n",
    "    't4_t3_displacedScore2',\n",
    "    't4_regressionRadius',\n",
    "    't4_nonAnchorRegressionRadius'\n",
    "]\n",
    "\n",
    "# Hit-dependent branches\n",
    "suffixes = ['r', 'z', 'eta', 'phi', 'layer']\n",
    "branches_list += [f't4_t3_{i}_{suffix}' for i in [0, 2, 4] for suffix in suffixes]\n",
    "\n",
    "PU_file_path = \"noCuts_Current_150925_500ev.root\"\n",
    "cube_file_path = \"noCuts_cube50_cpu_debugfull.root\"\n",
    "branches = load_root_files(PU_file_path, cube_file_path, branches_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z max: 267.2349853515625, R max: 110.10993957519531, Eta max: 2.5\n"
     ]
    }
   ],
   "source": [
    "z_max = np.max([np.max(event) for event in branches[f't4_t3_4_z'] if event.size>0])\n",
    "r_max = np.max([np.max(event) for event in branches[f't4_t3_4_r'] if event.size>0])\n",
    "eta_max = 2.5\n",
    "phi_max = np.pi\n",
    "\n",
    "print(f'Z max: {z_max}, R max: {r_max}, Eta max: {eta_max}')\n",
    "\n",
    "def delta_phi(phi1, phi2):\n",
    "    delta = phi1 - phi2\n",
    "    # Adjust delta to be within the range [-pi, pi]\n",
    "    if delta > np.pi:\n",
    "        delta -= 2 * np.pi\n",
    "    elif delta < -np.pi:\n",
    "        delta += 2 * np.pi\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = []\n",
    "eta_list = [] # Used for DNN cut values\n",
    "\n",
    "for event in range(branches['event']):\n",
    "    # Determine the number of elements in this event\n",
    "    num_elements = len(branches['t4_t3_idx0'][event])\n",
    "\n",
    "    for i in range(num_elements):\n",
    "        features_iter = []\n",
    "        eta_iter = []\n",
    "        \n",
    "        idx0 = branches['t4_t3_idx0'][event][i]\n",
    "        idx1 = branches['t4_t3_idx1'][event][i]\n",
    "\n",
    "        eta1 = np.abs(branches['t4_t3_0_eta'][event][idx0])\n",
    "        eta2 = np.abs(branches['t4_t3_2_eta'][event][idx0])\n",
    "        eta3 = np.abs(branches['t4_t3_4_eta'][event][idx0])\n",
    "        eta4 = np.abs(branches['t4_t3_4_eta'][event][idx1])\n",
    "\n",
    "        phi1 = (branches['t4_t3_0_phi'][event][idx0])\n",
    "        phi2 = (branches['t4_t3_2_phi'][event][idx0])\n",
    "        phi3 = (branches['t4_t3_4_phi'][event][idx0])\n",
    "        phi4 = (branches['t4_t3_4_phi'][event][idx1])\n",
    "\n",
    "        z1 = np.abs(branches['t4_t3_0_z'][event][idx0])\n",
    "        z2 = np.abs(branches['t4_t3_2_z'][event][idx0])\n",
    "        z3 = np.abs(branches['t4_t3_4_z'][event][idx0])\n",
    "        z4 = np.abs(branches['t4_t3_4_z'][event][idx1])\n",
    "\n",
    "        r1 = branches['t4_t3_0_r'][event][idx0]\n",
    "        r2 = branches['t4_t3_2_r'][event][idx0]\n",
    "        r3 = branches['t4_t3_4_r'][event][idx0]\n",
    "        r4 = branches['t4_t3_4_r'][event][idx1]\n",
    "\n",
    "        innerRad = branches['t4_innerRadius'][event][i]\n",
    "        outerRad = branches['t4_outerRadius'][event][i]\n",
    "\n",
    "        regRad = branches['t4_regressionRadius'][event][i]\n",
    "        nonAnchorRegRad = branches['t4_nonAnchorRegressionRadius'][event][i]\n",
    "\n",
    "        f1 = branches['t4_t3_fakeScore1'][event][i]\n",
    "        f2 = branches['t4_t3_fakeScore2'][event][i]\n",
    "        p1 = branches['t4_t3_promptScore1'][event][i]\n",
    "        p2 = branches['t4_t3_promptScore2'][event][i]\n",
    "        d1 = branches['t4_t3_displacedScore1'][event][i]\n",
    "        d2 = branches['t4_t3_displacedScore2'][event][i]\n",
    "\n",
    "\n",
    "        # Construct the input feature vector using pairwise differences\n",
    "        features_iter = [\n",
    "            eta1 / eta_max,                      # First hit eta, normalized\n",
    "            np.abs(phi1) / phi_max,              # First hit phi, normalized\n",
    "            z1 / z_max,                          # First hit z, normalized\n",
    "            r1 / r_max,                          # First hit r, normalized\n",
    "\n",
    "            eta2 - eta1,                         # Difference in eta between hit 2 and 1\n",
    "            delta_phi(phi2, phi1) / phi_max,     # Difference in phi between hit 2 and 1\n",
    "            (z2 - z1) / z_max,                   # Difference in z between hit 2 and 1, normalized\n",
    "            (r2 - r1) / r_max,                   # Difference in r between hit 2 and 1, normalized\n",
    "\n",
    "            eta3 - eta2,                         # Difference in eta between hit 3 and 2\n",
    "            delta_phi(phi3, phi2) / phi_max,     # Difference in phi between hit 3 and 2\n",
    "            (z3 - z2) / z_max,                   # Difference in z between hit 3 and 2, normalized\n",
    "            (r3 - r2) / r_max,                   # Difference in r between hit 3 and 2, normalized\n",
    "\n",
    "            eta4 - eta3,                         # Difference in eta between hit 4 and 3\n",
    "            delta_phi(phi4, phi3) / phi_max,     # Difference in phi between hit 4 and 3\n",
    "            (z4 - z3) / z_max,                   # Difference in z between hit 4 and 3, normalized\n",
    "            (r4 - r3) / r_max,                   # Difference in r between hit 4 and 3, normalized\n",
    "\n",
    "            1.0/innerRad,\n",
    "            1.0/outerRad,\n",
    "            innerRad/outerRad,\n",
    "            1.0/regRad,\n",
    "            1.0/nonAnchorRegRad,\n",
    "\n",
    "            f1,\n",
    "            p1,\n",
    "            d1,\n",
    "\n",
    "            f2,\n",
    "            p2,\n",
    "            d2,\n",
    "\n",
    "            (f2 - f1),\n",
    "            (p2 - p1),\n",
    "            (d2 - d1),\n",
    "        ]\n",
    "\n",
    "        # Use the abs eta value of first hit to select cut thresholds\n",
    "        eta_iter.extend([np.abs(branches['t4_t3_0_eta'][event][idx0])])\n",
    "        \n",
    "        # Append the feature vector to the list\n",
    "        features_list.append(features_iter)\n",
    "        eta_list.append(eta_iter)\n",
    "\n",
    "# Convert the list of features to a NumPy array\n",
    "features = np.array(features_list).T\n",
    "eta_list = np.array(eta_list).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Stack features along a new axis to form a single array suitable for NN input\n",
    "input_features_numpy = np.stack(features, axis=-1)\n",
    "\n",
    "# Identify rows with NaN or Inf values\n",
    "mask = ~np.isnan(input_features_numpy) & ~np.isinf(input_features_numpy)\n",
    "\n",
    "# Apply mask across all columns: retain a row only if all its entries are neither NaN nor Inf\n",
    "filtered_input_features_numpy = input_features_numpy[np.all(mask, axis=1)]\n",
    "t4_isFake_filtered = (np.concatenate(branches['t4_pMatched']) <= 0.75)[np.all(mask, axis=1)]\n",
    "t4_sim_vxy_filtered = np.concatenate(branches['t4_sim_vxy'])[np.all(mask, axis=1)]\n",
    "\n",
    "# Convert to PyTorch tensor when ready to use with NN\n",
    "input_features_tensor = torch.tensor(filtered_input_features_numpy, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total samples: 1946967\n",
      "Fake count: 1932985\n",
      "Real count: 13982\n",
      "Prompt count: 2190\n",
      "Displaced count: 11792\n",
      "Initial dataset size: 1946967\n",
      "Class distribution before downsampling - Fake: 1932985.0, Prompt: 2190.0, Displaced: 11792.0\n",
      "Class distribution after downsampling - Fake: 966492.0, Prompt: 2190.0, Displaced: 11792.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 0.8262, Test Loss: 0.7307\n",
      "Epoch [2/300], Train Loss: 0.6984, Test Loss: 0.6723\n",
      "Epoch [3/300], Train Loss: 0.6558, Test Loss: 0.6354\n",
      "Epoch [4/300], Train Loss: 0.6422, Test Loss: 0.6504\n",
      "Epoch [5/300], Train Loss: 0.6285, Test Loss: 0.6006\n",
      "Epoch [6/300], Train Loss: 0.5979, Test Loss: 0.5772\n",
      "Epoch [7/300], Train Loss: 0.5928, Test Loss: 0.5758\n",
      "Epoch [8/300], Train Loss: 0.5960, Test Loss: 0.5796\n",
      "Epoch [9/300], Train Loss: 0.5725, Test Loss: 0.5631\n",
      "Epoch [10/300], Train Loss: 0.5847, Test Loss: 0.5568\n",
      "Epoch [11/300], Train Loss: 0.5568, Test Loss: 0.5725\n",
      "Epoch [12/300], Train Loss: 0.5551, Test Loss: 0.5485\n",
      "Epoch [13/300], Train Loss: 0.5623, Test Loss: 0.5463\n",
      "Epoch [14/300], Train Loss: 0.5651, Test Loss: 0.5555\n",
      "Epoch [15/300], Train Loss: 0.5495, Test Loss: 0.5448\n",
      "Epoch [16/300], Train Loss: 0.5534, Test Loss: 0.5455\n",
      "Epoch [17/300], Train Loss: 0.5334, Test Loss: 0.6103\n",
      "Epoch [18/300], Train Loss: 0.5272, Test Loss: 0.5251\n",
      "Epoch [19/300], Train Loss: 0.5258, Test Loss: 0.5222\n",
      "Epoch [20/300], Train Loss: 0.5201, Test Loss: 0.5431\n",
      "Epoch [21/300], Train Loss: 0.5144, Test Loss: 0.5056\n",
      "Epoch [22/300], Train Loss: 0.5118, Test Loss: 0.5412\n",
      "Epoch [23/300], Train Loss: 0.4975, Test Loss: 0.5118\n",
      "Epoch [24/300], Train Loss: 0.4988, Test Loss: 0.5144\n",
      "Epoch [25/300], Train Loss: 0.4954, Test Loss: 0.5034\n",
      "Epoch [26/300], Train Loss: 0.4959, Test Loss: 0.5003\n",
      "Epoch [27/300], Train Loss: 0.4833, Test Loss: 0.4957\n",
      "Epoch [28/300], Train Loss: 0.4849, Test Loss: 0.5282\n",
      "Epoch [29/300], Train Loss: 0.4788, Test Loss: 0.4945\n",
      "Epoch [30/300], Train Loss: 0.4824, Test Loss: 0.4891\n",
      "Epoch [31/300], Train Loss: 0.4792, Test Loss: 0.4802\n",
      "Epoch [32/300], Train Loss: 0.4737, Test Loss: 0.5338\n",
      "Epoch [33/300], Train Loss: 0.4752, Test Loss: 0.5139\n",
      "Epoch [34/300], Train Loss: 0.4672, Test Loss: 0.4943\n",
      "Epoch [35/300], Train Loss: 0.4710, Test Loss: 0.5034\n",
      "Epoch [36/300], Train Loss: 0.4575, Test Loss: 0.5106\n",
      "Epoch [37/300], Train Loss: 0.4573, Test Loss: 0.4946\n",
      "Epoch [38/300], Train Loss: 0.4609, Test Loss: 0.4791\n",
      "Epoch [39/300], Train Loss: 0.4584, Test Loss: 0.4649\n",
      "Epoch [40/300], Train Loss: 0.4688, Test Loss: 0.4792\n",
      "Epoch [41/300], Train Loss: 0.4613, Test Loss: 0.4610\n",
      "Epoch [42/300], Train Loss: 0.4510, Test Loss: 0.4804\n",
      "Epoch [43/300], Train Loss: 0.4483, Test Loss: 0.4874\n",
      "Epoch [44/300], Train Loss: 0.4445, Test Loss: 0.4716\n",
      "Epoch [45/300], Train Loss: 0.4424, Test Loss: 0.4908\n",
      "Epoch [46/300], Train Loss: 0.4435, Test Loss: 0.4672\n",
      "Epoch [47/300], Train Loss: 0.4456, Test Loss: 0.4673\n",
      "Epoch [48/300], Train Loss: 0.4424, Test Loss: 0.4532\n",
      "Epoch [49/300], Train Loss: 0.4396, Test Loss: 0.4657\n",
      "Epoch [50/300], Train Loss: 0.4362, Test Loss: 0.5339\n",
      "Epoch [51/300], Train Loss: 0.4441, Test Loss: 0.4684\n",
      "Epoch [52/300], Train Loss: 0.4574, Test Loss: 0.4573\n",
      "Epoch [53/300], Train Loss: 0.4444, Test Loss: 0.4703\n",
      "Epoch [54/300], Train Loss: 0.4352, Test Loss: 0.4398\n",
      "Epoch [55/300], Train Loss: 0.4338, Test Loss: 0.4560\n",
      "Epoch [56/300], Train Loss: 0.4468, Test Loss: 0.4672\n",
      "Epoch [57/300], Train Loss: 0.4252, Test Loss: 0.4407\n",
      "Epoch [58/300], Train Loss: 0.4334, Test Loss: 0.4492\n",
      "Epoch [59/300], Train Loss: 0.4298, Test Loss: 0.4438\n",
      "Epoch [60/300], Train Loss: 0.4231, Test Loss: 0.4564\n",
      "Epoch [61/300], Train Loss: 0.4229, Test Loss: 0.4524\n",
      "Epoch [62/300], Train Loss: 0.4185, Test Loss: 0.4634\n",
      "Epoch [63/300], Train Loss: 0.4166, Test Loss: 0.4557\n",
      "Epoch [64/300], Train Loss: 0.4214, Test Loss: 0.4489\n",
      "Epoch [65/300], Train Loss: 0.4159, Test Loss: 0.4627\n",
      "Epoch [66/300], Train Loss: 0.4113, Test Loss: 0.4788\n",
      "Epoch [67/300], Train Loss: 0.4130, Test Loss: 0.4512\n",
      "Epoch [68/300], Train Loss: 0.4114, Test Loss: 0.4509\n",
      "Epoch [69/300], Train Loss: 0.4108, Test Loss: 0.4716\n",
      "Epoch [70/300], Train Loss: 0.4154, Test Loss: 0.4326\n",
      "Epoch [71/300], Train Loss: 0.4147, Test Loss: 0.4302\n",
      "Epoch [72/300], Train Loss: 0.4247, Test Loss: 0.5130\n",
      "Epoch [73/300], Train Loss: 0.4078, Test Loss: 0.4326\n",
      "Epoch [74/300], Train Loss: 0.4171, Test Loss: 0.4450\n",
      "Epoch [75/300], Train Loss: 0.4153, Test Loss: 0.4334\n",
      "Epoch [76/300], Train Loss: 0.4270, Test Loss: 0.4437\n",
      "Epoch [77/300], Train Loss: 0.4012, Test Loss: 0.4235\n",
      "Epoch [78/300], Train Loss: 0.4010, Test Loss: 0.4442\n",
      "Epoch [79/300], Train Loss: 0.4078, Test Loss: 0.4476\n",
      "Epoch [80/300], Train Loss: 0.4240, Test Loss: 0.4310\n",
      "Epoch [81/300], Train Loss: 0.4213, Test Loss: 0.4574\n",
      "Epoch [82/300], Train Loss: 0.4130, Test Loss: 0.4812\n",
      "Epoch [83/300], Train Loss: 0.3974, Test Loss: 0.4349\n",
      "Epoch [84/300], Train Loss: 0.3932, Test Loss: 0.4436\n",
      "Epoch [85/300], Train Loss: 0.3967, Test Loss: 0.4406\n",
      "Epoch [86/300], Train Loss: 0.3921, Test Loss: 0.4468\n",
      "Epoch [87/300], Train Loss: 0.3930, Test Loss: 0.4496\n",
      "Epoch [88/300], Train Loss: 0.3922, Test Loss: 0.4462\n",
      "Epoch [89/300], Train Loss: 0.4134, Test Loss: 0.4289\n",
      "Epoch [90/300], Train Loss: 0.4080, Test Loss: 0.4400\n",
      "Epoch [91/300], Train Loss: 0.4070, Test Loss: 0.4224\n",
      "Epoch [92/300], Train Loss: 0.3892, Test Loss: 0.4594\n",
      "Epoch [93/300], Train Loss: 0.4063, Test Loss: 0.4472\n",
      "Epoch [94/300], Train Loss: 0.3997, Test Loss: 0.4308\n",
      "Epoch [95/300], Train Loss: 0.4149, Test Loss: 0.5144\n",
      "Epoch [96/300], Train Loss: 0.4140, Test Loss: 0.4127\n",
      "Epoch [97/300], Train Loss: 0.3880, Test Loss: 0.4359\n",
      "Epoch [98/300], Train Loss: 0.4036, Test Loss: 0.4353\n",
      "Epoch [99/300], Train Loss: 0.4043, Test Loss: 0.4192\n",
      "Epoch [100/300], Train Loss: 0.4127, Test Loss: 0.4683\n",
      "Epoch [101/300], Train Loss: 0.4072, Test Loss: 0.4290\n",
      "Epoch [102/300], Train Loss: 0.3958, Test Loss: 0.4598\n",
      "Epoch [103/300], Train Loss: 0.4020, Test Loss: 0.4302\n",
      "Epoch [104/300], Train Loss: 0.4015, Test Loss: 0.4803\n",
      "Epoch [105/300], Train Loss: 0.3887, Test Loss: 0.4258\n",
      "Epoch [106/300], Train Loss: 0.3788, Test Loss: 0.4279\n",
      "Epoch [107/300], Train Loss: 0.3901, Test Loss: 0.4457\n",
      "Epoch [108/300], Train Loss: 0.3904, Test Loss: 0.4271\n",
      "Epoch [109/300], Train Loss: 0.3809, Test Loss: 0.4260\n",
      "Epoch [110/300], Train Loss: 0.3849, Test Loss: 0.4127\n",
      "Epoch [111/300], Train Loss: 0.3864, Test Loss: 0.4762\n",
      "Epoch [112/300], Train Loss: 0.3832, Test Loss: 0.4473\n",
      "Epoch [113/300], Train Loss: 0.3796, Test Loss: 0.4374\n",
      "Epoch [114/300], Train Loss: 0.3783, Test Loss: 0.4334\n",
      "Epoch [115/300], Train Loss: 0.3831, Test Loss: 0.4108\n",
      "Epoch [116/300], Train Loss: 0.3758, Test Loss: 0.4429\n",
      "Epoch [117/300], Train Loss: 0.3785, Test Loss: 0.4300\n",
      "Epoch [118/300], Train Loss: 0.3739, Test Loss: 0.4449\n",
      "Epoch [119/300], Train Loss: 0.3740, Test Loss: 0.4305\n",
      "Epoch [120/300], Train Loss: 0.3743, Test Loss: 0.4437\n",
      "Epoch [121/300], Train Loss: 0.3741, Test Loss: 0.4362\n",
      "Epoch [122/300], Train Loss: 0.3708, Test Loss: 0.4114\n",
      "Epoch [123/300], Train Loss: 0.3679, Test Loss: 0.4234\n",
      "Epoch [124/300], Train Loss: 0.3661, Test Loss: 0.5044\n",
      "Epoch [125/300], Train Loss: 0.3712, Test Loss: 0.4327\n",
      "Epoch [126/300], Train Loss: 0.3703, Test Loss: 0.4273\n",
      "Epoch [127/300], Train Loss: 0.3901, Test Loss: 0.4462\n",
      "Epoch [128/300], Train Loss: 0.3758, Test Loss: 0.4272\n",
      "Epoch [129/300], Train Loss: 0.3759, Test Loss: 0.4172\n",
      "Epoch [130/300], Train Loss: 0.3739, Test Loss: 0.4229\n",
      "Epoch [131/300], Train Loss: 0.3668, Test Loss: 0.4256\n",
      "Epoch [132/300], Train Loss: 0.3663, Test Loss: 0.4326\n",
      "Epoch [133/300], Train Loss: 0.3726, Test Loss: 0.4198\n",
      "Epoch [134/300], Train Loss: 0.3748, Test Loss: 0.4201\n",
      "Epoch [135/300], Train Loss: 0.3669, Test Loss: 0.4145\n",
      "Epoch [136/300], Train Loss: 0.3636, Test Loss: 0.4297\n",
      "Epoch [137/300], Train Loss: 0.3592, Test Loss: 0.4185\n",
      "Epoch [138/300], Train Loss: 0.3656, Test Loss: 0.4440\n",
      "Epoch [139/300], Train Loss: 0.3650, Test Loss: 0.4338\n",
      "Epoch [140/300], Train Loss: 0.3679, Test Loss: 0.4335\n",
      "Epoch [141/300], Train Loss: 0.3597, Test Loss: 0.4269\n",
      "Epoch [142/300], Train Loss: 0.3618, Test Loss: 0.4381\n",
      "Epoch [143/300], Train Loss: 0.3720, Test Loss: 0.4318\n",
      "Epoch [144/300], Train Loss: 0.3701, Test Loss: 0.4254\n",
      "Epoch [145/300], Train Loss: 0.3643, Test Loss: 0.4900\n",
      "Epoch [146/300], Train Loss: 0.3736, Test Loss: 0.4397\n",
      "Epoch [147/300], Train Loss: 0.3625, Test Loss: 0.4248\n",
      "Epoch [148/300], Train Loss: 0.3519, Test Loss: 0.4480\n",
      "Epoch [149/300], Train Loss: 0.3556, Test Loss: 0.4299\n",
      "Epoch [150/300], Train Loss: 0.3556, Test Loss: 0.4094\n",
      "Epoch [151/300], Train Loss: 0.3525, Test Loss: 0.4243\n",
      "Epoch [152/300], Train Loss: 0.3526, Test Loss: 0.4035\n",
      "Epoch [153/300], Train Loss: 0.3504, Test Loss: 0.4089\n",
      "Epoch [154/300], Train Loss: 0.3539, Test Loss: 0.4252\n",
      "Epoch [155/300], Train Loss: 0.3576, Test Loss: 0.4084\n",
      "Epoch [156/300], Train Loss: 0.3520, Test Loss: 0.4198\n",
      "Epoch [157/300], Train Loss: 0.3495, Test Loss: 0.4285\n",
      "Epoch [158/300], Train Loss: 0.3499, Test Loss: 0.4082\n",
      "Epoch [159/300], Train Loss: 0.3475, Test Loss: 0.4138\n",
      "Epoch [160/300], Train Loss: 0.3461, Test Loss: 0.4153\n",
      "Epoch [161/300], Train Loss: 0.3513, Test Loss: 0.4413\n",
      "Epoch [162/300], Train Loss: 0.3505, Test Loss: 0.4231\n",
      "Epoch [163/300], Train Loss: 0.3615, Test Loss: 0.4049\n",
      "Epoch [164/300], Train Loss: 0.3439, Test Loss: 0.4157\n",
      "Epoch [165/300], Train Loss: 0.3502, Test Loss: 0.4159\n",
      "Epoch [166/300], Train Loss: 0.3454, Test Loss: 0.4066\n",
      "Epoch [167/300], Train Loss: 0.3453, Test Loss: 0.4293\n",
      "Epoch [168/300], Train Loss: 0.3484, Test Loss: 0.4205\n",
      "Epoch [169/300], Train Loss: 0.3466, Test Loss: 0.4128\n",
      "Epoch [170/300], Train Loss: 0.3473, Test Loss: 0.4063\n",
      "Epoch [171/300], Train Loss: 0.3396, Test Loss: 0.4006\n",
      "Epoch [172/300], Train Loss: 0.3476, Test Loss: 0.4058\n",
      "Epoch [173/300], Train Loss: 0.3449, Test Loss: 0.4076\n",
      "Epoch [174/300], Train Loss: 0.3450, Test Loss: 0.4103\n",
      "Epoch [175/300], Train Loss: 0.3403, Test Loss: 0.4116\n",
      "Epoch [176/300], Train Loss: 0.3432, Test Loss: 0.4331\n",
      "Epoch [177/300], Train Loss: 0.3403, Test Loss: 0.4116\n",
      "Epoch [178/300], Train Loss: 0.3400, Test Loss: 0.4104\n",
      "Epoch [179/300], Train Loss: 0.3396, Test Loss: 0.4070\n",
      "Epoch [180/300], Train Loss: 0.3407, Test Loss: 0.4206\n",
      "Epoch [181/300], Train Loss: 0.3399, Test Loss: 0.4223\n",
      "Epoch [182/300], Train Loss: 0.3404, Test Loss: 0.4279\n",
      "Epoch [183/300], Train Loss: 0.3392, Test Loss: 0.4018\n",
      "Epoch [184/300], Train Loss: 0.3459, Test Loss: 0.4183\n",
      "Epoch [185/300], Train Loss: 0.3385, Test Loss: 0.4166\n",
      "Epoch [186/300], Train Loss: 0.3394, Test Loss: 0.4159\n",
      "Epoch [187/300], Train Loss: 0.3373, Test Loss: 0.4247\n",
      "Epoch [188/300], Train Loss: 0.3413, Test Loss: 0.4185\n",
      "Epoch [189/300], Train Loss: 0.3387, Test Loss: 0.4141\n",
      "Epoch [190/300], Train Loss: 0.3391, Test Loss: 0.4188\n",
      "Epoch [191/300], Train Loss: 0.3350, Test Loss: 0.4356\n",
      "Epoch [192/300], Train Loss: 0.3351, Test Loss: 0.4201\n",
      "Epoch [193/300], Train Loss: 0.3375, Test Loss: 0.4211\n",
      "Epoch [194/300], Train Loss: 0.3372, Test Loss: 0.4053\n",
      "Epoch [195/300], Train Loss: 0.3351, Test Loss: 0.4102\n",
      "Epoch [196/300], Train Loss: 0.3372, Test Loss: 0.4055\n",
      "Epoch [197/300], Train Loss: 0.3351, Test Loss: 0.4109\n",
      "Epoch [198/300], Train Loss: 0.3354, Test Loss: 0.4085\n",
      "Epoch [199/300], Train Loss: 0.3372, Test Loss: 0.4105\n",
      "Epoch [200/300], Train Loss: 0.3377, Test Loss: 0.4089\n",
      "Epoch [201/300], Train Loss: 0.3372, Test Loss: 0.4181\n",
      "Epoch [202/300], Train Loss: 0.3372, Test Loss: 0.4441\n",
      "Epoch [203/300], Train Loss: 0.3332, Test Loss: 0.4079\n",
      "Epoch [204/300], Train Loss: 0.3342, Test Loss: 0.4259\n",
      "Epoch [205/300], Train Loss: 0.3423, Test Loss: 0.4104\n",
      "Epoch [206/300], Train Loss: 0.3342, Test Loss: 0.4055\n",
      "Epoch [207/300], Train Loss: 0.3396, Test Loss: 0.4037\n",
      "Epoch [208/300], Train Loss: 0.3294, Test Loss: 0.4051\n",
      "Epoch [209/300], Train Loss: 0.3330, Test Loss: 0.4080\n",
      "Epoch [210/300], Train Loss: 0.3363, Test Loss: 0.4226\n",
      "Epoch [211/300], Train Loss: 0.3302, Test Loss: 0.4197\n",
      "Epoch [212/300], Train Loss: 0.3328, Test Loss: 0.4133\n",
      "Epoch [213/300], Train Loss: 0.3328, Test Loss: 0.4245\n",
      "Epoch [214/300], Train Loss: 0.3282, Test Loss: 0.4323\n",
      "Epoch [215/300], Train Loss: 0.3336, Test Loss: 0.4349\n",
      "Epoch [216/300], Train Loss: 0.3318, Test Loss: 0.4043\n",
      "Epoch [217/300], Train Loss: 0.3299, Test Loss: 0.4074\n",
      "Epoch [218/300], Train Loss: 0.3316, Test Loss: 0.4121\n",
      "Epoch [219/300], Train Loss: 0.3327, Test Loss: 0.4349\n",
      "Epoch [220/300], Train Loss: 0.3291, Test Loss: 0.4175\n",
      "Epoch [221/300], Train Loss: 0.3296, Test Loss: 0.4422\n",
      "Epoch [222/300], Train Loss: 0.3321, Test Loss: 0.4050\n",
      "Epoch [223/300], Train Loss: 0.3332, Test Loss: 0.4203\n",
      "Epoch [224/300], Train Loss: 0.3320, Test Loss: 0.4201\n",
      "Epoch [225/300], Train Loss: 0.3307, Test Loss: 0.4229\n",
      "Epoch [226/300], Train Loss: 0.3342, Test Loss: 0.4133\n",
      "Epoch [227/300], Train Loss: 0.3264, Test Loss: 0.4063\n",
      "Epoch [228/300], Train Loss: 0.3394, Test Loss: 0.4537\n",
      "Epoch [229/300], Train Loss: 0.3335, Test Loss: 0.4053\n",
      "Epoch [230/300], Train Loss: 0.3278, Test Loss: 0.4050\n",
      "Epoch [231/300], Train Loss: 0.3310, Test Loss: 0.4099\n",
      "Epoch [232/300], Train Loss: 0.3313, Test Loss: 0.4339\n",
      "Epoch [233/300], Train Loss: 0.3270, Test Loss: 0.4010\n",
      "Epoch [234/300], Train Loss: 0.3248, Test Loss: 0.4435\n",
      "Epoch [235/300], Train Loss: 0.3309, Test Loss: 0.4129\n",
      "Epoch [236/300], Train Loss: 0.3304, Test Loss: 0.4043\n",
      "Epoch [237/300], Train Loss: 0.3278, Test Loss: 0.4281\n",
      "Epoch [238/300], Train Loss: 0.3266, Test Loss: 0.4177\n",
      "Epoch [239/300], Train Loss: 0.3267, Test Loss: 0.4099\n",
      "Epoch [240/300], Train Loss: 0.3273, Test Loss: 0.4351\n",
      "Epoch [241/300], Train Loss: 0.3263, Test Loss: 0.4138\n",
      "Epoch [242/300], Train Loss: 0.3266, Test Loss: 0.4077\n",
      "Epoch [243/300], Train Loss: 0.3242, Test Loss: 0.4124\n",
      "Epoch [244/300], Train Loss: 0.3222, Test Loss: 0.4295\n",
      "Epoch [245/300], Train Loss: 0.3231, Test Loss: 0.4194\n",
      "Epoch [246/300], Train Loss: 0.3260, Test Loss: 0.4018\n",
      "Epoch [247/300], Train Loss: 0.3247, Test Loss: 0.4260\n",
      "Epoch [248/300], Train Loss: 0.3244, Test Loss: 0.4250\n",
      "Epoch [249/300], Train Loss: 0.3222, Test Loss: 0.4036\n",
      "Epoch [250/300], Train Loss: 0.3214, Test Loss: 0.4194\n",
      "Epoch [251/300], Train Loss: 0.3288, Test Loss: 0.4402\n",
      "Epoch [252/300], Train Loss: 0.3243, Test Loss: 0.4092\n",
      "Epoch [253/300], Train Loss: 0.3195, Test Loss: 0.4114\n",
      "Epoch [254/300], Train Loss: 0.3278, Test Loss: 0.4083\n",
      "Epoch [255/300], Train Loss: 0.3194, Test Loss: 0.4039\n",
      "Epoch [256/300], Train Loss: 0.3190, Test Loss: 0.4172\n",
      "Epoch [257/300], Train Loss: 0.3209, Test Loss: 0.4165\n",
      "Epoch [258/300], Train Loss: 0.3220, Test Loss: 0.4042\n",
      "Epoch [259/300], Train Loss: 0.3259, Test Loss: 0.4143\n",
      "Epoch [260/300], Train Loss: 0.3220, Test Loss: 0.4093\n",
      "Epoch [261/300], Train Loss: 0.3196, Test Loss: 0.4037\n",
      "Epoch [262/300], Train Loss: 0.3234, Test Loss: 0.4045\n",
      "Epoch [263/300], Train Loss: 0.3253, Test Loss: 0.4052\n",
      "Epoch [264/300], Train Loss: 0.3191, Test Loss: 0.4276\n",
      "Epoch [265/300], Train Loss: 0.3237, Test Loss: 0.4100\n",
      "Epoch [266/300], Train Loss: 0.3177, Test Loss: 0.4121\n",
      "Epoch [267/300], Train Loss: 0.3267, Test Loss: 0.4139\n",
      "Epoch [268/300], Train Loss: 0.3169, Test Loss: 0.4129\n",
      "Epoch [269/300], Train Loss: 0.3194, Test Loss: 0.3981\n",
      "Epoch [270/300], Train Loss: 0.3198, Test Loss: 0.4082\n",
      "Epoch [271/300], Train Loss: 0.3218, Test Loss: 0.4116\n",
      "Epoch [272/300], Train Loss: 0.3179, Test Loss: 0.4261\n",
      "Epoch [273/300], Train Loss: 0.3218, Test Loss: 0.4040\n",
      "Epoch [274/300], Train Loss: 0.3193, Test Loss: 0.4003\n",
      "Epoch [275/300], Train Loss: 0.3216, Test Loss: 0.4119\n",
      "Epoch [276/300], Train Loss: 0.3163, Test Loss: 0.4251\n",
      "Epoch [277/300], Train Loss: 0.3235, Test Loss: 0.4140\n",
      "Epoch [278/300], Train Loss: 0.3131, Test Loss: 0.4133\n",
      "Epoch [279/300], Train Loss: 0.3159, Test Loss: 0.4344\n",
      "Epoch [280/300], Train Loss: 0.3205, Test Loss: 0.4107\n",
      "Epoch [281/300], Train Loss: 0.3182, Test Loss: 0.4140\n",
      "Epoch [282/300], Train Loss: 0.3175, Test Loss: 0.4070\n",
      "Epoch [283/300], Train Loss: 0.3165, Test Loss: 0.4210\n",
      "Epoch [284/300], Train Loss: 0.3131, Test Loss: 0.4466\n",
      "Epoch [285/300], Train Loss: 0.3196, Test Loss: 0.4135\n",
      "Epoch [286/300], Train Loss: 0.3156, Test Loss: 0.4003\n",
      "Epoch [287/300], Train Loss: 0.3134, Test Loss: 0.4294\n",
      "Epoch [288/300], Train Loss: 0.3205, Test Loss: 0.4209\n",
      "Epoch [289/300], Train Loss: 0.3153, Test Loss: 0.4058\n",
      "Epoch [290/300], Train Loss: 0.3174, Test Loss: 0.4105\n",
      "Epoch [291/300], Train Loss: 0.3142, Test Loss: 0.4304\n",
      "Epoch [292/300], Train Loss: 0.3162, Test Loss: 0.4192\n",
      "Epoch [293/300], Train Loss: 0.3154, Test Loss: 0.4096\n",
      "Epoch [294/300], Train Loss: 0.3162, Test Loss: 0.4072\n",
      "Epoch [295/300], Train Loss: 0.3135, Test Loss: 0.4309\n",
      "Epoch [296/300], Train Loss: 0.3187, Test Loss: 0.4078\n",
      "Epoch [297/300], Train Loss: 0.3118, Test Loss: 0.4294\n",
      "Epoch [298/300], Train Loss: 0.3129, Test Loss: 0.4383\n",
      "Epoch [299/300], Train Loss: 0.3192, Test Loss: 0.4008\n",
      "Epoch [300/300], Train Loss: 0.3121, Test Loss: 0.4246\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create labels tensor\n",
    "def create_multiclass_labels(t4_isFake, t4_sim_vxy, displacement_threshold=0.1):\n",
    "    num_samples = len(t4_isFake)\n",
    "    labels = torch.zeros((num_samples, 3))\n",
    "    \n",
    "    # Fake tracks (class 0)\n",
    "    fake_mask = t4_isFake\n",
    "    labels[fake_mask, 0] = 1\n",
    "    \n",
    "    # Real tracks\n",
    "    real_mask = ~fake_mask \n",
    "    \n",
    "    # Split real tracks into prompt (class 1) and displaced (class 2)\n",
    "    prompt_mask = (t4_sim_vxy <= displacement_threshold) & real_mask\n",
    "    displaced_mask = (t4_sim_vxy > displacement_threshold) & real_mask\n",
    "    \n",
    "    labels[prompt_mask, 1] = 1\n",
    "    labels[displaced_mask, 2] = 1\n",
    "\n",
    "    print(f\"Total samples: {num_samples}\")\n",
    "    print(f\"Fake count: {fake_mask.sum().item()}\")\n",
    "    print(f\"Real count: {real_mask.sum().item()}\")\n",
    "    print(f\"Prompt count: {prompt_mask.sum().item()}\")\n",
    "    print(f\"Displaced count: {displaced_mask.sum().item()}\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "labels_tensor = create_multiclass_labels(\n",
    "    t4_isFake_filtered,\n",
    "    t4_sim_vxy_filtered\n",
    ")\n",
    "\n",
    "# Neural network for multi-class classification\n",
    "class MultiClassNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassNeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_features_numpy.shape[1], 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.output_layer = nn.Linear(32, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.output_layer(x)\n",
    "        return nn.functional.softmax(x, dim=1)\n",
    "\n",
    "# Weighted loss function for multi-class\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedCrossEntropyLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, outputs, targets, weights):\n",
    "        eps = 1e-7\n",
    "        log_probs = torch.log(outputs + eps)\n",
    "        losses = -weights * torch.sum(targets * log_probs, dim=1)\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "# Calculate class weights (each sample gets a weight to equalize class contributions)\n",
    "def calculate_class_weights(labels):\n",
    "    class_counts = torch.sum(labels, dim=0)\n",
    "    total_samples = len(labels)\n",
    "    class_weights = total_samples / (3 * class_counts)  # Normalize across 3 classes\n",
    "    \n",
    "    sample_weights = torch.zeros(len(labels))\n",
    "    for i in range(3):\n",
    "        sample_weights[labels[:, i] == 1] = class_weights[i]\n",
    "    \n",
    "    return sample_weights\n",
    "\n",
    "# Print initial dataset size\n",
    "print(f\"Initial dataset size: {len(labels_tensor)}\")\n",
    "\n",
    "# Remove rows with NaN and update weights accordingly\n",
    "nan_mask = torch.isnan(input_features_tensor).any(dim=1) | torch.isnan(labels_tensor).any(dim=1)\n",
    "filtered_inputs = input_features_tensor[~nan_mask]\n",
    "filtered_labels = labels_tensor[~nan_mask]\n",
    "\n",
    "# Count samples in each class before downsampling\n",
    "class_counts_before = torch.sum(filtered_labels, dim=0)\n",
    "print(f\"Class distribution before downsampling - Fake: {class_counts_before[0]}, Prompt: {class_counts_before[1]}, Displaced: {class_counts_before[2]}\")\n",
    "\n",
    "# Option to downsample each class (binary-class)\n",
    "downsample_classes = True  # Set to False to disable downsampling\n",
    "if downsample_classes:\n",
    "    # Define downsampling ratios for each class:\n",
    "    # For example, downsample fakes (class 0) to 50% and keep prompt (class 1) and displaced (class 2) at 100%\n",
    "    downsample_ratios = {0: 0.5, 1: 1.0, 2: 1.0}\n",
    "    indices_list = []\n",
    "    for cls in range(3):\n",
    "        # Find indices for the current class\n",
    "        cls_mask = (filtered_labels[:, cls] == 1)\n",
    "        cls_indices = torch.nonzero(cls_mask).squeeze()\n",
    "        ratio = downsample_ratios.get(cls, 1.0)\n",
    "        num_cls = cls_indices.numel()\n",
    "        num_to_sample = int(num_cls * ratio)\n",
    "        # Ensure at least one sample is kept if available\n",
    "        if num_to_sample < 1 and num_cls > 0:\n",
    "            num_to_sample = 1\n",
    "        # Shuffle and select the desired number of samples\n",
    "        cls_indices_shuffled = cls_indices[torch.randperm(num_cls)]\n",
    "        sampled_cls_indices = cls_indices_shuffled[:num_to_sample]\n",
    "        indices_list.append(sampled_cls_indices)\n",
    "    \n",
    "    # Combine the indices from all classes\n",
    "    selected_indices = torch.cat(indices_list)\n",
    "    filtered_inputs = filtered_inputs[selected_indices]\n",
    "    filtered_labels = filtered_labels[selected_indices]\n",
    "\n",
    "# Print class distribution after downsampling\n",
    "class_counts_after = torch.sum(filtered_labels, dim=0)\n",
    "print(f\"Class distribution after downsampling - Fake: {class_counts_after[0]}, Prompt: {class_counts_after[1]}, Displaced: {class_counts_after[2]}\")\n",
    "\n",
    "# Recalculate sample weights after downsampling (equal weighting per class based on new counts)\n",
    "sample_weights = calculate_class_weights(filtered_labels)\n",
    "filtered_weights = sample_weights    \n",
    "\n",
    "# Create dataset with weights\n",
    "dataset = TensorDataset(filtered_inputs, filtered_labels, filtered_weights)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=10, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False, num_workers=10, pin_memory=True)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = MultiClassNeuralNetwork().to(device)\n",
    "loss_function = WeightedCrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "def evaluate_loss(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, weights in loader:\n",
    "            inputs, targets, weights = inputs.to(device), targets.to(device), weights.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets, weights)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "train_loss_log = []\n",
    "test_loss_log = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for inputs, targets, weights in train_loader:\n",
    "        inputs, targets, weights = inputs.to(device), targets.to(device), weights.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, targets, weights)\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss = epoch_loss / num_batches\n",
    "    test_loss = evaluate_loss(test_loader)\n",
    "    \n",
    "    train_loss_log.append(train_loss)\n",
    "    test_loss_log.append(test_loss)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.8717\n",
      "\n",
      "Feature importances:\n",
      "Feature 28 importance: 0.0235\n",
      "Feature 27 importance: 0.0229\n",
      "Feature 0 importance: 0.0202\n",
      "Feature 14 importance: 0.0136\n",
      "Feature 18 importance: 0.0093\n",
      "Feature 6 importance: 0.0086\n",
      "Feature 16 importance: 0.0086\n",
      "Feature 23 importance: 0.0068\n",
      "Feature 24 importance: 0.0062\n",
      "Feature 13 importance: 0.0047\n",
      "Feature 17 importance: 0.0040\n",
      "Feature 11 importance: 0.0034\n",
      "Feature 22 importance: 0.0031\n",
      "Feature 5 importance: 0.0023\n",
      "Feature 25 importance: 0.0023\n",
      "Feature 15 importance: 0.0011\n",
      "Feature 10 importance: 0.0010\n",
      "Feature 20 importance: 0.0008\n",
      "Feature 1 importance: 0.0003\n",
      "Feature 19 importance: 0.0001\n",
      "Feature 8 importance: 0.0000\n",
      "Feature 21 importance: -0.0001\n",
      "Feature 2 importance: -0.0005\n",
      "Feature 9 importance: -0.0006\n",
      "Feature 3 importance: -0.0009\n",
      "Feature 7 importance: -0.0026\n",
      "Feature 29 importance: -0.0026\n",
      "Feature 26 importance: -0.0030\n",
      "Feature 12 importance: -0.0035\n",
      "Feature 4 importance: -0.0044\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Convert tensors to numpy for simplicity if you want to manipulate them outside of PyTorch\n",
    "input_features_np = input_features_tensor.numpy()\n",
    "labels_np = torch.argmax(labels_tensor, dim=1).numpy()  # Convert one-hot to class indices\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def model_accuracy(features, labels, model):\n",
    "    \"\"\"\n",
    "    Compute accuracy for a multi-class classification model\n",
    "    that outputs probabilities of size [batch_size, num_classes].\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Move the features and labels to the correct device\n",
    "    inputs = features.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)  # shape: [batch_size, num_classes]\n",
    "        # For multi-class, the predicted class is argmax of the probabilities\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        # Convert one-hot encoded labels to class indices if needed\n",
    "        if len(labels.shape) > 1:\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "        # Compute mean accuracy\n",
    "        accuracy = (predicted == labels).float().mean().item()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Compute baseline accuracy\n",
    "baseline_accuracy = model_accuracy(input_features_tensor, labels_tensor, model)\n",
    "print(f\"Baseline accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "# Initialize array to store feature importances\n",
    "feature_importances = np.zeros(input_features_tensor.shape[1])\n",
    "\n",
    "# Iterate over each feature for permutation importance\n",
    "for i in range(input_features_tensor.shape[1]):\n",
    "    # Create a copy of the original features\n",
    "    permuted_features = input_features_tensor.clone()\n",
    "    \n",
    "    # Permute feature i across all examples\n",
    "    # We do this by shuffling the rows for that specific column\n",
    "    permuted_features[:, i] = permuted_features[torch.randperm(permuted_features.size(0)), i]\n",
    "    \n",
    "    # Compute accuracy after permutation\n",
    "    permuted_accuracy = model_accuracy(permuted_features, labels_tensor, model)\n",
    "    \n",
    "    # The drop in accuracy is used as a measure of feature importance\n",
    "    feature_importances[i] = baseline_accuracy - permuted_accuracy\n",
    "\n",
    "# Sort features by descending importance\n",
    "important_features_indices = np.argsort(feature_importances)[::-1]\n",
    "important_features_scores = np.sort(feature_importances)[::-1]\n",
    "\n",
    "# Print out results\n",
    "print(\"\\nFeature importances:\")\n",
    "for idx, score in zip(important_features_indices, important_features_scores):\n",
    "    print(f\"Feature {idx} importance: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float bias_layer1[32] = {\n",
      "0.3130181f, -0.3157252f, -0.0845900f, -0.2268437f, 0.1305549f, 0.3839142f, 0.3933745f, -0.6758229f, -0.4188058f, -0.2523611f, 1.4036129f, 0.8239079f, 0.1575654f, 0.2041763f, 0.8787493f, 0.2706699f, -0.1112185f, 0.8988609f, 0.9274163f, -0.1023219f, 0.2916122f, -0.2606929f, 0.3098971f, -0.0602703f, -0.6031470f, -0.0826582f, 0.3605700f, 0.4836628f, -0.3951748f, 0.0171050f, 0.5156327f, 0.0655813f };\n",
      "\n",
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float wgtT_layer1[30][32] = {\n",
      "{ -0.3409404f, -0.2000102f, -0.0890483f, -0.6186467f, -1.7570605f, 0.7890699f, -0.8753229f, 0.5488843f, -0.5376814f, -0.2228569f, -0.3573552f, 2.1554949f, 0.2248887f, -0.6073594f, 0.2075009f, -0.1408760f, -0.7051892f, -0.0664303f, 0.2747473f, 0.1450685f, -2.2709231f, -0.4088669f, 0.5452566f, 0.3086576f, -0.1213564f, -0.9737034f, 0.2679004f, -0.1193755f, 0.9693206f, -0.7785844f, 0.4612639f, 0.7628022f },\n",
      "{ 0.0309823f, -0.5052885f, 0.0509167f, 0.1379152f, 0.2392345f, 0.0935747f, 0.1001187f, 0.2049023f, -0.1292204f, -0.1732666f, 0.0397899f, -0.1621571f, -0.5934961f, 0.1731108f, -0.0290751f, 0.1774067f, 0.0519257f, 0.0035256f, -0.0188142f, -0.0639332f, -0.2388456f, 0.0038807f, 0.0088869f, -0.1521942f, 0.0138392f, 0.0613728f, -0.0693704f, 0.0792511f, -0.0990796f, -0.4666552f, 0.0071133f, -0.1573301f },\n",
      "{ 0.0850391f, 0.1398510f, 0.7312427f, 1.6511540f, -0.1157119f, 1.8312333f, 0.4276405f, 0.1428780f, -0.0762665f, -0.1204791f, 0.5231065f, -1.9012266f, -0.2759933f, 2.1613867f, -0.3789352f, -0.7384162f, 0.9980950f, -1.3757244f, -0.3345136f, -0.1468498f, -0.8294499f, 0.0777341f, 1.4606416f, 0.5440134f, 2.1587088f, 0.0192866f, -4.7703443f, -0.3778406f, 2.0303624f, 0.4716987f, -0.5599666f, 1.0512540f },\n",
      "{ 0.1225046f, 0.1352115f, -0.6140373f, -0.4254693f, 0.2660460f, -0.7739570f, 1.0955174f, 1.2222520f, -0.1673346f, -0.0774871f, 0.4850340f, -0.2480809f, 2.9553666f, -0.6026165f, -0.5579752f, -0.3300077f, 0.0972013f, 1.9986047f, 0.9151404f, 0.0942718f, -0.0422994f, -0.0338943f, -0.3063800f, -0.6567743f, 2.2631214f, 0.6305654f, 0.2954915f, 0.8076106f, 0.7743834f, 0.7023419f, -1.6891261f, -0.6115909f },\n",
      "{ 2.2928166f, 8.8309431f, -1.5345458f, -4.7059007f, 1.2810588f, -0.4951739f, 4.4845004f, 7.7040706f, -1.8243797f, 0.0956211f, -0.3696172f, 0.6678835f, 4.7292924f, -9.7971382f, 2.0854435f, -3.7175915f, 3.1399553f, -0.6623941f, 0.1497208f, -0.0905410f, -3.9599996f, 3.3040602f, 2.2535894f, -8.0852966f, -16.3342419f, 14.6141891f, 2.6647313f, -10.8139477f, 0.5194562f, -2.2337329f, 1.4479593f, 10.4768066f },\n",
      "{ 6.3267903f, 8.4862614f, 18.2483406f, -2.5856876f, 2.9871955f, 6.0332689f, -11.9952469f, -1.8413588f, 1.3217239f, -0.1347535f, 0.8526750f, -4.6671519f, 1.7776268f, 0.2549058f, -14.5247345f, 3.1438231f, -9.3088989f, 1.1253707f, 0.0789171f, 0.1527994f, 1.7165481f, -0.9345309f, -5.6528535f, -9.5160980f, -1.0419953f, -1.0004028f, 1.2163434f, 6.5496387f, 0.0804405f, -6.1611404f, -9.2283039f, -5.5179152f },\n",
      "{ -2.3399000f, 4.7614522f, -0.7949132f, -1.8299819f, -3.5392070f, -0.0867105f, -0.6724365f, 0.4369151f, 5.0728159f, -0.2512915f, 0.8541925f, -0.0066838f, -3.1859064f, -1.5541859f, 0.0789470f, -1.3237801f, 2.6402714f, -0.0662765f, -0.2674660f, 0.0608886f, 1.1315312f, -0.1070065f, 1.8663841f, -0.4901078f, -1.0676215f, 1.3194273f, -1.2205451f, -1.0945961f, 0.2475978f, 0.9682984f, 0.2073012f, 1.7876559f },\n",
      "{ -0.4367641f, 0.4319819f, 1.1654582f, -3.1896923f, -1.2711153f, -0.6044747f, -3.3841281f, 0.1955531f, 4.4088063f, -0.1708138f, -0.3717940f, 0.1636910f, -2.8220074f, -0.1908980f, 1.0748017f, -0.3119625f, -1.5694339f, -0.1778283f, 0.9781732f, -0.0754240f, 4.2073040f, 2.0899282f, 1.2668608f, 0.0779037f, -1.9252455f, 2.5578146f, -0.9014751f, 0.0522716f, 0.4070499f, -0.2140355f, 0.7342044f, 0.8045168f },\n",
      "{ -6.6071205f, 0.6651391f, -5.2370253f, 5.0692110f, 0.7510651f, 2.4857941f, 11.5648117f, -1.7613629f, 1.9677536f, 0.1799172f, -0.3524120f, -1.9071139f, -7.4744873f, 2.6429889f, -0.1444485f, 2.3542099f, 1.9711516f, 0.3886011f, 0.0591138f, 0.0699189f, -3.0183461f, 5.6741471f, -1.1488796f, -5.7687993f, -7.4685879f, -9.6448822f, -3.4691532f, 5.0486813f, -12.6432810f, 8.1007729f, -1.7438285f, -17.6806660f },\n",
      "{ 2.1428952f, -6.8331566f, 13.9079485f, 0.1021993f, -3.7234893f, 4.1696281f, -2.8855472f, 2.6500645f, 0.2383825f, 0.0089392f, -1.2698770f, -1.4507635f, 1.9835703f, 1.9673402f, 0.0971966f, -3.0400901f, -1.6690960f, -1.8351660f, -0.5974421f, -0.1041481f, -4.3467126f, -0.8935851f, -4.2216101f, -3.5241582f, -1.7024223f, 2.4351561f, -3.5994439f, 8.1877632f, 1.3503532f, -8.8505297f, -4.3802824f, -1.2580007f },\n",
      "{ 0.5120507f, -0.8167784f, 0.0356091f, 0.9550222f, -0.0856578f, -1.6678712f, -1.3589050f, -0.4634860f, 1.7959082f, -0.0221804f, 1.6858692f, -0.2350332f, 1.5413535f, -0.8954431f, -1.9626563f, -0.8659950f, 1.5534185f, -0.2663006f, -0.0141180f, -0.0641252f, -2.7467253f, 1.3316264f, 0.1042683f, 1.3546844f, 1.3957758f, -1.1120845f, -0.4499652f, -1.0622435f, 1.5979129f, -2.7754719f, -1.8175740f, -0.2714084f },\n",
      "{ 2.8423781f, -0.5802551f, 2.4059951f, -0.1214163f, 0.6598997f, -2.7015202f, -0.6345362f, -0.1668582f, 0.9119438f, -0.1413521f, -0.1243188f, -1.5296214f, -2.2767708f, -0.4354365f, -0.2501381f, -4.5476084f, -2.0407526f, -0.1834106f, 0.4992486f, -0.0249097f, 4.2349467f, -2.2088041f, 0.8245230f, 3.3258171f, -0.8291156f, 1.8809289f, 2.0291409f, -0.1829567f, 0.3753935f, -2.1255214f, 0.3044383f, -0.8117877f },\n",
      "{ 1.0955867f, -10.5214415f, -2.5690074f, 5.7836456f, 8.4926434f, -0.9013922f, 3.8609581f, -18.2928333f, -0.1085265f, -0.1957067f, 0.4130777f, 8.3614740f, -0.1366454f, 15.5778351f, 4.3706384f, -6.1245522f, -0.2726971f, -2.2554579f, 0.1885025f, 0.1015946f, 3.1017957f, -9.0225515f, -2.1712937f, 3.6819403f, 0.3107212f, 5.4358387f, 0.1556205f, -5.7449660f, -8.0870762f, -15.4704018f, 2.6353786f, 3.1047711f },\n",
      "{ 7.6964021f, 5.5716128f, 10.1820107f, 0.3581720f, -3.3344195f, 0.0203794f, 4.8660736f, 0.9843333f, 1.5092058f, -0.1621139f, 1.0461260f, -3.6782911f, -3.2391973f, -1.3214555f, 2.3153496f, -0.1955887f, -0.4074941f, 1.3566486f, 0.4631876f, -0.1068150f, 0.6354957f, 0.4232795f, 1.1560757f, -1.6600587f, 5.4032493f, -0.7661732f, 0.9706129f, -1.9504737f, -2.6826806f, -1.7078609f, 0.2543025f, -0.4549442f },\n",
      "{ -0.9363269f, 3.2511759f, 1.4988805f, 0.3896330f, 3.5761805f, -1.8898439f, -0.5672647f, 0.0965158f, 2.3343837f, -0.2087380f, -1.6115571f, -2.6963377f, -2.8213720f, -1.1722008f, -0.7269015f, -3.2212329f, -2.5389972f, 1.8190597f, -0.3862028f, 0.0262889f, -0.8232661f, -0.2829636f, 0.9204201f, -0.5837291f, 1.1138207f, 1.4175742f, 0.6769784f, -1.8336256f, -0.9406338f, 1.4764718f, -0.4086397f, 1.3364717f },\n",
      "{ -0.2418238f, 0.2686039f, 0.5018921f, 3.8336344f, 0.0712455f, -2.2001183f, -0.9898972f, 0.2089690f, 3.5906739f, -0.1943938f, -0.6164807f, -2.6908536f, 0.5925170f, -0.3563481f, -1.3926654f, -1.4510415f, 0.0248772f, 0.8849464f, 0.3642189f, -0.1435763f, 1.5389724f, 0.9731716f, 0.7003614f, 1.1896435f, 0.0179679f, -0.7365713f, 0.8497150f, 0.2604503f, -2.4675038f, 1.0341363f, 1.5702873f, 0.1958498f },\n",
      "{ -2.8209245f, -8.6002207f, -7.2007174f, -4.5361319f, 1.8299714f, -10.6522865f, -11.7428637f, -5.2347007f, 7.4452977f, -0.1641102f, 0.6734878f, 6.6603560f, 0.1524790f, 10.6615028f, -19.3698826f, 2.1673820f, 11.9764709f, 1.0121659f, 4.0597305f, -0.1707846f, -2.1111398f, -6.5540004f, -18.7466564f, -9.4047441f, 3.0535262f, 2.7488508f, 9.8994598f, 1.6856064f, 19.2076931f, 6.8278632f, -21.9050026f, 9.4619055f },\n",
      "{ -9.7272358f, 11.2765751f, -7.5082231f, -1.7225909f, 13.1407022f, -14.8349190f, -3.2504196f, 15.8990402f, 10.4930105f, 0.0052292f, -1.3149272f, 4.6401610f, -0.3964378f, -4.2996163f, 0.8589647f, 3.0086250f, 16.7304554f, -1.8806626f, -2.5309651f, -0.1554628f, -1.7752417f, 6.6541591f, -16.9228725f, -4.0883851f, 2.5308323f, 1.2640512f, 6.7352295f, 4.6456785f, 8.4472675f, 6.6682787f, -22.7476864f, 5.0754099f },\n",
      "{ -0.0464466f, 0.0371061f, -0.3150824f, 0.2278159f, 0.0781510f, 0.3682392f, -0.0076914f, -0.0199720f, -0.0213357f, -0.0197757f, -3.6537273f, -1.3694360f, -0.1145320f, -0.1972410f, -1.1493046f, 0.5570444f, 0.0053487f, -2.8934319f, -3.5391710f, -0.0754734f, -1.4390492f, 0.5983394f, -1.1443400f, -0.0306496f, -0.0482763f, -0.3964492f, 0.0067964f, -0.3276957f, 0.0020439f, -0.3366909f, -0.0156225f, 0.0541534f },\n",
      "{ -19.9178314f, -10.1069622f, 4.6057873f, -1.6936491f, -1.3457170f, 4.9226985f, 9.7256756f, 3.1693432f, -14.5028820f, 0.0931624f, 0.0866399f, 0.5221885f, -5.0346432f, -9.0003071f, 6.8700786f, -0.3239930f, 10.3604975f, 0.2665467f, -0.8745342f, 0.1671114f, 11.8470201f, 7.2739577f, 0.3534940f, 9.4823771f, 7.6572828f, -0.6058345f, 4.8220043f, -4.4774542f, 6.0150776f, 13.1617508f, 8.9563951f, -5.3839235f },\n",
      "{ -15.3784885f, -4.7021117f, 1.3418291f, -5.5577135f, -3.5915122f, 2.1950095f, 9.0849819f, 4.1605716f, -9.9853363f, -0.2489175f, 0.0786141f, 1.3932520f, -13.3884411f, -7.0676465f, 8.2610102f, -0.6629214f, 9.6442671f, -1.4963982f, 0.0448971f, 0.0271809f, 11.0636673f, 10.8506622f, 1.5703944f, 11.9227686f, 8.8615837f, 2.9824717f, 6.5997915f, -5.1319938f, 6.7163310f, 8.8659868f, 8.8796968f, -0.0585466f },\n",
      "{ -0.5536407f, -0.0183597f, -0.8522137f, -1.6674993f, -0.5186954f, 0.2934249f, -0.3867851f, -0.2717040f, 0.3141194f, 0.0803582f, 0.9972823f, 1.0203497f, 0.1344238f, 0.2180226f, 0.3881106f, -0.5604802f, 0.8635465f, 0.7390655f, 0.9010500f, -0.1396771f, -0.5388748f, -0.0631624f, 0.1186292f, -0.5192882f, 0.4407078f, -1.0921305f, 0.4530205f, -0.2301908f, -0.0831401f, -1.6932087f, 0.6916737f, -7.0294051f },\n",
      "{ -0.7002707f, -1.2612772f, 0.1443634f, 0.1889855f, -1.5781668f, 0.5102594f, 1.0737917f, -0.3611829f, -9.7292194f, -0.2542418f, 0.7840535f, -0.1868368f, -3.0662625f, 0.1156164f, 1.8433700f, 0.5661708f, -0.7161480f, 0.5610115f, 1.1667061f, 0.0282633f, 2.0658562f, -0.1299639f, 0.8088669f, -0.3167280f, 0.0436082f, 0.2469898f, -0.0169766f, 1.4717587f, -0.6065165f, 0.3588113f, 2.0036082f, 1.5167968f },\n",
      "{ 0.8769321f, 0.4987610f, -0.1006753f, 0.1093301f, 0.6219906f, -0.0845869f, 0.5858616f, -0.3685570f, 0.1055476f, 0.0447120f, 0.9492686f, 0.6054065f, 0.3753049f, -0.0302966f, 0.1571343f, 0.5852838f, -0.0052162f, 0.6741483f, 1.0925988f, -0.0104674f, -0.2353034f, -0.2112046f, -0.1704750f, 0.2319756f, -0.6973480f, 0.5177563f, 0.0008280f, 0.0842060f, -0.4818317f, 0.2600521f, -0.8436811f, -0.3244939f },\n",
      "{ -0.2498078f, -0.0665436f, -0.6455372f, -11.8636007f, 0.5546330f, -0.1828474f, 0.4531056f, -0.5045753f, 0.0021860f, -0.1727674f, 0.8539888f, 0.9355938f, 0.1768771f, 0.1097697f, 0.3439056f, -2.7710619f, 0.5562497f, 0.7414805f, 0.9723032f, -0.0587594f, -0.2656697f, -0.2262570f, 0.2070577f, -0.4649415f, 0.6072245f, 0.4071639f, 0.7271490f, -0.3399665f, -0.2358655f, -0.6572027f, 0.3878818f, -2.0380676f },\n",
      "{ -1.0040656f, -15.5001106f, 0.1743044f, -0.1830439f, -12.8375502f, 0.6479514f, 0.4189325f, 0.0413221f, -2.3929427f, 0.0917285f, 0.8877478f, -0.4985425f, -3.9854083f, 0.2641849f, 1.1492108f, 1.1804928f, -0.4219547f, 0.7097337f, 0.7229153f, -0.1290352f, 1.0844772f, -0.2112084f, 0.2665555f, -0.0124178f, -0.7096525f, -0.0406028f, 0.1377101f, 0.9613231f, -0.5315894f, 0.3310910f, 1.4178389f, 1.1687748f },\n",
      "{ 0.6737692f, 0.5887758f, -0.0501311f, 1.0043997f, 0.9804797f, 0.1708015f, 0.1819106f, -0.5253279f, -0.4184220f, -0.0125217f, 1.0117618f, 0.4730924f, 0.6627691f, -0.1479898f, 0.2580604f, 0.7293127f, 0.0848437f, 0.7156146f, 0.7816427f, 0.0669967f, -0.1618401f, -0.0909416f, 0.1740791f, -0.0961089f, -0.9237953f, 0.1705291f, 0.2970416f, 0.2243806f, -0.2949813f, 0.1089575f, -0.3233873f, -0.5519235f },\n",
      "{ 0.8008638f, 0.1581205f, -0.0660521f, -1.8700145f, 1.8415585f, -0.1517836f, 0.9849059f, -0.3305838f, -0.1100404f, 0.1668913f, 0.0475755f, -0.7035441f, 0.0815494f, -0.4517657f, 0.3797469f, -0.7237183f, -0.4326949f, -0.0045456f, -0.1520977f, 0.0641505f, 0.3261537f, 0.3451711f, 0.3400122f, 0.1905922f, 0.5979490f, 1.8726524f, -0.0212991f, 0.5759162f, -0.3411353f, 0.6154488f, -0.6591337f, 1.1044852f },\n",
      "{ -0.5229920f, -1.9910779f, 0.0082411f, -1.0964926f, -2.8197410f, 0.0379619f, -0.6362457f, 0.2922821f, 2.1298475f, 0.0429705f, 0.0860498f, 0.0998842f, -1.5189127f, 0.8646886f, -0.3320501f, 0.8533753f, 0.2106902f, -0.1353741f, 0.1139200f, 0.1223277f, -1.3724730f, -0.4976718f, -0.8456128f, 0.6275283f, -0.7746818f, -0.4165801f, -0.3285437f, -1.1218213f, 0.2541133f, -0.0699065f, -0.3644446f, -0.1836386f },\n",
      "{ -0.3121711f, 0.6553325f, 0.0358024f, 2.9133885f, 0.0601561f, 0.0650956f, -0.3388072f, -0.1669552f, -1.0281963f, 0.0421454f, -0.0252209f, 0.6697660f, 0.3152214f, -0.1007413f, 0.0397672f, 0.0762647f, 0.6438169f, -0.1315191f, -0.0042900f, -0.0298286f, 0.4026648f, 0.1243395f, 0.5714200f, -0.5934660f, 0.1301748f, -0.8935670f, 0.3112782f, 0.2476320f, 0.3565531f, -0.2412163f, 0.8716605f, -0.7849768f },\n",
      "};\n",
      "\n",
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float bias_layer2[32] = {\n",
      "-0.2339502f, -2.1117039f, 1.3029057f, 0.1388091f, -0.1596625f, -0.0429177f, -1.2913821f, -3.2888331f, -0.1078175f, 0.3783462f, -0.1941358f, -0.4196923f, 1.5371246f, 1.0102557f, 0.0155053f, -0.1713930f, -0.8003848f, 1.2788725f, 1.2547292f, 0.9737166f, 0.0159556f, -0.9538616f, -0.6874874f, 1.4604672f, -0.0163722f, -0.6740248f, 0.4310561f, 0.4786355f, 0.4338695f, -0.2144726f, -0.3379008f, 0.9070537f };\n",
      "\n",
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float wgtT_layer2[32][32] = {\n",
      "{ -0.1525858f, -1.7981244f, 0.8225231f, 0.9017935f, -0.1756990f, -0.1731562f, 0.3533860f, -0.1859016f, 0.0066088f, -1.9248251f, -0.1184793f, 0.0248543f, 0.5696007f, -0.0907341f, 1.2301605f, -0.2350527f, -1.4149488f, -0.1846859f, -1.7834854f, 1.0467646f, -0.1320335f, -0.5828664f, -0.9305819f, -1.3805257f, -0.0421591f, -0.9069743f, -1.1278086f, 0.2304732f, -0.5726073f, 0.0060693f, -0.6733936f, 2.4653385f },\n",
      "{ -0.1177371f, -13.0121050f, -1.2429829f, -2.7850580f, -0.0618916f, -0.1455433f, -2.9844699f, 2.1670566f, 0.0382921f, 1.7592702f, 0.0663455f, -0.6991704f, 1.1692400f, -1.3941003f, -0.7780491f, -0.1350629f, -3.8783550f, -0.2548187f, 0.3231797f, 1.3278724f, -0.3614419f, -0.6638193f, -0.3801469f, 0.3153987f, 2.9468744f, 0.4463870f, -2.2508523f, 0.2242093f, -0.7827371f, 0.0342467f, -0.1136825f, -1.4055092f },\n",
      "{ -0.0814950f, 1.1034510f, -2.7856872f, 1.1012450f, -0.0762363f, 0.0192755f, 0.0942746f, -0.8603304f, 0.0463609f, -0.4289516f, -0.2340941f, 0.7183347f, -0.3241202f, -3.3051322f, 0.6164730f, -0.1720765f, -2.9967003f, -0.2995018f, 1.4385067f, -3.2133255f, -0.0849749f, 1.7341144f, 0.7924995f, 2.0554399f, 1.0967957f, 0.3311919f, -0.9581537f, -0.7702340f, 1.0316148f, 0.0471946f, 0.9495452f, 2.0788605f },\n",
      "{ 0.0446539f, -0.7262716f, 0.2242534f, -2.2987046f, -0.1550755f, -0.2146342f, 0.2053458f, 0.1453160f, -0.0573685f, -0.6836352f, -0.1972867f, 0.3058005f, -0.2511542f, 1.0547395f, 0.2851569f, -0.1420605f, 0.1955147f, -0.8554440f, -0.7066809f, -0.3464343f, -0.0067930f, -0.3269323f, -0.2229819f, -0.3971729f, -0.6963940f, -0.2993692f, -0.1417452f, -0.7594388f, 0.5256453f, -0.1079851f, 0.1336530f, -1.2257522f },\n",
      "{ -0.0354096f, -10.4679108f, 2.8168788f, 0.2949276f, -0.1779284f, -0.1868454f, -1.4323943f, 0.8472028f, -0.0747709f, 0.2078577f, -0.1229924f, 0.0567793f, 2.0551534f, -0.4557192f, -0.7714190f, -0.0745471f, -0.1583210f, 1.3936666f, -1.1807573f, -0.6911215f, -0.1539799f, -0.6865327f, 0.5996337f, -0.7800899f, 0.1926797f, 0.4295036f, -1.3045609f, 1.3919017f, -0.6824273f, -0.0342412f, -0.1761061f, -0.6753551f },\n",
      "{ -0.1453138f, -2.1702523f, -0.8408509f, 0.0124695f, 0.0548023f, -0.0599126f, -1.3220210f, -0.9635140f, 0.0304381f, 0.8915846f, -0.0114084f, -0.1811013f, -1.0392225f, 0.9355077f, 0.5600502f, -0.1725564f, 1.3016142f, 0.0637798f, 0.2127237f, -0.4201719f, -0.1813546f, -2.8520944f, -1.0162476f, 0.4036767f, -1.3526999f, -0.3187918f, 0.2210670f, -0.0195748f, 0.5739521f, -0.2086164f, -4.5898190f, 0.8253796f },\n",
      "{ -0.1084008f, -1.1329324f, -1.6204234f, 1.1476817f, -0.0628205f, -0.1122542f, 0.5731813f, 0.2743464f, -0.0484099f, -0.8538088f, 0.0105360f, -0.7381154f, -1.1602422f, -1.2242011f, 0.2023281f, -0.2181710f, -1.7745955f, -0.0181406f, 1.6978747f, -1.8673037f, -0.1872354f, 1.3988467f, -1.3061260f, 0.6495667f, -0.6886965f, -0.6353581f, 0.2563086f, 0.4972568f, 1.2073671f, -0.0324760f, 0.6208668f, 2.0307891f },\n",
      "{ -0.0193408f, -0.4770618f, 1.1954961f, -4.2316422f, -0.0323112f, -0.1459102f, -0.8704525f, 0.0916627f, 0.1220654f, 1.4996083f, 0.1548122f, -1.9588864f, -1.5469869f, -1.3433179f, -1.0718721f, -0.0825612f, -0.4096117f, 0.9981126f, -1.7012634f, -1.8265936f, -0.0371830f, 2.3563027f, -1.3538713f, 0.6455814f, 1.7223636f, 0.7526782f, -2.3576136f, 1.1849345f, 2.1408458f, -0.1714138f, 0.4818093f, -0.3588967f },\n",
      "{ 0.0729335f, -6.1714144f, 2.1946981f, -4.1299558f, 0.0044464f, -0.1933377f, 0.6864235f, -0.3803750f, -0.2148182f, 0.9621077f, -0.1807104f, 2.4574375f, 1.2062972f, 2.5480094f, 0.6602007f, -0.0865193f, -1.3168519f, 1.7552648f, 0.5018759f, 1.1154609f, -0.2282289f, 0.7631954f, -1.0011274f, -1.3729336f, 0.9056255f, -0.0414166f, -1.8079906f, 1.0183337f, 0.3374647f, 0.0572285f, -0.4237293f, 1.7220364f },\n",
      "{ -0.0589474f, -0.0295253f, 0.1225147f, 0.1446855f, 0.1071389f, 0.1601978f, -0.0401577f, -0.0364199f, 0.0953632f, 0.0868486f, 0.1608090f, 0.0642403f, 0.1249414f, 0.1899325f, 0.0255690f, -0.0968316f, -0.1216723f, -0.1698821f, 0.0820711f, 0.1747911f, 0.0620590f, -0.1446941f, -0.1555044f, 0.0741209f, -0.0763885f, -0.1246467f, 0.1337765f, -0.0873028f, 0.0942246f, 0.0860358f, 0.1234084f, 0.1226101f },\n",
      "{ 0.0643494f, -0.8147358f, -1.5970768f, -1.5264196f, -0.1014192f, 0.0894014f, -0.2399453f, 1.0807495f, -0.1235767f, -1.2951756f, -0.0810054f, -1.1764668f, 0.4282590f, 0.0908309f, 0.6702700f, 0.0942179f, -0.8752475f, 1.0613892f, 2.6491807f, 0.4649454f, 0.0426983f, -0.8645003f, -1.2832506f, 1.0818568f, -0.6891628f, -2.6222782f, -0.0669045f, -5.2834606f, -3.7087319f, -0.1093205f, -3.0082226f, 0.0202389f },\n",
      "{ -0.2165046f, -2.3887262f, 1.3350971f, 3.2154691f, -0.0899850f, -0.1132472f, 3.4892216f, 0.6763555f, -0.0366738f, -0.3074943f, -0.0737181f, -0.2638237f, -2.1615725f, 0.9533494f, 1.0567867f, -0.0743023f, -1.5021936f, -2.1020463f, -0.4901834f, 0.3259082f, -0.2369750f, -0.1386719f, -0.1885716f, -0.6453994f, -0.2157237f, -0.3096344f, -0.7145861f, -0.0144529f, -1.7015969f, 0.0518598f, -0.8833122f, 0.1368720f },\n",
      "{ -0.1693773f, -1.7163130f, 1.4214562f, -5.3610578f, -0.1453443f, 0.0007269f, 2.3939090f, -0.4252889f, -0.2080639f, 0.4997855f, -0.1604623f, -0.9568118f, 0.1174134f, 1.5827537f, 0.0471937f, 0.0410202f, -5.9910173f, -1.7195174f, 2.6566005f, -0.1300166f, -0.3056662f, -0.4734422f, 0.5415567f, -0.6322125f, -0.6389906f, 0.8110722f, -1.7760222f, -0.1890267f, 0.4568902f, 0.0178664f, 0.9815944f, -0.6494362f },\n",
      "{ -0.0803987f, -0.4049147f, -1.1458402f, -3.8709407f, -0.0748584f, 0.0285123f, 1.0476711f, -0.4686022f, -0.1482179f, -1.3519528f, -0.1977116f, 1.0795574f, -2.0080688f, 0.8830637f, -1.8861086f, -0.1462930f, -0.4670950f, -2.0300276f, -1.3247769f, 1.1512129f, -0.1377436f, 1.7818434f, 0.5111244f, -1.0817790f, -1.9341105f, -0.2747863f, 1.7866142f, -0.6981304f, 0.2916693f, -0.0235312f, -0.8175534f, -1.6998042f },\n",
      "{ -0.1061751f, 0.6215375f, -0.4626823f, 0.7672512f, 0.0020817f, -0.1591142f, 1.0898968f, -0.9204068f, 0.0574663f, 0.1774926f, 0.0484907f, -0.3295842f, 0.4489160f, -1.1343844f, 1.5402520f, -0.1346410f, -1.5354218f, -0.7182790f, 0.2196787f, -4.9884086f, -0.1196452f, -0.7342232f, -0.2625498f, -0.1683128f, -3.4147658f, 0.4656263f, -0.1907654f, -1.5676821f, -0.1993192f, 0.0392413f, -0.4939966f, 0.2587339f },\n",
      "{ -0.1575988f, 0.3401670f, 0.3991243f, 0.7242632f, -0.1744899f, -0.1183499f, 0.0967574f, -0.3592824f, -0.2704069f, -0.2581256f, -0.2485954f, 0.3446464f, -0.7076147f, -0.6296598f, 1.0094991f, -0.1585015f, -0.0655680f, -0.1737440f, -0.6190755f, 0.2832435f, -0.0455194f, -1.0389528f, 1.4021788f, 0.1367040f, -0.2918817f, -0.8456540f, -0.1551147f, -0.8092477f, 0.2053470f, 0.0517656f, 0.3124267f, -0.0847588f },\n",
      "{ 0.0037535f, 1.7123971f, -0.0702230f, -7.8142509f, -0.1969137f, -0.0857682f, -0.2811019f, -0.4737110f, -0.1132331f, -0.1521158f, -0.0600281f, 0.0706206f, 1.0167074f, 0.2372540f, 0.5142798f, -0.1154348f, -0.4991046f, 1.8288782f, -0.2266811f, 0.5554674f, -0.2068119f, -1.1852149f, 1.3449707f, -2.0556967f, -0.9963455f, 0.2519412f, 0.1331067f, -0.4544231f, 1.0645961f, -0.1106808f, 0.9460998f, 0.4178981f },\n",
      "{ -0.2119178f, 1.9799466f, -0.2797689f, -1.0932020f, 0.0028856f, -0.1588331f, -0.3786546f, 1.1156173f, -0.0215458f, -5.3119068f, -0.0948427f, -1.5506617f, -1.4219491f, 2.4189386f, 1.4762870f, -0.0996478f, 1.9492981f, 0.9989491f, 0.9490891f, -0.7439177f, -0.0704944f, -0.7028235f, 0.2047088f, -2.2349808f, 0.5578019f, -2.0313745f, -0.0651128f, -0.3024914f, -2.0939598f, -0.0180862f, -1.0688573f, -1.6645123f },\n",
      "{ -0.0167054f, -0.9788288f, 0.5361816f, 1.6640991f, -0.0242798f, 0.0521166f, -0.4116896f, -0.8344618f, -0.0963774f, 3.8331950f, -0.0267836f, 2.8407450f, 1.5915482f, -1.3799198f, -2.6205218f, -0.0732574f, 0.7984170f, -2.0659196f, -3.5870969f, 2.8777542f, -0.1461587f, 1.1945763f, 1.1590073f, 1.2939118f, 0.4921311f, 4.3183928f, 0.2045673f, 4.7427683f, 3.8618271f, -0.0308256f, 4.8768749f, 1.0944498f },\n",
      "{ -0.1358386f, -0.1472694f, -0.1158773f, -0.0939915f, 0.1222765f, 0.1192099f, 0.0518727f, 0.0441144f, 0.0335920f, 0.1631196f, -0.0097409f, 0.0913221f, -0.1351816f, -0.0724972f, -0.0489905f, 0.0919923f, -0.0411735f, 0.0600587f, 0.0222677f, 0.0181216f, -0.0119533f, 0.0149932f, 0.1105281f, 0.0120449f, -0.0900230f, 0.1096532f, -0.0958123f, 0.0478526f, -0.0528525f, -0.0530951f, 0.1649548f, 0.0884538f },\n",
      "{ -0.1365108f, -1.4470286f, -3.3190532f, -0.6094688f, -0.2091497f, -0.0999000f, -0.9306201f, -1.0360260f, -0.1827736f, 0.0683407f, -0.0502374f, 0.3790842f, -3.5734098f, -0.9349656f, 0.3886600f, 0.1461777f, -2.0582819f, 1.1910707f, -1.9501384f, -2.9547875f, -0.1470737f, -1.4672521f, -0.8007376f, -0.9336768f, 1.3155514f, 0.4972472f, -5.6431427f, -3.5151341f, -5.8025484f, -0.0358306f, 0.4548545f, 0.2571939f },\n",
      "{ -0.1756670f, -1.3679352f, -0.3739035f, -0.4339395f, -0.2128811f, -0.1038225f, 0.5042853f, 0.4281598f, -0.2762718f, -0.3050812f, -0.0561761f, -0.3663020f, -0.7264599f, -0.7135370f, -1.4355675f, -0.0855041f, -1.6003639f, -3.3269587f, 0.3331296f, -0.2510884f, -0.0435672f, 2.3782334f, -0.8790841f, -0.4602313f, 0.3282675f, 1.0674137f, -0.4261901f, 0.6184355f, -0.6284660f, -0.2009129f, -1.3984579f, -0.1182039f },\n",
      "{ -0.0179437f, -2.4176567f, -0.0757853f, -0.9053250f, -0.1604971f, 0.0653039f, -0.0456533f, -1.2324991f, -0.2042288f, -1.4000354f, -0.1496307f, 0.7025797f, -0.8148692f, -2.2639663f, -0.1080219f, -0.1692714f, -0.0350256f, 1.1112232f, -1.2173100f, -3.3865623f, -0.0014034f, 1.0519972f, -0.4149089f, -0.9822370f, 1.0764426f, 1.0734540f, 0.2395243f, -0.9695317f, -0.2875118f, 0.1405493f, -0.8919160f, 0.4231086f },\n",
      "{ -0.2191032f, 1.8018681f, -1.7559167f, 0.0348163f, -0.0597553f, 0.0096346f, 0.4048410f, -2.5880859f, -0.2669998f, 0.5746318f, -0.1141291f, 0.3722134f, -0.6411135f, -0.8711343f, 0.9618454f, -0.1413043f, 0.5999789f, 0.4171342f, -0.0654649f, -1.1597379f, -0.0378705f, -1.1590790f, 1.3731012f, -0.1211245f, -0.4004700f, -0.8745431f, 0.3397753f, -0.4758925f, 0.4651093f, -0.1950274f, 0.7756749f, 0.2193565f },\n",
      "{ -0.0431680f, 1.4789274f, -1.2237777f, 1.1632382f, -0.0625869f, -0.0474214f, -1.3440026f, 1.4450136f, -0.1436337f, 0.7880324f, -0.2373608f, 1.4573339f, 0.7586362f, -0.9148275f, -0.2211355f, 0.0550283f, -6.0262470f, 0.3978752f, -0.1995126f, -0.0479593f, -0.1120174f, -1.2093679f, 3.4314570f, 0.0222109f, -1.1163449f, 0.2131999f, 2.2462623f, -0.4972607f, -0.5182921f, -0.0701132f, 0.5019436f, -0.1937658f },\n",
      "{ -0.1667943f, 0.0196575f, -0.9141287f, -0.2902696f, 0.0318007f, -0.0335459f, 0.5858797f, -0.0900941f, -0.0614002f, 1.2998632f, -0.0909332f, -0.5147573f, -0.0871529f, 1.2078508f, -0.2408318f, 0.0943059f, 2.0550728f, -2.3531728f, -0.4569368f, -0.0462965f, -0.1727646f, 0.8314115f, -0.0838026f, 1.1487722f, -2.3224678f, 0.6001235f, 0.4483957f, -2.0149994f, -0.5540643f, -0.0993498f, -0.3938250f, 1.0691496f },\n",
      "{ -0.2128484f, -0.9067336f, -0.8067648f, -0.5638098f, 0.0466387f, -0.1250697f, -1.0633522f, 2.0702426f, -0.2612922f, 0.4281299f, 0.0145171f, 0.8089548f, 1.0637047f, -1.6296221f, -0.0324031f, -0.1453255f, 0.5244270f, 0.4705407f, 0.3187122f, 0.8282533f, -0.0451930f, 0.6502138f, -0.3958839f, 0.8502606f, 0.9307000f, 0.3706145f, -1.7802641f, 0.1567971f, 0.0740849f, -0.0895442f, 0.3950359f, -1.7540554f },\n",
      "{ 0.0023149f, 0.7263200f, 1.8512523f, -0.4771179f, -0.1163291f, 0.0224466f, 2.1579833f, -0.2968757f, -0.0573871f, 0.9053006f, -0.0305074f, -0.9798708f, -0.0732942f, 1.8749733f, 0.1304994f, -0.1875225f, 1.1224291f, 0.5052640f, -0.4813080f, -1.1107147f, -0.1909173f, -2.3484604f, 0.1204147f, -0.4592973f, -0.2538125f, 0.4104489f, -0.7043998f, 1.0401071f, 0.0527195f, -0.1755383f, -0.0225556f, -1.7959408f },\n",
      "{ -0.1738733f, -2.5466421f, 0.4712865f, -4.1575985f, -0.0673208f, -0.0929229f, 1.4804220f, -2.0162396f, -0.0699170f, -1.2635130f, 0.0033261f, -0.2207318f, 1.6315613f, -0.2808344f, 0.6500907f, 0.0873509f, 1.2061590f, 0.9874881f, 0.0503163f, 1.1066431f, -0.2690387f, 3.0120494f, 1.0901014f, -0.8387108f, -0.1055128f, 0.7355948f, -1.6113559f, 0.1626149f, 0.6186574f, -0.1442034f, -1.3751197f, -0.7357581f },\n",
      "{ 0.0801182f, -1.7048576f, -0.6614535f, 0.2159117f, -0.1067486f, -0.0390127f, -1.7236508f, -4.8257847f, -0.1392784f, 2.6549494f, -0.0076066f, -0.2928389f, -1.4872373f, -2.2248919f, 0.3740856f, 0.0089244f, -2.1601932f, 0.2364899f, 2.2799277f, 0.6705108f, 0.0404501f, 0.8240705f, 0.9378061f, 1.7379807f, 0.6561645f, -2.2824814f, 0.4317684f, 0.9481612f, 3.3949718f, -0.1386193f, -0.5392355f, -0.2192377f },\n",
      "{ -0.0682519f, 2.1510222f, -1.3331705f, 1.7252599f, -0.0390061f, -0.0311894f, -1.4449217f, 0.7528419f, 0.0482161f, -0.9996649f, -0.1083589f, 0.7481404f, -1.0902423f, 0.3155636f, -0.7373950f, -0.0225023f, -0.5480027f, -1.7043972f, 0.1796622f, -1.9089470f, -0.0284076f, 0.1727537f, 0.9399895f, 1.2661113f, -0.1629798f, 0.9542015f, -0.3868639f, -0.2013974f, 0.7035374f, -0.1902321f, 0.0618807f, 1.6585518f },\n",
      "{ -0.0382449f, -0.3556600f, 0.7932450f, -1.7579209f, 0.0754038f, -0.1452644f, -0.0262194f, 0.0631723f, -0.1482574f, 0.4631275f, 0.0041328f, -0.5673072f, 0.6279140f, -0.4053148f, 0.7965385f, 0.0043115f, -0.7390732f, -0.1081802f, -0.3900261f, 0.2581029f, -0.0874927f, 0.5915189f, -1.1614733f, 0.6526067f, -0.0630925f, -1.2862672f, 0.0975272f, -1.6696635f, -2.1918375f, 0.0246310f, -0.5078331f, -1.6059371f },\n",
      "};\n",
      "\n",
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float bias_output_layer[3] = {\n",
      "-0.0299599f, -0.5291600f, 0.4870134f };\n",
      "\n",
      "ALPAKA_STATIC_ACC_MEM_GLOBAL const float wgtT_output_layer[32][3] = {\n",
      "{ 0.0675000f, 0.0580845f, 0.0252083f },\n",
      "{ -0.5480256f, 0.5014070f, -0.4848022f },\n",
      "{ 0.0558893f, -0.9268196f, 0.6123658f },\n",
      "{ -0.1926070f, 0.4826855f, -0.7119671f },\n",
      "{ -0.0407576f, -0.0624052f, 0.0473613f },\n",
      "{ 0.0114524f, -0.0661213f, 0.0861401f },\n",
      "{ 0.4889011f, 0.3984242f, -0.1460386f },\n",
      "{ -0.0307660f, -1.1088817f, 0.5681090f },\n",
      "{ -0.0103742f, -0.0451352f, 0.0913075f },\n",
      "{ 0.2240870f, -0.4127513f, -0.3115116f },\n",
      "{ -0.1053132f, 0.0329629f, -0.0964765f },\n",
      "{ 0.1293648f, -0.0118805f, -0.5233608f },\n",
      "{ 0.0734460f, 0.5619589f, -0.4186259f },\n",
      "{ -0.3173129f, 0.1465155f, -0.1484945f },\n",
      "{ -0.5634108f, 0.2698199f, 0.1681544f },\n",
      "{ 0.1714749f, -0.1649845f, 0.1014268f },\n",
      "{ 0.1057630f, 0.9072341f, -1.1890781f },\n",
      "{ -0.3175716f, -0.2992002f, 0.3401313f },\n",
      "{ -0.4994496f, -0.1189708f, 0.3650176f },\n",
      "{ 0.4023024f, -0.9219202f, 0.0693439f },\n",
      "{ -0.1709604f, -0.0994071f, 0.0222464f },\n",
      "{ 0.3324146f, 0.0158491f, -0.4939574f },\n",
      "{ 0.0952293f, -0.5191534f, 0.3818873f },\n",
      "{ 0.0176812f, 0.2723607f, -0.3078566f },\n",
      "{ 0.1919187f, -0.8505318f, 0.1964855f },\n",
      "{ 0.0822281f, 0.0565761f, -0.5816049f },\n",
      "{ -0.0743144f, 0.0852944f, -0.7256451f },\n",
      "{ 0.1760607f, 0.0578475f, -0.8243266f },\n",
      "{ 0.1617602f, 0.1823115f, -0.4042889f },\n",
      "{ -0.0384557f, -0.0115344f, 0.0508929f },\n",
      "{ 0.5513267f, -0.0695007f, -1.2113558f },\n",
      "{ 0.0910288f, -0.4101452f, 0.3242988f },\n",
      "};\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_formatted_weights_biases(weights, biases, layer_name):\n",
    "    # Print biases\n",
    "    print(f\"ALPAKA_STATIC_ACC_MEM_GLOBAL const float bias_{layer_name}[{len(biases)}] = {{\")\n",
    "    print(\", \".join(f\"{b:.7f}f\" for b in biases) + \" };\")\n",
    "    print()\n",
    "\n",
    "    # Print weights\n",
    "    print(f\"ALPAKA_STATIC_ACC_MEM_GLOBAL const float wgtT_{layer_name}[{len(weights[0])}][{len(weights)}] = {{\")\n",
    "    for row in weights.T:\n",
    "        formatted_row = \", \".join(f\"{w:.7f}f\" for w in row)\n",
    "        print(f\"{{ {formatted_row} }},\")\n",
    "    print(\"};\")\n",
    "    print()\n",
    "\n",
    "def print_model_weights_biases(model):\n",
    "    # Make sure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate through all named modules in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is a linear layer\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Get weights and biases\n",
    "            weights = module.weight.data.cpu().numpy()\n",
    "            biases = module.bias.data.cpu().numpy()\n",
    "\n",
    "            # Print formatted weights and biases\n",
    "            print_formatted_weights_biases(weights, biases, name.replace('.', '_'))\n",
    "\n",
    "print_model_weights_biases(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure input_features_tensor is moved to the appropriate device\n",
    "input_features_tensor = input_features_tensor.to(device)\n",
    "filtered_inputs = input_features_tensor[~nan_mask]\n",
    "filtered_labels = labels_tensor[~nan_mask]\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(input_features_tensor)\n",
    "    predictions = outputs.squeeze().cpu().numpy()\n",
    "\n",
    "full_tracks = (np.concatenate(branches['t4_pMatched']) > 0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta bin 0.00-0.10: 343773 fakes, 783 true Displaced\n",
      "Eta bin 0.10-0.20: 277198 fakes, 685 true Displaced\n",
      "Eta bin 0.20-0.30: 236575 fakes, 679 true Displaced\n",
      "Eta bin 0.30-0.40: 243786 fakes, 803 true Displaced\n",
      "Eta bin 0.40-0.50: 236255 fakes, 682 true Displaced\n",
      "Eta bin 0.50-0.60: 215018 fakes, 856 true Displaced\n",
      "Eta bin 0.60-0.70: 157631 fakes, 989 true Displaced\n",
      "Eta bin 0.70-0.80: 117039 fakes, 746 true Displaced\n",
      "Eta bin 0.80-0.90: 103566 fakes, 893 true Displaced\n",
      "Eta bin 0.90-1.00: 63672 fakes, 710 true Displaced\n",
      "Eta bin 1.00-1.10: 46689 fakes, 803 true Displaced\n",
      "Eta bin 1.10-1.20: 54520 fakes, 765 true Displaced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eta bin 1.20-1.30: 69878 fakes, 988 true Displaced\n",
      "Eta bin 1.30-1.40: 28034 fakes, 447 true Displaced\n",
      "Eta bin 1.40-1.50: 10484 fakes, 458 true Displaced\n",
      "Eta bin 1.50-1.60: 16192 fakes, 1550 true Displaced\n",
      "Eta bin 1.60-1.70: 26335 fakes, 1499 true Displaced\n",
      "Eta bin 1.70-1.80: 29781 fakes, 1367 true Displaced\n",
      "Eta bin 1.80-1.90: 45547 fakes, 1894 true Displaced\n",
      "Eta bin 1.90-2.00: 41836 fakes, 1748 true Displaced\n",
      "Eta bin 2.00-2.10: 11233 fakes, 1706 true Displaced\n",
      "Eta bin 2.10-2.20: 8634 fakes, 1151 true Displaced\n",
      "Eta bin 2.20-2.30: 4555 fakes, 1104 true Displaced\n",
      "Eta bin 2.30-2.40: 7739 fakes, 1940 true Displaced\n",
      "Eta bin 2.40-2.50: 1872 fakes, 1428 true Displaced\n",
      "<Figure size 2000x800 with 3 Axes>\n",
      "\n",
      "Displaced tracks, pt: 0.0 to 5.0 GeV\n",
      "Number of true displaced tracks: 26674\n",
      "Number of fake tracks in pt bin: 2397842\n",
      "\n",
      "65% Retention Cut Values: {0.6532, 0.6635, 0.7246, 0.7622, 0.6931, 0.7138, 0.7964, 0.7897, 0.8166, 0.8314, 0.7027, 0.6347, 0.6674, 0.7002, 0.7721, 0.8819, 0.8457, 0.8013, 0.7852, 0.7547, 0.7885, 0.8362, 0.8480, 0.6366, 0.3407} Mean: 0.7376000285148621\n",
      "65% Cut Fake Rejections: {96.8, 97.5, 97.9, 98.2, 97.6, 97.3, 98.0, 97.2, 97.5, 97.2, 91.8, 89.7, 88.0, 86.0, 88.2, 84.2, 89.7, 89.2, 89.3, 89.4, 82.8, 82.6, 85.9, 81.8, 76.0} Mean: 90.8%\n",
      "\n",
      "70% Retention Cut Values: {0.5721, 0.5600, 0.6553, 0.7036, 0.6332, 0.6271, 0.7428, 0.7396, 0.7384, 0.7892, 0.6409, 0.5853, 0.6323, 0.6704, 0.7149, 0.8548, 0.8051, 0.7519, 0.7197, 0.6995, 0.7382, 0.7676, 0.8132, 0.4517, 0.2545} Mean: 0.6744999885559082\n",
      "70% Cut Fake Rejections: {95.9, 96.6, 97.2, 97.6, 97.1, 96.4, 97.4, 96.5, 96.3, 96.4, 89.7, 87.9, 86.7, 84.5, 85.9, 81.9, 87.5, 87.2, 86.8, 87.3, 80.1, 77.6, 83.7, 74.8, 66.8} Mean: 88.6%\n",
      "\n",
      "75% Retention Cut Values: {0.4679, 0.4943, 0.5824, 0.6214, 0.5755, 0.5700, 0.6640, 0.6667, 0.6209, 0.7128, 0.5568, 0.5166, 0.5729, 0.6201, 0.6468, 0.8087, 0.7613, 0.6858, 0.6370, 0.6208, 0.6688, 0.6535, 0.7243, 0.2648, 0.1871} Mean: 0.5960000157356262\n",
      "75% Cut Fake Rejections: {94.7, 95.9, 96.5, 96.8, 96.5, 95.7, 96.6, 95.6, 94.5, 95.0, 86.6, 84.9, 84.3, 81.9, 83.2, 78.0, 85.2, 84.6, 83.5, 84.4, 77.0, 71.8, 78.3, 60.6, 55.9} Mean: 85.5%\n",
      "\n",
      "80% Retention Cut Values: {0.3643, 0.3997, 0.4992, 0.5466, 0.4813, 0.5027, 0.5943, 0.5423, 0.5045, 0.6272, 0.5120, 0.4585, 0.5139, 0.5219, 0.5931, 0.7501, 0.7040, 0.5831, 0.5597, 0.4881, 0.6129, 0.5526, 0.3974, 0.1495, 0.1389} Mean: 0.5038999915122986\n",
      "80% Cut Fake Rejections: {93.0, 94.6, 95.7, 96.1, 95.4, 94.8, 95.7, 93.7, 92.3, 92.9, 84.8, 82.1, 81.7, 77.0, 81.1, 73.8, 82.3, 80.9, 80.6, 79.4, 74.6, 67.6, 65.3, 42.6, 46.4} Mean: 81.8%\n",
      "\n",
      "85% Retention Cut Values: {0.2693, 0.2885, 0.3381, 0.3925, 0.3886, 0.3998, 0.5003, 0.4532, 0.3624, 0.5571, 0.4461, 0.3688, 0.4487, 0.4183, 0.4207, 0.6097, 0.5510, 0.4526, 0.4826, 0.3136, 0.4704, 0.4141, 0.2126, 0.1203, 0.1067} Mean: 0.391400009393692\n",
      "85% Cut Fake Rejections: {90.9, 92.7, 93.6, 94.2, 94.3, 93.2, 94.4, 92.1, 88.4, 91.0, 81.7, 77.3, 78.6, 71.6, 73.2, 66.4, 75.2, 75.8, 77.5, 71.1, 68.6, 60.6, 48.5, 35.7, 35.8} Mean: 76.9%\n",
      "\n",
      "90% Retention Cut Values: {0.1981, 0.2071, 0.1651, 0.2351, 0.2320, 0.2366, 0.3681, 0.3035, 0.2574, 0.4382, 0.3126, 0.3057, 0.3526, 0.2610, 0.3053, 0.4718, 0.4004, 0.3037, 0.3010, 0.2001, 0.2483, 0.2288, 0.0990, 0.0992, 0.0847} Mean: 0.26460000872612\n",
      "90% Cut Fake Rejections: {88.3, 90.4, 89.0, 90.9, 91.0, 89.4, 92.0, 88.2, 83.8, 87.0, 73.9, 73.1, 73.8, 61.3, 66.5, 59.0, 67.7, 67.9, 68.6, 63.1, 55.6, 47.6, 29.6, 30.0, 30.0} Mean: 70.3%\n",
      "\n",
      "95% Retention Cut Values: {0.0852, 0.0952, 0.0733, 0.1015, 0.1282, 0.1008, 0.2063, 0.1310, 0.1406, 0.2143, 0.1995, 0.1805, 0.2321, 0.1726, 0.1861, 0.3016, 0.2233, 0.1839, 0.1699, 0.1414, 0.1340, 0.1005, 0.0799, 0.0775, 0.0640} Mean: 0.14890000224113464\n",
      "95% Cut Fake Rejections: {78.3, 83.4, 80.8, 83.3, 86.0, 80.4, 86.7, 77.7, 74.8, 75.3, 64.9, 61.7, 65.6, 53.5, 56.4, 48.3, 55.7, 58.6, 58.5, 56.8, 42.5, 29.4, 24.1, 24.7, 22.8} Mean: 61.2%\n",
      "<Figure size 2000x800 with 3 Axes>\n",
      "\n",
      "Fake tracks, pt: 0.0 to 5.0 GeV\n",
      "Number of true fake tracks: 26674\n",
      "Number of fake tracks in pt bin: 2397842\n",
      "\n",
      "65% Retention Cut Values: {0.1999, 0.1969, 0.1692, 0.1742, 0.1824, 0.1582, 0.1384, 0.1035, 0.0906, 0.1073, 0.1916, 0.2504, 0.1690, 0.2609, 0.0723, 0.0417, 0.0597, 0.0442, 0.0552, 0.0297, 0.0270, 0.0183, 0.0123, 0.0100, 0.0057} Mean: 0.11069999635219574\n",
      "65% Cut Fake Rejections: {97.2, 97.8, 98.1, 97.8, 97.5, 97.5, 97.5, 97.8, 97.8, 97.2, 89.9, 87.9, 90.0, 85.7, 88.9, 88.6, 90.1, 91.9, 91.3, 94.6, 87.0, 88.4, 89.3, 85.0, 78.9} Mean: 92.1%\n",
      "\n",
      "70% Retention Cut Values: {0.2629, 0.2361, 0.2003, 0.2037, 0.2165, 0.1897, 0.1541, 0.1188, 0.1068, 0.1368, 0.2128, 0.2902, 0.1947, 0.3002, 0.0869, 0.0497, 0.0713, 0.0502, 0.0650, 0.0370, 0.0340, 0.0211, 0.0137, 0.0117, 0.0063} Mean: 0.13079999387264252\n",
      "70% Cut Fake Rejections: {96.1, 97.2, 97.6, 97.3, 96.8, 96.8, 97.1, 97.3, 97.3, 96.2, 88.6, 85.4, 88.5, 83.8, 86.9, 86.4, 88.6, 90.9, 90.2, 93.4, 84.2, 87.0, 87.6, 82.8, 77.6} Mean: 90.9%\n",
      "\n",
      "75% Retention Cut Values: {0.3379, 0.3084, 0.2357, 0.2501, 0.2501, 0.2346, 0.1864, 0.1535, 0.1249, 0.1868, 0.2382, 0.3318, 0.2401, 0.3343, 0.1031, 0.0625, 0.0818, 0.0615, 0.0831, 0.0467, 0.0445, 0.0243, 0.0153, 0.0140, 0.0074} Mean: 0.1582999974489212\n",
      "75% Cut Fake Rejections: {94.6, 96.0, 97.0, 96.4, 96.2, 95.9, 96.2, 96.3, 96.7, 94.8, 87.0, 83.0, 85.9, 81.9, 85.0, 83.9, 87.3, 89.4, 88.3, 92.0, 80.8, 85.2, 86.6, 80.2, 74.5} Mean: 89.2%\n",
      "\n",
      "80% Retention Cut Values: {0.4175, 0.4224, 0.2946, 0.3265, 0.3264, 0.2734, 0.2478, 0.1879, 0.1520, 0.2460, 0.2781, 0.3844, 0.2920, 0.3993, 0.1187, 0.0765, 0.0982, 0.0805, 0.1061, 0.0644, 0.0557, 0.0288, 0.0187, 0.0168, 0.0097} Mean: 0.19689999520778656\n",
      "80% Cut Fake Rejections: {92.9, 94.0, 95.9, 95.0, 94.6, 95.0, 94.6, 95.3, 95.8, 92.6, 84.4, 80.0, 83.1, 78.4, 83.6, 81.5, 85.6, 86.9, 86.2, 89.7, 78.0, 83.2, 83.7, 78.1, 70.2} Mean: 87.1%\n",
      "\n",
      "85% Retention Cut Values: {0.5190, 0.5398, 0.3760, 0.4063, 0.3922, 0.3490, 0.3289, 0.2385, 0.2072, 0.3168, 0.3340, 0.4398, 0.3609, 0.5060, 0.1707, 0.0933, 0.1248, 0.1158, 0.1441, 0.0827, 0.0738, 0.0402, 0.0314, 0.0208, 0.0131} Mean: 0.24899999797344208\n",
      "85% Cut Fake Rejections: {90.3, 91.5, 94.4, 93.4, 93.1, 93.2, 92.3, 93.7, 94.0, 89.9, 81.0, 76.5, 79.4, 72.9, 79.3, 78.8, 83.2, 83.6, 83.1, 87.7, 74.8, 78.8, 75.1, 74.8, 65.5} Mean: 84.0%\n",
      "\n",
      "90% Retention Cut Values: {0.6180, 0.6521, 0.6022, 0.5880, 0.5662, 0.4464, 0.4244, 0.3351, 0.3012, 0.4045, 0.4663, 0.5329, 0.4525, 0.6504, 0.2746, 0.1339, 0.1790, 0.1685, 0.2066, 0.1302, 0.0960, 0.0623, 0.0498, 0.0278, 0.0167} Mean: 0.3353999853134155\n",
      "90% Cut Fake Rejections: {87.2, 88.4, 89.5, 89.1, 88.4, 90.7, 89.5, 90.6, 90.6, 86.3, 73.2, 70.5, 74.3, 64.5, 72.9, 73.7, 79.0, 79.6, 79.4, 83.4, 71.5, 73.1, 67.9, 70.0, 61.3} Mean: 79.4%\n",
      "\n",
      "95% Retention Cut Values: {0.7176, 0.7772, 0.8458, 0.8151, 0.7574, 0.7156, 0.6743, 0.4925, 0.4833, 0.5268, 0.6188, 0.6430, 0.5647, 0.8129, 0.4544, 0.2189, 0.2672, 0.3052, 0.3373, 0.2165, 0.1635, 0.0924, 0.0915, 0.0545, 0.0241} Mean: 0.4668000042438507\n",
      "95% Cut Fake Rejections: {82.9, 83.2, 78.2, 79.1, 80.4, 81.1, 79.5, 84.4, 82.9, 80.6, 63.3, 62.8, 68.0, 52.1, 63.4, 65.2, 73.2, 71.3, 72.2, 77.2, 63.5, 66.3, 56.5, 59.0, 56.9} Mean: 71.3%\n",
      "Eta bin 0.00-0.10: 58324 fakes, 32 true Displaced\n",
      "Eta bin 0.10-0.20: 42710 fakes, 8 true Displaced\n",
      "Eta bin 0.20-0.30: 37318 fakes, 13 true Displaced\n",
      "Eta bin 0.30-0.40: 35874 fakes, 13 true Displaced\n",
      "Eta bin 0.40-0.50: 35293 fakes, 18 true Displaced\n",
      "Eta bin 0.50-0.60: 14925 fakes, 7 true Displaced\n",
      "Eta bin 0.60-0.70: 13246 fakes, 10 true Displaced\n",
      "Eta bin 0.70-0.80: 8969 fakes, 7 true Displaced\n",
      "Eta bin 0.80-0.90: 7843 fakes, 9 true Displaced\n",
      "Eta bin 0.90-1.00: 4815 fakes, 2 true Displaced\n",
      "Eta bin 1.00-1.10: 4865 fakes, 10 true Displaced\n",
      "Eta bin 1.10-1.20: 6657 fakes, 6 true Displaced\n",
      "Eta bin 1.20-1.30: 9686 fakes, 24 true Displaced\n",
      "Eta bin 1.30-1.40: 2782 fakes, 10 true Displaced\n",
      "Eta bin 1.40-1.50: 1961 fakes, 11 true Displaced\n",
      "Eta bin 1.50-1.60: 1527 fakes, 29 true Displaced\n",
      "Eta bin 1.60-1.70: 2750 fakes, 63 true Displaced\n",
      "Eta bin 1.70-1.80: 3531 fakes, 37 true Displaced\n",
      "Eta bin 1.80-1.90: 4829 fakes, 50 true Displaced\n",
      "Eta bin 1.90-2.00: 4246 fakes, 16 true Displaced\n",
      "Eta bin 2.00-2.10: 1789 fakes, 16 true Displaced\n",
      "Eta bin 2.10-2.20: 1601 fakes, 10 true Displaced\n",
      "Eta bin 2.20-2.30: 785 fakes, 3 true Displaced\n",
      "Eta bin 2.30-2.40: 1073 fakes, 16 true Displaced\n",
      "Eta bin 2.40-2.50: 242 fakes, 15 true Displaced\n",
      "<Figure size 2000x800 with 3 Axes>\n",
      "\n",
      "Displaced tracks, pt: 5.0 to inf GeV\n",
      "Number of true displaced tracks: 435\n",
      "Number of fake tracks in pt bin: 307641\n",
      "\n",
      "65% Retention Cut Values: {0.2265, 0.0395, 0.4312, 0.4840, 0.4537, 0.9577, 0.9294, 0.9280, 0.4712, 0.4557, 0.4738, 0.0794, 0.1110, 0.5503, 0.1385, 0.5199, 0.4993, 0.6410, 0.4735, 0.3342, 0.3112, 0.8436, 0.7717, 0.4069, 0.4031} Mean: 0.477400004863739\n",
      "65% Cut Fake Rejections: {96.8, 86.8, 98.8, 98.8, 98.5, 100.0, 99.8, 99.9, 98.2, 97.7, 97.8, 72.3, 75.0, 93.5, 68.4, 86.5, 88.1, 95.1, 87.3, 83.7, 80.4, 99.4, 97.5, 81.3, 85.1} Mean: 90.7%\n",
      "\n",
      "70% Retention Cut Values: {0.1905, 0.0395, 0.4040, 0.3383, 0.3143, 0.9561, 0.9278, 0.9043, 0.4546, 0.4192, 0.4578, 0.0747, 0.0802, 0.5133, 0.1262, 0.5143, 0.4778, 0.6301, 0.3928, 0.3137, 0.3038, 0.8183, 0.7583, 0.3755, 0.3992} Mean: 0.4474000036716461\n",
      "70% Cut Fake Rejections: {96.2, 86.8, 98.7, 98.5, 97.9, 100.0, 99.8, 99.9, 98.1, 97.4, 97.6, 70.9, 69.9, 93.2, 67.0, 86.2, 87.2, 94.4, 84.8, 82.8, 79.8, 98.6, 97.5, 78.9, 85.1} Mean: 89.9%\n",
      "\n",
      "75% Retention Cut Values: {0.1580, 0.0394, 0.3631, 0.1789, 0.2648, 0.9539, 0.9259, 0.8688, 0.4380, 0.3828, 0.4338, 0.0699, 0.0682, 0.4578, 0.1201, 0.3865, 0.4560, 0.5532, 0.3760, 0.2905, 0.2956, 0.8145, 0.7449, 0.3459, 0.3417} Mean: 0.413100004196167\n",
      "75% Cut Fake Rejections: {95.6, 86.8, 98.5, 97.2, 97.6, 100.0, 99.8, 99.8, 97.8, 97.1, 97.2, 69.6, 67.8, 90.5, 66.0, 72.4, 86.5, 92.9, 84.2, 81.5, 79.2, 98.4, 97.2, 77.5, 79.8} Mean: 88.4%\n",
      "\n",
      "80% Retention Cut Values: {0.0391, 0.0394, 0.3378, 0.1402, 0.1203, 0.9516, 0.9220, 0.8333, 0.4039, 0.3463, 0.4204, 0.0652, 0.0429, 0.3625, 0.1140, 0.3114, 0.3984, 0.4187, 0.3275, 0.2243, 0.2907, 0.7854, 0.7315, 0.3334, 0.2778} Mean: 0.3695000112056732\n",
      "80% Cut Fake Rejections: {84.2, 86.8, 98.4, 96.4, 95.2, 100.0, 99.8, 99.7, 97.5, 96.4, 97.1, 68.5, 60.6, 88.0, 65.2, 67.6, 83.7, 82.0, 82.1, 77.3, 78.3, 98.1, 96.9, 77.4, 72.7} Mean: 86.0%\n",
      "\n",
      "85% Retention Cut Values: {0.0246, 0.0394, 0.2889, 0.0983, 0.0281, 0.9173, 0.9160, 0.7343, 0.3697, 0.3098, 0.4204, 0.0617, 0.0345, 0.2174, 0.1020, 0.2642, 0.3695, 0.3122, 0.2431, 0.1738, 0.2899, 0.7246, 0.7180, 0.1358, 0.2513} Mean: 0.32179999351501465\n",
      "85% Cut Fake Rejections: {78.3, 86.8, 98.1, 94.8, 82.5, 99.9, 99.8, 99.3, 97.0, 96.2, 97.1, 67.3, 57.5, 79.4, 63.3, 64.5, 82.5, 75.2, 78.4, 73.7, 78.3, 97.3, 96.8, 57.7, 69.8} Mean: 82.9%\n",
      "\n",
      "90% Retention Cut Values: {0.0245, 0.0330, 0.1931, 0.0502, 0.0179, 0.8189, 0.8216, 0.5082, 0.3526, 0.2734, 0.4204, 0.0582, 0.0184, 0.1018, 0.0899, 0.2338, 0.2594, 0.2093, 0.1854, 0.1399, 0.2743, 0.6624, 0.7046, 0.0640, 0.2394} Mean: 0.2702000141143799\n",
      "90% Cut Fake Rejections: {78.3, 84.8, 97.2, 89.5, 75.9, 99.8, 99.6, 97.9, 96.7, 95.6, 97.1, 66.2, 48.2, 66.8, 61.2, 61.7, 75.9, 66.4, 74.9, 69.5, 76.0, 95.7, 96.7, 43.2, 68.6} Mean: 79.3%\n",
      "\n",
      "95% Retention Cut Values: {0.0129, 0.0255, 0.1427, 0.0241, 0.0030, 0.7204, 0.4184, 0.2822, 0.3526, 0.2369, 0.4204, 0.0546, 0.0125, 0.0888, 0.0824, 0.1519, 0.2437, 0.0519, 0.0186, 0.1089, 0.2072, 0.5954, 0.6912, 0.0577, 0.2156} Mean: 0.20880000293254852\n",
      "95% Cut Fake Rejections: {67.7, 81.4, 96.2, 81.4, 43.9, 99.6, 97.3, 95.9, 96.7, 94.7, 97.1, 65.3, 42.5, 64.8, 60.4, 52.7, 74.9, 47.2, 43.3, 66.3, 71.0, 94.2, 96.6, 40.9, 65.3} Mean: 73.5%\n",
      "<Figure size 2000x800 with 3 Axes>\n",
      "\n",
      "Fake tracks, pt: 5.0 to inf GeV\n",
      "Number of true fake tracks: 435\n",
      "Number of fake tracks in pt bin: 307641\n",
      "\n",
      "65% Retention Cut Values: {0.6532, 0.9603, 0.0759, 0.3199, 0.4190, 0.0409, 0.0602, 0.0498, 0.1935, 0.4991, 0.0806, 0.4732, 0.2191, 0.4040, 0.5088, 0.0703, 0.0402, 0.0639, 0.1041, 0.0875, 0.0805, 0.0054, 0.0079, 0.0199, 0.0103} Mean: 0.21789999306201935\n",
      "65% Cut Fake Rejections: {93.7, 65.2, 99.9, 98.8, 97.7, 100.0, 99.8, 99.9, 99.2, 95.4, 99.5, 91.5, 95.0, 84.9, 72.8, 91.2, 96.3, 92.9, 94.8, 92.7, 94.2, 100.0, 99.6, 97.6, 88.4} Mean: 93.6%\n",
      "\n",
      "70% Retention Cut Values: {0.7254, 0.9603, 0.2475, 0.4581, 0.6331, 0.0423, 0.0618, 0.0564, 0.2225, 0.5325, 0.0835, 0.4905, 0.2978, 0.4040, 0.5088, 0.0727, 0.0461, 0.0679, 0.1579, 0.0921, 0.0845, 0.0064, 0.0085, 0.0225, 0.0123} Mean: 0.251800000667572\n",
      "70% Cut Fake Rejections: {89.8, 65.2, 99.5, 97.6, 93.5, 99.9, 99.8, 99.9, 99.0, 95.0, 99.5, 90.9, 92.8, 84.9, 72.8, 91.2, 95.6, 92.6, 92.7, 92.5, 94.0, 100.0, 99.6, 97.2, 88.4} Mean: 93.0%\n",
      "\n",
      "75% Retention Cut Values: {0.7619, 0.9603, 0.5050, 0.6356, 0.6610, 0.0442, 0.0629, 0.0665, 0.2516, 0.5659, 0.0837, 0.5078, 0.3080, 0.4040, 0.5780, 0.0727, 0.0750, 0.0730, 0.3107, 0.1022, 0.0906, 0.0072, 0.0092, 0.0348, 0.0130} Mean: 0.2874000072479248\n",
      "75% Cut Fake Rejections: {88.0, 65.2, 97.5, 94.7, 91.9, 99.9, 99.8, 99.9, 98.7, 94.3, 99.5, 90.5, 92.2, 84.9, 67.9, 91.2, 94.4, 92.1, 83.5, 92.0, 93.7, 100.0, 99.5, 95.7, 88.4} Mean: 91.8%\n",
      "\n",
      "80% Retention Cut Values: {0.7911, 0.9604, 0.5849, 0.6872, 0.7378, 0.0462, 0.0638, 0.0765, 0.3630, 0.5993, 0.0892, 0.5251, 0.3419, 0.4363, 0.6472, 0.0885, 0.0940, 0.0798, 0.4018, 0.1204, 0.1090, 0.0087, 0.0098, 0.0659, 0.0147} Mean: 0.31769999861717224\n",
      "80% Cut Fake Rejections: {86.5, 65.2, 96.4, 91.6, 88.0, 99.9, 99.8, 99.8, 97.2, 93.2, 99.5, 89.9, 91.0, 83.9, 58.3, 89.6, 93.2, 91.7, 80.4, 91.0, 92.8, 99.8, 99.4, 90.2, 88.4} Mean: 90.3%\n",
      "\n",
      "85% Retention Cut Values: {0.7957, 0.9605, 0.6452, 0.7256, 0.8383, 0.0780, 0.0646, 0.1521, 0.4745, 0.6328, 0.1012, 0.5348, 0.4404, 0.5091, 0.7080, 0.1168, 0.1020, 0.0817, 0.4583, 0.1210, 0.1090, 0.0113, 0.0105, 0.0698, 0.0197} Mean: 0.35040000081062317\n",
      "85% Cut Fake Rejections: {86.2, 65.2, 94.6, 90.2, 81.1, 99.8, 99.8, 99.6, 94.6, 91.8, 99.4, 89.2, 87.3, 81.9, 52.5, 87.2, 92.9, 91.5, 77.5, 91.0, 92.8, 99.7, 99.4, 89.5, 87.6} Mean: 88.9%\n",
      "\n",
      "90% Retention Cut Values: {0.9115, 0.9605, 0.6660, 0.7374, 0.9263, 0.1698, 0.1485, 0.3590, 0.5302, 0.6662, 0.1273, 0.5445, 0.5916, 0.5985, 0.7687, 0.1317, 0.2187, 0.1160, 0.4810, 0.1532, 0.3180, 0.0155, 0.0111, 0.1336, 0.1455} Mean: 0.4171999990940094\n",
      "90% Cut Fake Rejections: {74.6, 65.2, 93.5, 89.7, 70.8, 99.6, 99.5, 96.9, 93.3, 90.4, 99.0, 88.7, 79.9, 76.9, 48.0, 85.7, 88.0, 89.4, 76.5, 89.6, 73.1, 98.9, 99.4, 77.6, 57.4} Mean: 84.1%\n",
      "\n",
      "95% Retention Cut Values: {0.9423, 0.9605, 0.7309, 0.8130, 0.9891, 0.2616, 0.5234, 0.5660, 0.5302, 0.6996, 0.2025, 0.5542, 0.6456, 0.7462, 0.7962, 0.1357, 0.2877, 0.1279, 0.5260, 0.3493, 0.6150, 0.0258, 0.0118, 0.1965, 0.2335} Mean: 0.49880000948905945\n",
      "95% Cut Fake Rejections: {67.2, 65.2, 90.4, 86.0, 38.0, 99.0, 93.9, 93.5, 93.3, 88.6, 97.6, 88.2, 74.3, 64.7, 46.6, 85.1, 84.4, 89.0, 74.3, 77.8, 47.1, 97.7, 99.4, 68.2, 41.7} Mean: 78.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import torch\n",
    "\n",
    "# Ensure input_features_tensor is on the right device\n",
    "input_features_tensor = input_features_tensor.to(device)\n",
    "\n",
    "t4_pt = np.concatenate(branches['t4_pt'])\n",
    "\n",
    "# Get model predictions\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(input_features_tensor)\n",
    "    predictions = outputs.cpu().numpy()  # Shape will be [n_samples, 3]\n",
    "\n",
    "\n",
    "def plot_for_pt_bin(pt_min, pt_max, percentiles, eta_bin_edges, t4_pt, predictions, t4_sim_vxy, eta_list):\n",
    "    \"\"\"\n",
    "    Calculate and plot cut values for specified percentiles in a given pt bin, separately for prompt and displaced tracks\n",
    "    \"\"\"\n",
    "    # Filter data based on pt bin\n",
    "    pt_mask = (t4_pt > pt_min) & (t4_pt <= pt_max)\n",
    "    \n",
    "    # Get absolute eta values for all tracks in pt bin\n",
    "    abs_eta = np.abs(eta_list[0][pt_mask])\n",
    "    \n",
    "    # Get predictions for all tracks in pt bin\n",
    "    pred_filtered = predictions[pt_mask]\n",
    "    \n",
    "    # Get track types using pMatched and t4_sim_vxy\n",
    "    matched = (np.concatenate(branches['t4_pMatched']) > 0.95)[pt_mask]\n",
    "    fake_tracks = (np.concatenate(branches['t4_pMatched']) <= 0.75)[pt_mask]\n",
    "    true_displaced = (t4_sim_vxy[pt_mask] > 0.1) & matched\n",
    "    \n",
    "    # Separate plots for prompt and displaced tracks\n",
    "    for track_type, true_mask, pred_idx, title_suffix in [\n",
    "        (\"Displaced\", true_displaced, 2, \"Displaced Real Tracks\"),\n",
    "        (\"Fake\", true_displaced, 0, \"Displaced Real Tracks\")\n",
    "    ]:\n",
    "        # Dictionaries to store values\n",
    "        cut_values = {p: [] for p in percentiles}\n",
    "        fake_rejections = {p: [] for p in percentiles}\n",
    "        \n",
    "        # Get probabilities for this class\n",
    "        probs = pred_filtered[:, pred_idx]\n",
    "        \n",
    "        # Create two side-by-side plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Plot probability distribution (only for true tracks of this type)\n",
    "        h = ax1.hist2d(abs_eta[true_mask], \n",
    "                      probs[true_mask], \n",
    "                      bins=[eta_bin_edges, 50], \n",
    "                      norm=LogNorm())\n",
    "        plt.colorbar(h[3], ax=ax1, label='Counts')\n",
    "        \n",
    "        # For each eta bin\n",
    "        bin_centers = []\n",
    "        for i in range(len(eta_bin_edges) - 1):\n",
    "            eta_min, eta_max = eta_bin_edges[i], eta_bin_edges[i+1]\n",
    "            bin_center = (eta_min + eta_max) / 2\n",
    "            bin_centers.append(bin_center)\n",
    "            \n",
    "            # Get tracks in this eta bin\n",
    "            eta_mask = (abs_eta >= eta_min) & (abs_eta < eta_max)\n",
    "            \n",
    "            # True tracks of this type in this bin\n",
    "            true_type_mask = eta_mask & true_mask\n",
    "            # Fake tracks in this bin\n",
    "            fake_mask = eta_mask & fake_tracks\n",
    "            \n",
    "            if track_type == \"Displaced\":\n",
    "                print(f\"Eta bin {eta_min:.2f}-{eta_max:.2f}: {np.sum(fake_mask)} fakes, {np.sum(true_type_mask)} true {track_type}\")\n",
    "            \n",
    "            if np.sum(true_type_mask) > 0:  # If we have true tracks in this bin\n",
    "                for percentile in percentiles:\n",
    "                    # Calculate cut value to keep desired percentage of true tracks\n",
    "                    if track_type == \"Fake\":\n",
    "                        cut_value = np.percentile(probs[true_type_mask], percentile)\n",
    "                    else:\n",
    "                        cut_value = np.percentile(probs[true_type_mask], 100 - percentile)\n",
    "                    cut_values[percentile].append(cut_value)\n",
    "                    \n",
    "                    # Calculate fake rejection for this cut\n",
    "                    if np.sum(fake_mask) > 0:\n",
    "                        if track_type == \"Fake\":\n",
    "                            fake_rej = 100 * np.mean(probs[fake_mask] > cut_value)\n",
    "                        else:\n",
    "                            fake_rej = 100 * np.mean(probs[fake_mask] < cut_value)\n",
    "                        fake_rejections[percentile].append(fake_rej)\n",
    "                    else:\n",
    "                        fake_rejections[percentile].append(np.nan)\n",
    "            else:\n",
    "                for percentile in percentiles:\n",
    "                    cut_values[percentile].append(np.nan)\n",
    "                    fake_rejections[percentile].append(np.nan)\n",
    "        \n",
    "        # Plot cut values and fake rejections\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(percentiles)))\n",
    "        bin_centers = np.array(bin_centers)\n",
    "        \n",
    "        for (percentile, color) in zip(percentiles, colors):\n",
    "            values = np.array(cut_values[percentile])\n",
    "            mask = ~np.isnan(values)\n",
    "            if np.any(mask):\n",
    "                # Plot cut values\n",
    "                ax1.plot(bin_centers[mask], values[mask], '-', color=color, marker='o',\n",
    "                        label=f'{percentile}% Retention Cut')\n",
    "                # Plot fake rejections\n",
    "                rej_values = np.array(fake_rejections[percentile])\n",
    "                ax2.plot(bin_centers[mask], rej_values[mask], '-', color=color, marker='o',\n",
    "                        label=f'{percentile}% Cut')\n",
    "        \n",
    "        # Set plot labels and titles\n",
    "        ax1.set_xlabel(\"Absolute Eta\")\n",
    "        ax1.set_ylabel(f\"DNN {track_type} Probability\")\n",
    "        ax1.set_title(f\"DNN Score vs Eta ({title_suffix})\\npt: {pt_min:.1f} to {pt_max:.1f} GeV\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.set_xlabel(\"Absolute Eta\")\n",
    "        ax2.set_ylabel(\"Fake Rejection (%)\")\n",
    "        ax2.set_title(f\"Fake Rejection vs Eta\\npt: {pt_min:.1f} to {pt_max:.1f} GeV\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.set_ylim(0, 100)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n{track_type} tracks, pt: {pt_min:.1f} to {pt_max:.1f} GeV\")\n",
    "        print(f\"Number of true {track_type.lower()} tracks: {np.sum(true_mask)}\")\n",
    "        print(f\"Number of fake tracks in pt bin: {np.sum(fake_tracks)}\")\n",
    "        \n",
    "        for percentile in percentiles:\n",
    "            print(f\"\\n{percentile}% Retention Cut Values:\",\n",
    "                  '{' + ', '.join(f\"{x:.4f}\" if not np.isnan(x) else 'nan' for x in cut_values[percentile]) + '}',\n",
    "                  f\"Mean: {np.round(np.nanmean(cut_values[percentile]), 4)}\")\n",
    "            print(f\"{percentile}% Cut Fake Rejections:\",\n",
    "                  '{' + ', '.join(f\"{x:.1f}\" if not np.isnan(x) else 'nan' for x in fake_rejections[percentile]) + '}',\n",
    "                  f\"Mean: {np.round(np.nanmean(fake_rejections[percentile]), 1)}%\")\n",
    "\n",
    "def analyze_pt_bins(pt_bins, percentiles, eta_bin_edges, t4_pt, predictions, t4_sim_vxy, eta_list):\n",
    "    \"\"\"\n",
    "    Analyze and plot for multiple pt bins and percentiles\n",
    "    \"\"\"\n",
    "    for i in range(len(pt_bins) - 1):\n",
    "        plot_for_pt_bin(pt_bins[i], pt_bins[i + 1], percentiles, eta_bin_edges,\n",
    "                       t4_pt, predictions, t4_sim_vxy, eta_list)\n",
    "\n",
    "# Run the analysis with same parameters as before\n",
    "percentiles = [65, 70, 75, 80, 85, 90, 95]\n",
    "pt_bins = [0, 5, np.inf]\n",
    "eta_bin_edges = np.arange(0, 2.6, 0.1)\n",
    "\n",
    "analyze_pt_bins(\n",
    "    pt_bins=pt_bins,\n",
    "    percentiles=percentiles,\n",
    "    eta_bin_edges=eta_bin_edges,\n",
    "    t4_pt=t4_pt,\n",
    "    predictions=predictions,\n",
    "    t4_sim_vxy=np.concatenate(branches['t4_sim_vxy']),\n",
    "    eta_list=eta_list\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
