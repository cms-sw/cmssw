[CRAB]

jobtype = cmssw
scheduler = glite
#scheduler    = caf


### NOTE: just setting the name of the server (pi, lnl etc etc )
###       crab will submit the jobs to the server...
#server_name = rwth
#server_name = bari 
##server_name = cern 
use_server =  0
##server_name = legnaro
#server_name = None


[CMSSW]

### The data you want to access (to be found on DBS)
#datasetpath=/RelValTTbar/marfin-53xRelVal2011Redigi-844a175eacc74cc51be2e7d71ca2539e/USER
#dbs_url =  https://cmsdbsprod.cern.ch:8443/cms_dbs_caf_analysis_01_writer/servlet/DBSServlet
datasetpath= /RelValTTbar/marfin-53xRelVal2011Redigi_v2-844a175eacc74cc51be2e7d71ca2539e/USER
dbs_url =  https://cmsdbsprod.cern.ch:8443/cms_dbs_ph_analysis_01_writer/servlet/DBSServlet


### The ParameterSet you want to use
pset=hltHarvesting_cfg.py 

### Splitting parameters
total_number_of_events=-1

#total_number_of_events=41400
#events_per_job =40000

number_of_jobs = 10

### The output files (comma separated list)
output_file=DQM_V0001_R000000001__CMSSW_5_3_11__RelVal__TrigVal.root
get_edm_output=1


[USER]
### OUTPUT files Management
##  output back into UI
return_data = 1

additional_input_files = my.ini

### To use a specific name of UI directory where CRAB will create job to submit (with full path).
### the default directory will be "crab_0_data_time"
#ui_working_dir = /full/path/Name_of_Directory

### To specify the UI directory where to store the CMS executable output
### FULL path is mandatory. Default is  <ui_working_dir>/res will be used.
#outputdir= 

### To specify the UI directory where to store the stderr, stdout and .BrokerInfo of submitted jobs
### FULL path is mandatory. Default is <ui_working_dir>/res will be used.
#logdir= /full/path/yourLogDir

### OUTPUT files INTO A SE
copy_data = 0
#check_user_remote_dir = 0
#check_user_remote_dir = 1


### if you want to copy data in a "official CMS site"
### you have to specify the name as written in 
#storage_element = T2_IT_Bari
### the user_remote_dir will be created under the SE mountpoint
### in the case of publication this directory is not considered
#user_remote_dir = name_directory_you_want

### if you want to copy your data at CAF
#storage_element = T2_CH_CAF
#storage_element = T2_CH_CERN 
### the user_remote_dir will be created under the SE mountpoint
### in the case of publication this directory is not considered
#user_remote_dir = name_directory_you_want

### if you want to copy your data to your area in castor at cern
### or in a "not official CMS site" you have to specify the complete name of SE

#! It works!!!!!!!!!!!!!!!!!
#storage_element=T2_DE_DESY
#storage_element=T2_IT_Bari
storage_element=srm-cms.cern.ch
#storage_element=T1_CH_CERN_Buffer


#!!!!!!CHECKING
#storage_element=dcache-se-cms.desy.de

### this directory is the mountpoin of SE 

#! It works!!!!!!!!!!!!!!!!!
#storage_path=/srm/managerv2?SFN=/castor/cern.ch/
storage_path=/srm/managerv2?SFN=/castor/cern.ch/user/m/marfin

#!!!!!CHECKING
#storage_path=/srm/managerv2?SFN=/pnfs/desy.de/cms/tier2/store/user/



#! It works!!!!!!!!!!!!!!!!!
#user_remote_dir=/user/m/marfin/BtagCalibration2
#user_remote_dir=/castor/cern.ch/cms/store/user/marfin/BtagCalibration2

#!!!!!CHECKING
#user_remote_dir=/53xRelVal2011Redig
#user_remote_dir=/53xRelVal2011Redig

user_remote_dir=/53xRelVal2011Redigi

### directory or tree of directory under the mounpoint 
#user_remote_dir = name_directory_you_want
### To publish produced output in a local istance of DBS set publish_data = 1
publish_data=0
### Specify the dataset name. The full path will be <primarydataset>/<publish_data_name>/USE
publish_data_name = 53xRelVal2011Redigi
### Specify the URL of DBS istance where CRAB has to publish the output files
dbs_url_for_publication = https://cmsdbsprod.cern.ch:8443/cms_dbs_caf_analysis_01_writer/servlet/DBSServlet 
#dbs_url_for_publication=http://dbs.physik.rwth-aachen.de:8081/cms_dbs_prod_test/servlet/DBSServlet
#publish_with_import_all_parents=1

### To specify additional files to be put in InputSandBox
### write the full path  if the files are not in the current directory
### (wildcard * are allowed): comma separated list
#additional_input_files = file1, file2, /full/path/file3

#if server
#hresholdLevel = 10
#eMail = marfin@mail.desy.de


#[SGE]
#
# parameters for SGE job submission
#resource = -V -l h_vmem=10G  -l h_cpu=48:00:00 -l os='sl5'  -m bae
#resource = -V -l h_vmem=10G  -l h_cpu=48:00:00 -l os='sl5' 

[CAF]
queue = cmscaf8nh 

[GRID]
#
## RB/WMS management:
rb = CERN

##  Black and White Lists management:
## By Storage
#se_black_list = T0,T1,T2_IN_TIFR

##se_white_list = T2_DE_DESY
##ce_white_list = T2_DE_DESY

#se_white_list=desy


## By ComputingElement
#se_black_list = 
#ce_black_list =
#ce_white_list =
#se_white_list =

[CONDORG]

# Set this to condor to override the batchsystem defined in gridcat.
# batchsystem = condor

# Specify addition condor_g requirments
# use this requirment to run on a cms dedicated hardare
# globus_rsl = (condor_submit=(requirements 'ClusterName == \"CMS\" && (Arch == \"INTEL\" || Arch == \"X86_64\")'))
# use this requirement to run on the new hardware
# globus_rsl = (condor_submit=(requirements 'regexp(\"cms-*\",Machine)'))
